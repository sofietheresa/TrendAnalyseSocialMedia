# --------------------------------------------
# ðŸ”§ ENV-VARIABLEN & BASISKONFIGURATION
# --------------------------------------------

include .env
export

API_URL=https://trendanalysesocialmedia.onrender.com
TOKEN=Bearer $(API_SECRET)

FILES=reddit_data.csv tiktok_data.csv youtube_data.csv
LOGS=reddit.log tiktok.log youtube.log

IMAGE_NAME=trendsage
CONTAINER_NAME=trend_scheduler
PORT=8080
CONTAINER_PORT=10000

# --------------------------------------------
# ðŸ“¡ REMOTE SCRAPING & DATEN-SYNC
# --------------------------------------------

.PHONY: sync trigger-api trigger-loop

# â±ï¸ Scraping-Job Ã¼ber API auf Render auslÃ¶sen + Daten/Logs lokal abholen
sync:
	curl -X POST $(API_URL)/run-scrapers \
		-H "Authorization: $(TOKEN)"

	@echo "\nâ¬‡ï¸  Lade Daten herunter & hÃ¤nge sie an bestehende Dateien an ..."
	@mkdir -p data/raw
	@for file in $(FILES); do \
		tmp="data/raw/tmp_$$file"; \
		curl -s $(API_URL)/data/download/$$file -H "Authorization: $(TOKEN)" -o $$tmp; \
		if [ -f data/raw/$$file ]; then \
			tail -n +2 $$tmp >> data/raw/$$file; \
		else \
			mv $$tmp data/raw/$$file; \
		fi; \
		rm -f $$tmp; \
	done

	@echo "\nâ¬‡ï¸  Lade Logs herunter & hÃ¤nge sie an bestehende Dateien an ..."
	@mkdir -p logs
	@for log in $(LOGS); do \
		tmp="logs/tmp_$$log"; \
		curl -s $(API_URL)/logs/download/$$log -H "Authorization: $(TOKEN)" -o $$tmp; \
		if [ -f logs/$$log ]; then \
			cat $$tmp >> logs/$$log; \
		else \
			mv $$tmp logs/$$log; \
		fi; \
		rm -f $$tmp; \
	done

	@echo "\nâœ… Alle Dateien synchronisiert."

# ðŸ” Nur Scraper via API triggern (z. B. fÃ¼r Testzwecke)
trigger-api:
	curl -X POST $(API_URL)/run-scrapers \
		-H "Authorization: $(TOKEN)"

# ðŸ•“ Lokal laufende Endlosschleife, die alle 15min Render abfragt & lokal scrapt
trigger-loop:
	podman run -it --rm \
		--env-file .env \
		-v $(PWD)/logs:/app/logs \
		-v $(PWD)/data:/app/data \
		-v $(PWD):/app \
		-w /app \
		$(IMAGE_NAME) \
		python src/scheduler/scraper_trigger.py

# --------------------------------------------
# ðŸ§± PODMAN CONTAINER-STEUERUNG
# --------------------------------------------

.PHONY: build run run-detached clean-container restart logs shell

# ðŸ“¦ Container-Image bauen
build:
	podman build -t $(IMAGE_NAME) .

# ðŸš€ Container interaktiv starten (API lokal aufrufbar)
run:
	podman run -p $(PORT):$(CONTAINER_PORT) \
		--name $(CONTAINER_NAME) \
		--env-file .env \
		-v $(PWD)/logs:/app/logs \
		-v $(PWD)/data:/app/data \
		-v $(PWD):/app \
		-w /app \
		$(IMAGE_NAME)

# ðŸ” Container im Hintergrund starten
run-detached:
	podman run -d -p $(PORT):$(CONTAINER_PORT) \
		--name $(CONTAINER_NAME) \
		--env-file .env \
		-v $(PWD)/logs:/app/logs \
		-v $(PWD)/data:/app/data \
		-v $(PWD):/app \
		-w /app \
		$(IMAGE_NAME)

# ðŸ§¼ Container stoppen und lÃ¶schen
clean-container:
	-podman stop $(CONTAINER_NAME)
	-podman rm $(CONTAINER_NAME)

# ðŸ§¨ Alles lÃ¶schen (inkl. lokale Logs & Daten)
clean-all: clean-container
	rm -rf logs/* data/*

# ðŸ”„ Container neustarten
restart:
	podman restart $(CONTAINER_NAME)

# ðŸ“‹ Live-Logs anzeigen
logs:
	podman logs -f $(CONTAINER_NAME)

# ðŸ” Interaktive Shell im Container Ã¶ffnen
shell:
	podman exec -it $(CONTAINER_NAME) /bin/sh

# --------------------------------------------
# âš™ï¸ SCRAPER LOKAL AUSFÃœHREN (zum Debuggen)
# --------------------------------------------

.PHONY: run-all-scrapers

run-all-scrapers:
	python src/scheduler/run_all_scrapers.py

# --------------------------------------------
# ðŸ³ OPTIONAL: DOCKER-COMPOSE UNTERSTÃœTZUNG
# --------------------------------------------

.PHONY: compose-up compose-logs

compose-up:
	docker-compose up --build

compose-logs:
	docker-compose logs -f scraper-trigger

# --------------------------------------------
# ðŸ› ï¸ ENTWICKLUNG & WARTUNG
# --------------------------------------------

.PHONY: install start qdrant stop-qdrant lint test pipeline download-nltk clean

# ðŸ“¦ AbhÃ¤ngigkeiten installieren
install:
	pip install -e .

# ðŸš€ FastAPI-Server starten
start:
	uvicorn src.main:app --reload

# ðŸ” Qdrant Vektordatenbank starten
qdrant:
	docker run -d --name qdrant -p 6333:6333 -p 6334:6334 qdrant/qdrant

# ðŸ›‘ Qdrant stoppen
stop-qdrant:
	docker stop qdrant || true
	docker rm qdrant || true

# âœ¨ Code formatieren und linten
lint:
	black src/ || true
	flake8 src/ || true

# ðŸ§ª Tests ausfÃ¼hren
test:
	pytest || echo "No tests found."

# ðŸ”„ Pipeline ausfÃ¼hren
pipeline:
	python src/setup_pipeline.py

# ðŸ“š NLTK-Ressourcen herunterladen
download-nltk:
	python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet'); nltk.download('averaged_perceptron_tagger'); nltk.download('averaged_perceptron_tagger_eng')"

# ðŸ§¹ TemporÃ¤re Dateien aufrÃ¤umen
clean:
	rm -rf __pycache__ .pytest_cache .mypy_cache
	rm -rf data/processed/*.csv data/processed/*.json
	rm -rf .venv
	echo "Cleaned up temporary and output files."

# Add this section to your Makefile

.PHONY: restart-server

# ðŸ”„ Restart FastAPI server
restart-server:
	@echo "Stopping any running FastAPI server..."
	-pkill -f "uvicorn src.main:app" || true
	@echo "Starting FastAPI server..."
	uvicorn src.main:app --host 0.0.0.0 --port 8000 --reload

	# Port, auf dem Streamlit lÃ¤uft
PORT ?= 8501
APP ?= frontend/app.py  # Pfad zu deiner Streamlit-Hauptdatei

# â–¶ï¸ Startet das Streamlit-Frontend
start-frontend:
	streamlit run $(APP) --server.port=$(PORT)

# ðŸ” Startet neu, beendet ggf. laufende Instanz
restart-frontend:
	-@kill -9 $$(lsof -t -i :$(PORT)) 2>/dev/null || true
	sleep 1
	make start-frontend

# ðŸ›‘ Beendet Frontend-Prozess auf dem Port
stop-frontend:
	-@kill -9 $$(lsof -t -i :$(PORT)) 2>/dev/null || echo "âœ… Kein laufender Streamlit-Prozess gefunden."

# ðŸ“‚ Ã–ffnet das Frontend im Browser
open-frontend:
	python -m webbrowser http://localhost:$(PORT)

