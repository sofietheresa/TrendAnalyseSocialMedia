{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03316f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_label\n",
      "LABEL_1    0.840846\n",
      "ERROR      0.112818\n",
      "LABEL_0    0.024679\n",
      "LABEL_2    0.021657\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/processed/sentiment_annotated_data.csv\")\n",
    "\n",
    "print(df[\"sentiment_label\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22651ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SofiePischl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SofiePischl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SofiePischl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenisierte Datei gespeichert unter: data/processed/tokenized_social_media_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialisiere NLTK-Tools\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Daten laden\n",
    "df = pd.read_csv(\"../data/processed/cleaned_social_media_data.csv\")\n",
    "\n",
    "# Text vorbereiten (title + description)\n",
    "df[\"text\"] = df[\"title\"].fillna(\"\") + \" \" + df[\"description/text\"].fillna(\"\")\n",
    "\n",
    "# Preprocessing- & Tokenisierung-Funktion\n",
    "def tokenize_and_lemmatize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    return tokens\n",
    "\n",
    "# Token-Spalte erstellen\n",
    "df[\"tokens\"] = df[\"text\"].apply(tokenize_and_lemmatize)\n",
    "\n",
    "# Optional: Token als Joined-String (für späteres Vectorizing)\n",
    "df[\"tokenized_text\"] = df[\"tokens\"].apply(lambda tokens: \" \".join(tokens))\n",
    "\n",
    "# Speichern\n",
    "df.to_csv(\"../data/processed/tokenized_social_media_data.csv\", index=False)\n",
    "print(\"Tokenisierte Datei gespeichert unter: data/processed/tokenized_social_media_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cce9d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('subreddits', 2231), ('community', 1960), ('year', 1529), ('subscriber', 1436), ('trending', 1172), ('subreddit', 646), ('interesting', 603), ('2021', 579), ('und', 449), ('die', 439), ('also', 411), ('discussion', 394), ('work', 389), ('feel', 376), ('video', 357), ('please', 355), ('discus', 344), ('based', 336), ('free', 331), ('started', 331), ('der', 328), ('like', 328), ('small', 324), ('keep', 320), ('try', 314), ('comment', 304), ('front', 303), ('page', 302), ('check', 300), ('hope', 300)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "all_tokens = list(chain.from_iterable(df[\"tokens\"]))\n",
    "print(Counter(all_tokens).most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe481844",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyLDAvis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyLDAvis\u001b[39;00m\n\u001b[0;32m      4\u001b[0m pyLDAvis\u001b[38;5;241m.\u001b[39menable_notebook()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.sklearn\n",
    "import pyLDAvis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(lda, dtm, vectorizer)\n",
    "pyLDAvis.save_html(panel, \"lda_topics.html\")  # erzeugt klickbare Übersicht\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7b34ab9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m topic_distributions \u001b[38;5;241m=\u001b[39m \u001b[43mlda\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(dtm)\n\u001b[0;32m      2\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdominant_topic\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m topic_distributions\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lda' is not defined"
     ]
    }
   ],
   "source": [
    "topic_distributions = lda.transform(dtm)\n",
    "df[\"dominant_topic\"] = topic_distributions.argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c221b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Konfigurierbarer Pfad ===\n",
    "BASE_DIR = os.getenv(\"DATA_DIR\", \"../data\")\n",
    "INPUT_FILE = os.path.join(BASE_DIR, \"processed/cleaned_social_media_data.csv\")\n",
    "OUTPUT_TOPIC_FILE = os.path.join(BASE_DIR, \"processed/bertopic_topics.csv\")\n",
    "TOPIC_VIS_FILE = os.path.join(BASE_DIR, \"processed/bertopic_visualization.html\")\n",
    "\n",
    "# === Daten laden ===\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "texts = df[\"description/text\"].astype(str).tolist()\n",
    "\n",
    "# === Embeddings vorbereiten ===\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # klein & schnell\n",
    "embeddings = embedding_model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "# === BERTopic Modell trainieren ===\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\")\n",
    "topic_model = BERTopic(vectorizer_model=vectorizer_model)\n",
    "topics, probs = topic_model.fit_transform(texts, embeddings)\n",
    "\n",
    "# === Ergebnisse speichern ===\n",
    "df[\"topic\"] = topics\n",
    "df[\"topic_probability\"] = probs\n",
    "df.to_csv(OUTPUT_TOPIC_FILE, index=False)\n",
    "\n",
    "# === Top Themen anzeigen ===\n",
    "print(\"\\nTop 10 Themen mit Stichwörtern:\")\n",
    "for topic_id, words in topic_model.get_topics().items():\n",
    "    if topic_id == -1:\n",
    "        continue\n",
    "    print(f\"\\nTopic {topic_id}: {[w[0] for w in words[:5]]}\")\n",
    "\n",
    "# === Beispieltexte pro Topic ===\n",
    "for topic_id in topic_model.get_topic_freq().head(5)[\"Topic\"]:\n",
    "    if topic_id == -1:\n",
    "        continue\n",
    "    print(f\"\\n=== Beispiele für Topic {topic_id} ===\")\n",
    "    examples = df[df[\"topic\"] == topic_id].head(3)\n",
    "    for _, row in examples.iterrows():\n",
    "        print(\"-\", row[\"description/text\"][:200])\n",
    "\n",
    "# === Interaktive Visualisierung speichern ===\n",
    "topic_model.visualize_topics().write_html(TOPIC_VIS_FILE)\n",
    "print(f\"\\n✅ Interaktive Visualisierung gespeichert unter: {TOPIC_VIS_FILE}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
