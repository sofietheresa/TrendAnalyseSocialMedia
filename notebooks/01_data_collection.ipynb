{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bdbda90",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 16px;\">\n",
    "    <strong>Course:</strong> Machine Learning Operations |\n",
    "    <strong>Lecturer:</strong> Prof. Dr. Klotz |\n",
    "    <strong>Date:</strong> 17.05.2025 |\n",
    "    <strong>Name:</strong> Sofie Pischl\n",
    "</div>\n",
    "\n",
    "# <center>Data Collection </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a95362a",
   "metadata": {},
   "source": [
    "Konzept & Inhalt:\n",
    "\n",
    "Daten von den gr√∂√üten Social media Apps sollen abgegriffen werden. besonderer Fokus auf Texten.\n",
    "\n",
    "1. Setup & Imports\n",
    "2. Reddit: Hot Posts aus Subreddits\n",
    "3. Instagram: Top Posts per Scraping/API (light)\n",
    "4. Twitter: Aktuelle Tweets via snscrape oder Tweepy\n",
    "5. TikTok: Trending Videos\n",
    "6. YouTube: Trending Videos (API/Scraping)\n",
    "7. Fazit & Learnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b780b59",
   "metadata": {},
   "source": [
    "----\n",
    "# 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d312ad",
   "metadata": {},
   "source": [
    "Import allgemeiner Bibliotheken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef2f679e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Standardbibliotheken\n",
    "import asyncio            # F√ºr asynchrone Programmierung (z.‚ÄØB. async/await)\n",
    "import csv                # Zum Lesen/Schreiben von CSV-Dateien auf niedriger Ebene\n",
    "import logging            # F√ºr Logging von Status- und Fehlermeldungen\n",
    "import os                 # F√ºr Zugriff auf Umgebungsvariablen, Dateipfade etc.\n",
    "import time               # F√ºr Zeitfunktionen wie sleep()\n",
    "from datetime import datetime  # F√ºr aktuelle Zeit und Datumsmanipulation\n",
    "from pathlib import Path  # F√ºr objektorientierten Umgang mit Dateipfaden\n",
    "\n",
    "#Drittanbieter-Bibliotheken\n",
    "import pandas as pd       # F√ºr Datenanalyse und CSV-Verarbeitung\n",
    "from dotenv import load_dotenv  # Zum Laden von Umgebungsvariablen aus .env-Dateien\n",
    "\n",
    "# Load from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa66e0a",
   "metadata": {},
   "source": [
    "# 2. Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b79d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import praw  # Reddit API Wrapper f√ºr den Zugriff auf Subreddits, Posts und Kommentare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089b93d3",
   "metadata": {},
   "source": [
    "### Funktionen:\n",
    "- Authentifizierung √ºber OAuth2 via `praw`\n",
    "- Abruf der `hot`-Beitr√§ge aus ausgew√§hlten Subreddits\n",
    "- Speicherung als `.csv` unter `/data/raw/reddit_data.csv`\n",
    "- Fehler-Handling und Logging integriert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d039ab76",
   "metadata": {},
   "source": [
    "**Authentifizierung**\n",
    "\n",
    "Zur Authentifizierung an der Reddit-API wird ein Reddit-Objekt der Bibliothek praw initialisiert. Die ben√∂tigten Zugangsdaten ‚Äì client_id, client_secret und user_agent ‚Äì werden aus einer .env-Datei geladen, um die Trennung von Code und Konfiguration zu gew√§hrleisten und Sicherheitsrisiken zu minimieren.\n",
    "\n",
    "Diese Parameter dienen der eindeutigen Identifikation der Anwendung gegen√ºber der API und sind notwendig, um Zugriff auf Reddit-Inhalte zu erhalten. Der user_agent erm√∂glicht zudem die R√ºckverfolgbarkeit von API-Anfragen seitens Reddit. Ohne diese Authentifizierung ist ein reguliertes, automatisiertes Scraping nicht zul√§ssig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5789b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=(os.getenv(\"REDDIT_ID\")),\n",
    "    client_secret=(os.getenv(\"REDDIT_SECRET\")),\n",
    "    user_agent=(os.getenv(\"USER_AGENT\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c34f9d7",
   "metadata": {},
   "source": [
    "**Logging-Konfiguration**\n",
    "\n",
    "Bevor das Reddit-Scraping startet, wird ein Logging-System eingerichtet. Dazu wird zun√§chst ein Pfad zur Log-Datei definiert ‚Äì in diesem Fall `logs/reddit.log`. Falls das Verzeichnis `logs/` noch nicht existiert, wird es automatisch erstellt. Anschlie√üend wird das Python-Logging so konfiguriert, dass alle Log-Meldungen in diese Datei geschrieben werden.\n",
    "\n",
    "Die Konfiguration legt fest, dass nur Meldungen ab dem Schweregrad `INFO` gespeichert werden. Au√üerdem wird das Format der Eintr√§ge so definiert, dass jeder Log-Eintrag einen Zeitstempel, den Log-Level (wie `INFO` oder `ERROR`) sowie die eigentliche Nachricht enth√§lt. So entsteht eine nachvollziehbare Chronik √ºber den Ablauf und m√∂gliche Fehler des Scripts.\n",
    "\n",
    "Ein typischer Eintrag k√∂nnte zum Beispiel so aussehen:\n",
    "\n",
    "```\n",
    "2025-04-19 14:33:07,512 - INFO - Starte Reddit-Scraping...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b20f953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Logging aktiv unter: C:\\Users\\SofiePischl\\Documents\\01_HdM\\10_ML_OPS\\TrendAnalyse Social Media\\logs\\reddit.log\n"
     ]
    }
   ],
   "source": [
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "# Logging einrichten\n",
    "log_path = Path(\"../logs/reddit.log\")\n",
    "log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    filename=log_path,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "print(f\" Logging aktiv unter: {log_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6781e111",
   "metadata": {},
   "source": [
    "**Reddit-Datensammlung mittels Python und PRAW**\n",
    "\n",
    "Die Funktion `scrape_reddit()` dient der systematischen Erhebung textbasierter Inhalte aus der Social-Media-Plattform **Reddit**. Ziel ist es, strukturierte Daten zur Analyse von Trendthemen zu generieren. Zur Umsetzung wird die Bibliothek `praw` (Python Reddit API Wrapper) verwendet, die eine komfortable Schnittstelle zur Reddit-API bereitstellt.\n",
    "\n",
    "Nach der Initialisierung der Protokollierung via `logging` erfolgt die Authentifizierung an der Reddit-API. Hierf√ºr wird ein `Reddit`-Objekt instanziiert, wobei sensible Zugangsdaten wie `client_id`, `client_secret` und `user_agent` aus einer `.env`-Datei geladen werden. Dieses Vorgehen erm√∂glicht eine sichere Trennung von Konfiguration und Codebasis und sch√ºtzt vor dem unbeabsichtigten Leaken von API-Schl√ºsseln.\n",
    "\n",
    "Im Anschluss wird eine Liste von Subreddits definiert, die sowohl popul√§re als auch als ‚Äûtrending‚Äú markierte Communities umfasst. Aus diesen Subreddits werden jeweils bis zu 100 Beitr√§ge aus dem Hot-Feed abgerufen. Dieses Verfahren stellt sicher, dass aktuelle, stark diskutierte Inhalte gesammelt werden, die ein hohes Relevanzpotenzial f√ºr Trendanalysen aufweisen.\n",
    "\n",
    "Die Datenextraktion erfolgt √ºber eine doppelte Schleife: F√ºr jedes Subreddit werden Hot-Beitr√§ge iteriert, wobei ausschlie√ülich ‚Äûself-posts‚Äú ber√ºcksichtigt werden. Diese beinhalten keine externen Links und erm√∂glichen dadurch eine fokussierte Analyse des vom Nutzer selbst verfassten Textinhalts. Pro Beitrag werden zentrale Metriken wie Titel, Text, Anzahl der Kommentare, Upvotes, Erstellungszeitpunkt sowie die URL gespeichert. Zur besseren zeitlichen Einordnung wird au√üerdem ein einheitlicher Zeitstempel f√ºr alle Eintr√§ge vergeben.\n",
    "\n",
    "Die gesammelten Beitr√§ge werden in einem `pandas.DataFrame` strukturiert und anschlie√üend unter `data/raw/reddit_data.csv` abgespeichert. Dabei wird sichergestellt, dass ben√∂tigte Verzeichnisse automatisch erstellt werden. Falls bereits Daten vorhanden sind, werden die neuen Eintr√§ge angeh√§ngt und anschlie√üend Duplikate basierend auf Titel, Textinhalt und Subreddit entfernt. Die finale Version wird ohne Index in die CSV-Datei geschrieben.\n",
    "\n",
    "Abschlie√üend wird die Anzahl der gespeicherten Beitr√§ge im Logfile vermerkt. Etwaige Fehler werden w√§hrend der Ausf√ºhrung abgefangen und entsprechend protokolliert. Die Funktion kann sowohl als Modul importiert als auch direkt per Skriptausf√ºhrung genutzt werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f757739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Gespeichert unter: C:\\Users\\SofiePischl\\Documents\\01_HdM\\10_ML_OPS\\TrendAnalyse Social Media\\app\\data\\raw\\reddit_data.csv\n"
     ]
    }
   ],
   "source": [
    "def scrape_reddit():\n",
    "    try:\n",
    "        logging.info(\"Starte Reddit-Scraping...\")\n",
    "\n",
    "        subreddits = [\"all\", \"popular\", \"trendingreddits\", \"trendingsubreddits\"]\n",
    "        post_limit = 100\n",
    "        all_posts = []\n",
    "        scrape_time = datetime.now()\n",
    "\n",
    "        for sub in subreddits:\n",
    "            subreddit = reddit.subreddit(sub)\n",
    "            for post in subreddit.hot(limit=post_limit):\n",
    "                if post.is_self:\n",
    "                    all_posts.append({\n",
    "                        \"subreddit\": sub,\n",
    "                        \"title\": post.title,\n",
    "                        \"text\": post.selftext,\n",
    "                        \"score\": post.score,\n",
    "                        \"comments\": post.num_comments,\n",
    "                        \"created\": datetime.fromtimestamp(post.created),\n",
    "                        \"url\": post.url,\n",
    "                        \"scraped_at\": scrape_time\n",
    "                    })\n",
    "\n",
    "        df = pd.DataFrame(all_posts)\n",
    "        csv_path = Path(\"../app/data/raw/reddit_data.csv\")\n",
    "        csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if csv_path.exists():\n",
    "            df_existing = pd.read_csv(csv_path)\n",
    "            df = pd.concat([df_existing, df], ignore_index=True)\n",
    "\n",
    "        df.drop_duplicates(subset=[\"title\", \"text\", \"subreddit\"], inplace=True)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\" Gespeichert unter: {csv_path.resolve()}\")\n",
    "\n",
    "        logging.info(f\"{len(df)} Eintr√§ge gespeichert unter {csv_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fehler beim Reddit-Scraping: {e}\")\n",
    "\n",
    "# Ausf√ºhrung bei direktem Aufruf\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_reddit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba082732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>comments</th>\n",
       "      <th>created</th>\n",
       "      <th>url</th>\n",
       "      <th>scraped_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "      <td>AITAH for telling my best friend her marriage ...</td>\n",
       "      <td>This weekend was a disaster...\\n\\nI 27F have b...</td>\n",
       "      <td>17380</td>\n",
       "      <td>1914</td>\n",
       "      <td>2025-05-07 02:03:00</td>\n",
       "      <td>https://www.reddit.com/r/AITAH/comments/1kgjqg...</td>\n",
       "      <td>2025-05-07 11:03:58.051533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all</td>\n",
       "      <td>[Post Game Thread] The Indiana Pacers head bac...</td>\n",
       "      <td>\\n||\\n|:-:|\\n|[](/IND) **120 -  119** [](/CLE)...</td>\n",
       "      <td>6946</td>\n",
       "      <td>2069</td>\n",
       "      <td>2025-05-07 03:48:31</td>\n",
       "      <td>https://www.reddit.com/r/nba/comments/1kgltqu/...</td>\n",
       "      <td>2025-05-07 11:03:58.051533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>popular</td>\n",
       "      <td>Americans, how do you feel about the firing of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21007</td>\n",
       "      <td>5167</td>\n",
       "      <td>2025-05-06 19:46:16</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/1k...</td>\n",
       "      <td>2025-05-07 11:03:58.051533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>popular</td>\n",
       "      <td>Claim your Trailer 2 OG flair here!</td>\n",
       "      <td>Out of the blue we got the second trailer...FI...</td>\n",
       "      <td>12751</td>\n",
       "      <td>46078</td>\n",
       "      <td>2025-05-06 16:43:19</td>\n",
       "      <td>https://www.reddit.com/r/GTA6/comments/1kg68js...</td>\n",
       "      <td>2025-05-07 11:03:58.051533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>popular</td>\n",
       "      <td>AITA for refusing to let my daughter‚Äôs fianc√© ...</td>\n",
       "      <td>So I (M49) might be in the wrong here, but I h...</td>\n",
       "      <td>8064</td>\n",
       "      <td>4265</td>\n",
       "      <td>2025-05-06 18:29:17</td>\n",
       "      <td>https://www.reddit.com/r/AmItheAsshole/comment...</td>\n",
       "      <td>2025-05-07 11:03:58.051533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit                                              title  \\\n",
       "0       all  AITAH for telling my best friend her marriage ...   \n",
       "1       all  [Post Game Thread] The Indiana Pacers head bac...   \n",
       "2   popular  Americans, how do you feel about the firing of...   \n",
       "3   popular                Claim your Trailer 2 OG flair here!   \n",
       "4   popular  AITA for refusing to let my daughter‚Äôs fianc√© ...   \n",
       "\n",
       "                                                text  score  comments  \\\n",
       "0  This weekend was a disaster...\\n\\nI 27F have b...  17380      1914   \n",
       "1  \\n||\\n|:-:|\\n|[](/IND) **120 -  119** [](/CLE)...   6946      2069   \n",
       "2                                                NaN  21007      5167   \n",
       "3  Out of the blue we got the second trailer...FI...  12751     46078   \n",
       "4  So I (M49) might be in the wrong here, but I h...   8064      4265   \n",
       "\n",
       "               created                                                url  \\\n",
       "0  2025-05-07 02:03:00  https://www.reddit.com/r/AITAH/comments/1kgjqg...   \n",
       "1  2025-05-07 03:48:31  https://www.reddit.com/r/nba/comments/1kgltqu/...   \n",
       "2  2025-05-06 19:46:16  https://www.reddit.com/r/AskReddit/comments/1k...   \n",
       "3  2025-05-06 16:43:19  https://www.reddit.com/r/GTA6/comments/1kg68js...   \n",
       "4  2025-05-06 18:29:17  https://www.reddit.com/r/AmItheAsshole/comment...   \n",
       "\n",
       "                   scraped_at  \n",
       "0  2025-05-07 11:03:58.051533  \n",
       "1  2025-05-07 11:03:58.051533  \n",
       "2  2025-05-07 11:03:58.051533  \n",
       "3  2025-05-07 11:03:58.051533  \n",
       "4  2025-05-07 11:03:58.051533  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zeuge neuste Eintr√§ge\n",
    "df = pd.read_csv(\"../app/data/raw/reddit_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8152499",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Instagram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125499c",
   "metadata": {},
   "source": [
    "Im Rahmen der Datenerhebung f√ºr Instagram wurden zun√§chst etablierte Bibliotheken wie *Instaloader* und *Playwright* getestet. Beide Ans√§tze erwiesen sich jedoch als unzureichend: *Instaloader* erm√∂glicht nur den Zugriff auf √∂ffentlich verf√ºgbare Inhalte registrierter Nutzer und bietet keinen Zugriff auf die dynamisch generierte Explore-Seite. *Playwright*, obwohl f√ºr modernes Web-Scraping geeignet, stie√ü auf technische Einschr√§nkungen hinsichtlich der Authentifizierung und der zuverl√§ssigen Navigation innerhalb der dynamischen Inhalte Instagrams.\n",
    "\n",
    "Aus diesem Grund wurde ein alternativer Ansatz mittels *Selenium* implementiert. Dieser verfolgt die Strategie, das Nutzerverhalten im Browser m√∂glichst realit√§tsnah zu simulieren. Der automatisierte Prozess umfasst dabei das Aufrufen der Instagram-Loginseite, die Anmeldung √ºber Umgebungsvariablen gespeicherte Zugangsdaten, die Navigation zur Explore-Seite sowie das Extrahieren von Beitragsinformationen (z.‚ÄØB. Bildquelle, Beschreibung, URL) √ºber XPath-Selektoren.\n",
    "\n",
    "Grunds√§tzlich zeigte sich der Selenium-basierte Ansatz als funktional, jedoch mit erheblichen Stabilit√§tsproblemen. Da die DOM-Struktur der Explore-Seite nicht statisch ist, k√∂nnen sich XPath-Referenzen zwischenzeitlich √§ndern oder variieren, was zu fehleranf√§lligen Selektionsoperationen f√ºhrt. Zudem ist die Explore-Seite stark dynamisch und in hohem Ma√üe vom Nutzerverhalten und den Algorithmus-basierten Inhalten abh√§ngig, was eine konsistente und reproduzierbare Datenextraktion erschwert. Somit stellt die Nutzung von Selenium eine technisch m√∂gliche, jedoch fragile und wartungsintensive L√∂sung f√ºr das Scraping dynamischer Instagram-Inhalte dar.\n",
    "\n",
    "Deshalb wird der Ansatz im folgenden dargestellt, wurde aber im weiteren Verlauf des Projekts verworden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ba8d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver  # Startet und steuert Browser-Instanzen (z.‚ÄØB. Chrome)\n",
    "from selenium.webdriver.chrome.service import Service  # Verwaltet den ChromeDriver-Dienst\n",
    "from selenium.webdriver.common.by import By  # Selektoren zum Finden von Elementen (z.‚ÄØB. By.ID, By.XPATH)\n",
    "from selenium.webdriver.chrome.options import Options  # Konfiguration f√ºr den Chrome-Browser (z.‚ÄØB. headless-Modus)\n",
    "from selenium.webdriver.support.ui import WebDriverWait  # Erm√∂glicht explizite Wartezeiten f√ºr Elemente\n",
    "from selenium.webdriver.support import expected_conditions as EC  # Vordefinierte Bedingungen zum Warten (z.‚ÄØB. Sichtbarkeit von Elementen)\n",
    "from webdriver_manager.chrome import ChromeDriverManager  # Automatische Verwaltung und Installation des ChromeDrivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e585a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credentials aus der .env Datei laden\n",
    "INSTA_USERNAME= os.getenv(\"INSTA_USERNAME\")\n",
    "INSTA_PASSWORD= os.getenv(\"INSTA_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9727179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging aktiv unter: C:\\Users\\SofiePischl\\Documents\\01_HdM\\10_ML_OPS\\TrendAnalyse Social Media\\logs\\instagram.log\n"
     ]
    }
   ],
   "source": [
    "# Logging initialisieren\n",
    "log_path = Path(\"../logs/instagram.log\")\n",
    "log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "# Logger holen (Root-Logger)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Bestehende Handler entfernen (z.‚ÄØB. alte Datei-Handler)\n",
    "logger.handlers.clear()\n",
    "\n",
    "# Neuen FileHandler hinzuf√ºgen\n",
    "file_handler = logging.FileHandler(log_path)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=log_path,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "print(f\"Logging aktiv unter: {log_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31c0d83",
   "metadata": {},
   "source": [
    "Die folgende Funktion scrape_instagram_explore() automatisiert den Zugriff auf die Instagram-Explore-Seite, um dort eingebettete Beitr√§ge zu extrahieren und zu speichern. Zun√§chst wird ein Headless-Browser mittels Selenium konfiguriert und gestartet, um Instagram ohne sichtbares Browserfenster zu laden. Anschlie√üend erfolgt ein Login mit Benutzerdaten, die aus Umgebungsvariablen gelesen werden. Nach erfolgreicher Anmeldung wird zur Explore-Seite navigiert, wo durch wiederholtes Scrollen weitere Inhalte dynamisch nachgeladen werden. Die so sichtbaren Beitr√§ge werden dann anhand von XPath-Selektoren identifiziert, und von jedem Beitrag werden die URL, das zugeh√∂rige Bild sowie die Bildbeschreibung ausgelesen. Diese Informationen werden zusammen mit einem Zeitstempel gesammelt und in eine CSV-Datei gespeichert. Bestehende Daten in der Datei werden dabei ber√ºcksichtigt und Duplikate entfernt. Die Funktion schlie√üt mit dem Speichern der kombinierten Daten und dem Beenden des Browsers. Fehler w√§hrend des Prozesses werden per Logging dokumentiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c344738",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_instagram_explore():\n",
    "    try:\n",
    "        logging.info(\"Starte Instagram-Explore-Scraping...\")\n",
    "\n",
    "        # Headless-Browser konfigurieren\n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless=new\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--window-size=1920x1080\")\n",
    "\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        driver.get(\"https://www.instagram.com/accounts/login/\")\n",
    "\n",
    "        # Login\n",
    "        username = os.getenv(\"INSTA_USERNAME\")\n",
    "        password = os.getenv(\"INSTA_PASSWORD\")\n",
    "\n",
    "        time.sleep(3)\n",
    "        driver.find_element(By.NAME, \"username\").send_keys(username)\n",
    "        driver.find_element(By.NAME, \"password\").send_keys(password)\n",
    "        driver.find_element(By.XPATH, \"//button[@type='submit']\").click()\n",
    "\n",
    "        time.sleep(7)  # Warte auf Login\n",
    "\n",
    "        # Gehe zur Explore-Seite\n",
    "        driver.get(\"https://www.instagram.com/explore/\")\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Seite scrollen, um mehr Posts zu laden\n",
    "        for _ in range(3):  # 3x scrollen ‚Üí kannst du erh√∂hen\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(3)\n",
    "\n",
    "        # Beitr√§ge extrahieren\n",
    "        posts = driver.find_elements(By.XPATH, '//a[contains(@href, \"/p/\")]')\n",
    "        post_data = []\n",
    "\n",
    "        for post in posts:\n",
    "            try:\n",
    "                url = post.get_attribute(\"href\")\n",
    "                img = post.find_element(By.TAG_NAME, \"img\")\n",
    "                img_url = img.get_attribute(\"src\")\n",
    "                description = img.get_attribute(\"alt\")\n",
    "\n",
    "                post_data.append({\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"url\": url,\n",
    "                    \"image\": img_url,\n",
    "                    \"description\": description\n",
    "                })\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Fehler beim Extrahieren eines Posts: {e}\")\n",
    "                continue\n",
    "\n",
    "        # In CSV speichern\n",
    "        csv_path = Path(\"../app/data/raw/instagram_data.csv\")\n",
    "        csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        df = pd.DataFrame(post_data)\n",
    "\n",
    "        if csv_path.exists():\n",
    "            df_existing = pd.read_csv(csv_path)\n",
    "            df = pd.concat([df_existing, df], ignore_index=True)\n",
    "\n",
    "        df.drop_duplicates(subset=[\"url\", \"image\", \"description\"], inplace=True)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "\n",
    "        logging.info(f\"{len(df)} Eintr√§ge gespeichert unter {csv_path}\")\n",
    "        driver.quit()\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fehler beim Instagram-Explore-Scraping: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e397062e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Instagram-Explore-Scraping...\n"
     ]
    }
   ],
   "source": [
    "# Jetzt ausf√ºhren\n",
    "scrape_instagram_explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f83d79c",
   "metadata": {},
   "source": [
    "Zur gezielten Extraktion von Informationen aus einzelnen Instagram-Posts wurde ein browsergest√ºtzter Ansatz mittels Selenium implementiert. Die Funktion scrape_instagram_post(url) nimmt eine URL zu einem √∂ffentlichen Instagram-Beitrag entgegen und automatisiert den Zugriff darauf. Dazu wird ein Headless-Chrome-Browser im Hintergrund gestartet, um die angegebene Seite zu laden und die dort eingebetteten Inhalte auszulesen. Die Funktion wartet zun√§chst auf das vollst√§ndige Laden des Beitrags und versucht anschlie√üend, ein ggf. erscheinendes Login-Popup automatisiert zu schlie√üen, um eine ungehinderte Datenerfassung zu erm√∂glichen.\n",
    "\n",
    "Es werden folgende Informationen extrahiert: der Benutzername des ver√∂ffentlichenden Accounts, die Bildunterschrift (Caption) sowie ‚Äì sofern vorhanden ‚Äì die Anzahl der Likes. Diese Daten werden zusammen mit der aufgerufenen URL und einem Zeitstempel in strukturierter Form gespeichert. Die Speicherung erfolgt in einer zentralen CSV-Datei, wobei bestehende Daten ber√ºcksichtigt und potenzielle Duplikate entfernt werden. Insgesamt erlaubt dieser Ansatz eine punktuelle Analyse individueller Inhalte, ist jedoch wie andere Selenium-basierte Verfahren anf√§llig f√ºr strukturelle √Ñnderungen in der Instagram-Oberfl√§che, insbesondere bei dynamischen Komponenten und nicht stabilen XPath-Referenzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c60440bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_instagram_post(url):\n",
    "    try:\n",
    "        logging.info(f\"Starte Instagram-Scraping f√ºr URL: {url}\")\n",
    "        \n",
    "        # Zielpfad vorbereiten\n",
    "        csv_path = Path(\"../app/data/raw/instagram_data.csv\")\n",
    "        csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Browser initialisieren\n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless=new\")  # im Hintergrund ausf√ºhren\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--window-size=1920x1080\")\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        wait = WebDriverWait(driver, 15)\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        # Beitrag laden\n",
    "        wait.until(EC.presence_of_element_located((By.TAG_NAME, \"article\")))\n",
    "\n",
    "        # Login-Popup schlie√üen (falls vorhanden)\n",
    "        try:\n",
    "            close_btn = wait.until(EC.element_to_be_clickable((By.XPATH, \"//div[@role='dialog']//button\")))\n",
    "            close_btn.click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Username extrahieren\n",
    "        try:\n",
    "            username_elem = wait.until(EC.visibility_of_element_located(\n",
    "                (By.XPATH, \"//a[contains(@href, '/') and contains(@class, 'notranslate')]//span[1]\")\n",
    "            ))\n",
    "            username = username_elem.text.strip()\n",
    "        except:\n",
    "            username = \"\"\n",
    "\n",
    "        # Caption extrahieren\n",
    "        try:\n",
    "            caption_elem = wait.until(EC.visibility_of_element_located(\n",
    "                (By.XPATH, \"//div[@data-testid='post-comment-root']//span\")\n",
    "            ))\n",
    "            caption = caption_elem.text.strip()\n",
    "        except:\n",
    "            caption = \"\"\n",
    "\n",
    "        # Likes (optional)\n",
    "        try:\n",
    "            likes_elem = wait.until(EC.visibility_of_element_located(\n",
    "                (By.XPATH, \"//section//span[contains(text(), 'Gef√§llt')]\")\n",
    "            ))\n",
    "            likes = likes_elem.text.strip()\n",
    "        except:\n",
    "            likes = \"\"\n",
    "\n",
    "        # Timestamp + Struktur\n",
    "        data = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"url\": url,\n",
    "            \"username\": username,\n",
    "            \"caption\": caption,\n",
    "            \"likes\": likes\n",
    "        }\n",
    "\n",
    "        df_new = pd.DataFrame([data])\n",
    "\n",
    "        if csv_path.exists():\n",
    "            df_existing = pd.read_csv(csv_path)\n",
    "            df_new = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "\n",
    "        df_new.drop_duplicates(subset=[\"url\", \"caption\", \"username\"], inplace=True)\n",
    "        df_new.to_csv(csv_path, index=False)\n",
    "\n",
    "        logging.info(f\"Erfolgreich gespeichert unter {csv_path}\")\n",
    "        driver.quit()\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fehler beim Scrapen von {url}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9f1d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispielaufruf\n",
    "scrape_instagram_post(\"https://www.instagram.com/p/DICWDtYNq7E/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafbbd32",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. TWITTER / X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25574e53",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Versuch mit der offiziellen Twitter API (v2)\n",
    "\n",
    "Zun√§chst wurde ein Zugriff √ºber die offizielle Twitter Developer API (v2) realisiert, wobei ein Bearer Token √ºber ein entsprechendes Developer-Konto eingebunden wurde. Die Verbindung wurde in der Regel erfolgreich hergestellt, jedoch resultierten s√§mtliche Anfragen bereits nach wenigen Requests in einem **HTTP 429 ‚Äì Too Many Requests** Fehler. Selbst bei einer Limitierung auf nur drei Tweets wurde die Anfrage durch die API geblockt. Dies deutet auf eine √§u√üerst restriktive Ratenbegrenzung hin, selbst im Rahmen von ‚ÄûEssential Access‚Äú-Leveln.\n",
    "\n",
    "Au√üerdem sind in der kostenlos erstellbaren Developer App nur 100 Tweets abgreifbar, sodass diese L√∂sung, selbst wenn sie funktionieren w√ºrde, nicht sillf√ºhrend w√§hre.\n",
    "\n",
    "### 2. Nutzung alternativer Scraping-Bibliotheken (z.‚ÄØB. `snscrape`)\n",
    "\n",
    "In einem zweiten Schritt wurde die popul√§re Bibliothek [`snscrape`](https://github.com/JustAnotherArchivist/snscrape) eingesetzt, welche ohne API-Zugang direkt √ºber die √∂ffentliche Webschnittstelle von Twitter (u.‚ÄØa. GraphQL-Endpunkte) operiert. Dieses Vorgehen war fr√ºher eine g√§ngige Methode zur Umgehung von Authentifizierungsbeschr√§nkungen.\n",
    "\n",
    "Aktuell f√ºhrt jedoch auch dieser Ansatz regelm√§√üig zu Fehlern. Konkret wurde mehrfach der folgende Exception ausgel√∂st:\n",
    "\n",
    "```\n",
    "ScraperException: 4 requests to .../SearchTimeline failed, giving up.\n",
    "```\n",
    "\n",
    "Diese Meldung ist ein Hinweis darauf, dass Twitter die internen API-Endpunkte (GraphQL) entweder modifiziert oder stark abgesichert hat. `snscrape` kann infolgedessen die erwarteten Objekte nicht mehr extrahieren. Entsprechende Issues sind auch in der GitHub-Community der Bibliothek dokumentiert, eine stabile L√∂sung existiert derzeit nicht.\n",
    "\n",
    "\n",
    "\n",
    "### 3. Headless-Scraping mit Playwright\n",
    "\n",
    "Ein weiterer Ansatz zur Umgehung offizieller API-Beschr√§nkungen bestand im Einsatz der Automatisierungsbibliothek [`Playwright`](https://playwright.dev/python/), die √ºber einen echten Chromium-Browser Webinhalte visuell bzw. DOM-basiert extrahieren kann. Diese Methode ist grunds√§tzlich vielversprechend, da sie wie ein menschlicher Nutzer agiert und damit keine API-Limits verletzt.\n",
    "\n",
    "Im konkreten Fall wurde Playwright zun√§chst als **synchrones Skript** innerhalb eines Jupyter Notebooks getestet. Dabei trat jedoch ein technisches Problem auf: Da Jupyter selbst auf einem laufenden `asyncio`-Event-Loop basiert, ist die Verwendung von `sync_playwright()` nicht erlaubt. Dies f√ºhrte unmittelbar zu folgender Fehlermeldung:\n",
    "\n",
    "```\n",
    "Error: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
    "```\n",
    "\n",
    "Als Reaktion darauf wurde auf die empfohlene **asynchrone API-Variante** (`async_playwright()`) umgestellt. Auch dieser Ansatz scheiterte jedoch ‚Äì insbesondere unter Windows ‚Äì mit folgendem Fehler:\n",
    "\n",
    "```\n",
    "NotImplementedError: asyncio subprocess transport is not implemented\n",
    "```\n",
    "\n",
    "Diese Meldung verweist auf eine bekannte Einschr√§nkung der Playwright-Implementierung im Zusammenspiel mit dem Event-Loop von Windows und Jupyter. Der Start des internen Chromium-Drivers schl√§gt aufgrund fehlender Unterst√ºtzung f√ºr `asyncio.create_subprocess_exec()` fehl. Damit ist Playwright im Rahmen einer Notebook-Umgebung faktisch **nicht lauff√§hig**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2717e9e3",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. TikTok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8604b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TikTokApi ist eine Bibliothek zum Abrufen von TikTok-Daten (inoffizielle API)\n",
    "from TikTokApi import TikTokApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6cff74e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token f√ºr die Session ‚Äì muss aus Cookies der TikTok website gezogen werden\n",
    "ms_token = os.getenv(\"MS_TOKEN\")  \n",
    "\n",
    "# Pfad zur Ausgabedatei (CSV), Standardwert: \"trending_videos.csv\"\n",
    "csv_path = os.getenv(\"OUTPUT_PATH\", \"trending_videos.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef890c15",
   "metadata": {},
   "source": [
    "Der nachfolgende Code nutzt nun die inoffizielle TikTokApi, um die aktuell beliebtesten TikTok-Videos (Trending-Videos) automatisiert abzurufen. Mithilfe des g√ºltigen ms_token (Session-Token) wird eine Browser-Sitzung erstellt, die den Zugriff auf √∂ffentliche TikTok-Inhalte erm√∂glicht. Die Funktion extrahiert Informationen zu 30 popul√§ren Videos ‚Äì darunter Beschreibung, Autor, Likes, Views und URL ‚Äì und speichert die Ergebnisse in einer CSV-Datei. Die gesamte Logik l√§uft asynchron ab, um die Effizienz bei der Netzwerkkommunikation zu verbessern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1415a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def trending_videos():\n",
    "    # TikTok-API-Session asynchron starten\n",
    "    async with TikTokApi() as api:\n",
    "        await api.create_sessions(\n",
    "            ms_tokens=[ms_token],  # Session-Token f√ºr Authentifizierung\n",
    "            num_sessions=1,        # Anzahl gleichzeitiger Sessions\n",
    "            sleep_after=3,         # Wartezeit zwischen Session-Anfragen\n",
    "            browser=os.getenv(\"TIKTOK_BROWSER\", \"chromium\")  # Standardbrowser\n",
    "        )\n",
    "\n",
    "        data = []\n",
    "\n",
    "        # Schleife √ºber die 30 beliebtesten Trending-Videos\n",
    "        async for video in api.trending.videos(count=30):\n",
    "            info = video.as_dict  # Rohdaten des Videos als Dictionary\n",
    "\n",
    "            # Relevante Felder extrahieren und speichern\n",
    "            data.append({\n",
    "                \"id\": info.get(\"id\"),\n",
    "                \"description\": info.get(\"desc\"),\n",
    "                \"author_username\": info.get(\"author\", {}).get(\"uniqueId\"),\n",
    "                \"author_id\": info.get(\"author\", {}).get(\"id\"),\n",
    "                \"likes\": info.get(\"stats\", {}).get(\"diggCount\"),\n",
    "                \"shares\": info.get(\"stats\", {}).get(\"shareCount\"),\n",
    "                \"comments\": info.get(\"stats\", {}).get(\"commentCount\"),\n",
    "                \"plays\": info.get(\"stats\", {}).get(\"playCount\"),\n",
    "                \"video_url\": info.get(\"video\", {}).get(\"downloadAddr\"),\n",
    "                \"created_time\": info.get(\"createTime\"),\n",
    "            })\n",
    "\n",
    "        # Sicherstellen, dass der Zielordner existiert\n",
    "        os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "\n",
    "        # Ergebnisse in CSV-Datei schreiben (append-Modus)\n",
    "        file_exists = os.path.isfile(csv_path)\n",
    "        with open(csv_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=data[0].keys())\n",
    "\n",
    "            if not file_exists:\n",
    "                writer.writeheader()  # Schreibe Header nur, wenn Datei neu ist\n",
    "            writer.writerows(data)   # Anh√§ngen der neuen Zeilen\n",
    "\n",
    "        print(f\"\\n‚úÖ Erfolgreich {len(data)} Videos in '{csv_path}' gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc307378",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. YouTube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ab09fe",
   "metadata": {},
   "source": [
    "Um YouTube zu scrapen wird die offizielle YouTube Data API v3 genutzt, um die derzeit beliebtesten Videos (Trending) aus einer bestimmten Region abzurufen. Es extrahiert Titel, Beschreibung, Kanalname, Ver√∂ffentlichungsdatum sowie Statistiken wie Aufrufe, Likes und Kommentare. Die Daten werden als Pandas DataFrame verarbeitet und in einer CSV-Datei gespeichert. Bestehende Daten werden ber√ºcksichtigt, Duplikate entfernt und das Ergebnis ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a1a7fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#offizieller Google API Client f√ºr Python\n",
    "from googleapiclient.discovery import build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3984f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Set-up\n",
    "API_KEY = os.getenv(\"YT_KEY\")  # API-Key aus Umgebungsvariablen\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)  # YouTube-API-Client erzeugen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc6c0041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_youtube_trending(region=\"DE\", max_results=50):\n",
    "    try:\n",
    "        print(\"üîç Starte YouTube-Scraping...\")\n",
    "\n",
    "        # API-Anfrage vorbereiten (meistgesehene Videos in gegebener Region)\n",
    "        request = youtube.videos().list(\n",
    "            part=\"snippet,statistics\",\n",
    "            chart=\"mostPopular\",\n",
    "            regionCode=region,\n",
    "            maxResults=max_results\n",
    "        )\n",
    "        response = request.execute()  # Anfrage ausf√ºhren\n",
    "\n",
    "        videos = []\n",
    "        scrape_time = datetime.now()  # Zeitstempel des Abrufs\n",
    "\n",
    "        # Schleife √ºber alle zur√ºckgegebenen Videos\n",
    "        for item in response.get(\"items\", []):\n",
    "            snippet = item[\"snippet\"]\n",
    "            stats = item.get(\"statistics\", {})\n",
    "\n",
    "            videos.append({\n",
    "                \"video_id\": item[\"id\"],\n",
    "                \"title\": snippet.get(\"title\"),\n",
    "                \"description\": snippet.get(\"description\"),\n",
    "                \"channel_title\": snippet.get(\"channelTitle\"),\n",
    "                \"published_at\": snippet.get(\"publishedAt\"),\n",
    "                \"view_count\": stats.get(\"viewCount\"),\n",
    "                \"like_count\": stats.get(\"likeCount\"),\n",
    "                \"comment_count\": stats.get(\"commentCount\"),\n",
    "                \"url\": f\"https://www.youtube.com/watch?v={item['id']}\",\n",
    "                \"scraped_at\": scrape_time\n",
    "            })\n",
    "\n",
    "        # In DataFrame umwandeln\n",
    "        df = pd.DataFrame(videos)\n",
    "\n",
    "        # CSV-Pfad vorbereiten\n",
    "        csv_path = Path(\"../app/data/raw/youtube_data.csv\")\n",
    "        csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Falls Datei bereits existiert ‚Üí zusammenf√ºhren\n",
    "        if csv_path.exists():\n",
    "            df_existing = pd.read_csv(csv_path)\n",
    "            df = pd.concat([df_existing, df], ignore_index=True)\n",
    "\n",
    "        # Duplikate entfernen und speichern\n",
    "        df.drop_duplicates(subset=[\"video_id\"], inplace=True)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "\n",
    "        # Vorschau der Daten anzeigen\n",
    "        display(df.head())\n",
    "\n",
    "        print(f\"‚úÖ {len(df)} Videos gespeichert unter: {csv_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fehler beim YouTube-Scraping: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1069a8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Starte YouTube-Scraping...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>published_at</th>\n",
       "      <th>view_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>url</th>\n",
       "      <th>scraped_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mLpa3zzrVyI</td>\n",
       "      <td>Inter Mailand 4:3 FC Barcelona | Highlights - ...</td>\n",
       "      <td>Viel Spa√ü mit den Highlights des Spiels Inter ...</td>\n",
       "      <td>Prime Video Sport Deutschland</td>\n",
       "      <td>2025-05-06T21:48:35Z</td>\n",
       "      <td>1375723</td>\n",
       "      <td>25099</td>\n",
       "      <td>2585</td>\n",
       "      <td>https://www.youtube.com/watch?v=mLpa3zzrVyI</td>\n",
       "      <td>2025-05-07 14:12:27.972453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nRw_GB_78xQ</td>\n",
       "      <td>Drei Festnahmen &amp; eine Waffe | Nachtschicht mi...</td>\n",
       "      <td>WERBUNG Mit dem Code HEYAARON kann Finanzguru ...</td>\n",
       "      <td>Hey Aaron!!!</td>\n",
       "      <td>2025-05-06T16:01:28Z</td>\n",
       "      <td>319867</td>\n",
       "      <td>17503</td>\n",
       "      <td>338</td>\n",
       "      <td>https://www.youtube.com/watch?v=nRw_GB_78xQ</td>\n",
       "      <td>2025-05-07 14:12:27.972453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SJP2M5IumRc</td>\n",
       "      <td>Merz scheitert im 1. Wahlgang - und ist jetzt ...</td>\n",
       "      <td>Friedrich Merz ist neuer Kanzler der Bundesrep...</td>\n",
       "      <td>MrWissen2go</td>\n",
       "      <td>2025-05-06T15:21:05Z</td>\n",
       "      <td>430465</td>\n",
       "      <td>20861</td>\n",
       "      <td>3085</td>\n",
       "      <td>https://www.youtube.com/watch?v=SJP2M5IumRc</td>\n",
       "      <td>2025-05-07 14:12:27.972453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m5GRR4D23k0</td>\n",
       "      <td>Was ich euch sagen m√∂chte...</td>\n",
       "      <td>Mein zweiter Kanal: https://www.youtube.com/@A...</td>\n",
       "      <td>SkylineTV</td>\n",
       "      <td>2025-05-06T15:15:05Z</td>\n",
       "      <td>129142</td>\n",
       "      <td>9029</td>\n",
       "      <td>1812</td>\n",
       "      <td>https://www.youtube.com/watch?v=m5GRR4D23k0</td>\n",
       "      <td>2025-05-07 14:12:27.972453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HwMeliz7paA</td>\n",
       "      <td>IN 7 TAGEN 491 KM üèÉüèª‚Äç‚ôÇÔ∏èü§Ø| I run the full lengt...</td>\n",
       "      <td>Instagram: https://www.instagram.com/ardasaatc...</td>\n",
       "      <td>Arda Saat√ßi</td>\n",
       "      <td>2025-05-06T16:59:06Z</td>\n",
       "      <td>286018</td>\n",
       "      <td>27371</td>\n",
       "      <td>992</td>\n",
       "      <td>https://www.youtube.com/watch?v=HwMeliz7paA</td>\n",
       "      <td>2025-05-07 14:12:27.972453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                              title  \\\n",
       "0  mLpa3zzrVyI  Inter Mailand 4:3 FC Barcelona | Highlights - ...   \n",
       "1  nRw_GB_78xQ  Drei Festnahmen & eine Waffe | Nachtschicht mi...   \n",
       "2  SJP2M5IumRc  Merz scheitert im 1. Wahlgang - und ist jetzt ...   \n",
       "3  m5GRR4D23k0                       Was ich euch sagen m√∂chte...   \n",
       "4  HwMeliz7paA  IN 7 TAGEN 491 KM üèÉüèª‚Äç‚ôÇÔ∏èü§Ø| I run the full lengt...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Viel Spa√ü mit den Highlights des Spiels Inter ...   \n",
       "1  WERBUNG Mit dem Code HEYAARON kann Finanzguru ...   \n",
       "2  Friedrich Merz ist neuer Kanzler der Bundesrep...   \n",
       "3  Mein zweiter Kanal: https://www.youtube.com/@A...   \n",
       "4  Instagram: https://www.instagram.com/ardasaatc...   \n",
       "\n",
       "                   channel_title          published_at view_count like_count  \\\n",
       "0  Prime Video Sport Deutschland  2025-05-06T21:48:35Z    1375723      25099   \n",
       "1                   Hey Aaron!!!  2025-05-06T16:01:28Z     319867      17503   \n",
       "2                    MrWissen2go  2025-05-06T15:21:05Z     430465      20861   \n",
       "3                      SkylineTV  2025-05-06T15:15:05Z     129142       9029   \n",
       "4                    Arda Saat√ßi  2025-05-06T16:59:06Z     286018      27371   \n",
       "\n",
       "  comment_count                                          url  \\\n",
       "0          2585  https://www.youtube.com/watch?v=mLpa3zzrVyI   \n",
       "1           338  https://www.youtube.com/watch?v=nRw_GB_78xQ   \n",
       "2          3085  https://www.youtube.com/watch?v=SJP2M5IumRc   \n",
       "3          1812  https://www.youtube.com/watch?v=m5GRR4D23k0   \n",
       "4           992  https://www.youtube.com/watch?v=HwMeliz7paA   \n",
       "\n",
       "                  scraped_at  \n",
       "0 2025-05-07 14:12:27.972453  \n",
       "1 2025-05-07 14:12:27.972453  \n",
       "2 2025-05-07 14:12:27.972453  \n",
       "3 2025-05-07 14:12:27.972453  \n",
       "4 2025-05-07 14:12:27.972453  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 50 Videos gespeichert unter: ..\\app\\data\\raw\\youtube_data.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    scrape_youtube_trending()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bafb10",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Fazit\n",
    "\n",
    "Das Scraping f√ºr die Plattformen **Reddit**, **TikTok** und **YouTube** wurde erfolgreich implementiert. F√ºr jede dieser Plattformen wurden funktionierende Ans√§tze entwickelt, die auf √∂ffentlich zug√§ngliche APIs oder geeignete inoffizielle Schnittstellen zur√ºckgreifen. Die lauff√§higen und getesteten Skripte befinden sich im Verzeichnis **`src/1_scheduler/jobs`** und k√∂nnen dort f√ºr die regelm√§√üige Datenerhebung genutzt werden.\n",
    "F√ºr **X (ehemals Twitter)** und **Instagram** konnte hingegen kein stabiles und zuverl√§ssiges Scraping-Verfahren umgesetzt werden, da beide Plattformen den Zugriff durch technische und rechtliche Ma√ünahmen erheblich einschr√§nken.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
