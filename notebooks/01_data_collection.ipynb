{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bdbda90",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 16px;\">\n",
    "    <strong>Course:</strong> Machine Learning Operations |\n",
    "    <strong>Lecturer:</strong> Prof. Dr. Klotz |\n",
    "    <strong>Date:</strong> 17.05.2025 |\n",
    "    <strong>Name:</strong> Sofie Pischl\n",
    "</div>\n",
    "\n",
    "# <center>Data Collection </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a95362a",
   "metadata": {},
   "source": [
    "Konzept & Inhalt:\n",
    "\n",
    "Daten von den gr√∂√üten Social media Apps sollen abgegriffen werden. besonderer Fokus auf texten.\n",
    "\n",
    "1. Reddit\n",
    "2. Instagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96b79d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load from .env file\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa66e0a",
   "metadata": {},
   "source": [
    "# 1. Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f757739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>comments</th>\n",
       "      <th>created</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>books</td>\n",
       "      <td>Weekly FAQ Thread March 09, 2025: What are the...</td>\n",
       "      <td>Hello readers and welcome to our Weekly FAQ th...</td>\n",
       "      <td>54</td>\n",
       "      <td>96</td>\n",
       "      <td>2025-03-09 12:00:31</td>\n",
       "      <td>https://www.reddit.com/r/books/comments/1j75ax...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>books</td>\n",
       "      <td>Weekly FAQ Thread April 06, 2025: What are you...</td>\n",
       "      <td>Hello readers and welcome to our Weekly FAQ th...</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>2025-04-06 13:00:31</td>\n",
       "      <td>https://www.reddit.com/r/books/comments/1jsrko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>books</td>\n",
       "      <td>Romance author Ali Hazelwood cancels UK tours ...</td>\n",
       "      <td></td>\n",
       "      <td>6476</td>\n",
       "      <td>423</td>\n",
       "      <td>2025-04-05 18:55:34</td>\n",
       "      <td>https://www.usatoday.com/story/entertainment/b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>books</td>\n",
       "      <td>I want to buy new books at the book fair and b...</td>\n",
       "      <td>In Japanese, there's a word for people who buy...</td>\n",
       "      <td>213</td>\n",
       "      <td>357</td>\n",
       "      <td>2025-04-06 12:48:15</td>\n",
       "      <td>https://www.reddit.com/r/books/comments/1jsre8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>books</td>\n",
       "      <td>'Oliver and Amanda Pig' series author Jean Van...</td>\n",
       "      <td></td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-06 12:09:48</td>\n",
       "      <td>https://www.npr.org/2025/04/05/nx-s1-5353478/o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit                                              title  \\\n",
       "0     books  Weekly FAQ Thread March 09, 2025: What are the...   \n",
       "1     books  Weekly FAQ Thread April 06, 2025: What are you...   \n",
       "2     books  Romance author Ali Hazelwood cancels UK tours ...   \n",
       "3     books  I want to buy new books at the book fair and b...   \n",
       "4     books  'Oliver and Amanda Pig' series author Jean Van...   \n",
       "\n",
       "                                                text  score  comments  \\\n",
       "0  Hello readers and welcome to our Weekly FAQ th...     54        96   \n",
       "1  Hello readers and welcome to our Weekly FAQ th...     13        17   \n",
       "2                                                      6476       423   \n",
       "3  In Japanese, there's a word for people who buy...    213       357   \n",
       "4                                                        48         1   \n",
       "\n",
       "              created                                                url  \n",
       "0 2025-03-09 12:00:31  https://www.reddit.com/r/books/comments/1j75ax...  \n",
       "1 2025-04-06 13:00:31  https://www.reddit.com/r/books/comments/1jsrko...  \n",
       "2 2025-04-05 18:55:34  https://www.usatoday.com/story/entertainment/b...  \n",
       "3 2025-04-06 12:48:15  https://www.reddit.com/r/books/comments/1jsre8...  \n",
       "4 2025-04-06 12:09:48  https://www.npr.org/2025/04/05/nx-s1-5353478/o...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# üîë Authentifizierung (trage deine echten Keys ein)\n",
    "reddit = praw.Reddit(\n",
    "    client_id=(os.getenv(\"REDDIT_ID\")),\n",
    "    client_secret=(os.getenv(\"REDDIT_SECRET\")),\n",
    "    user_agent=(os.getenv(\"USER_AGENT\"))\n",
    ")\n",
    "\n",
    "# üîç Subreddits und Query definieren\n",
    "subreddits = [\"books\", \"writing\", \"technology\", \"future\"]\n",
    "post_limit = 100\n",
    "\n",
    "all_posts = []\n",
    "\n",
    "for sub in subreddits:\n",
    "    subreddit = reddit.subreddit(sub)\n",
    "    for post in subreddit.hot(limit=post_limit):\n",
    "        all_posts.append({\n",
    "            \"subreddit\": sub,\n",
    "            \"title\": post.title,\n",
    "            \"text\": post.selftext,\n",
    "            \"score\": post.score,\n",
    "            \"comments\": post.num_comments,\n",
    "            \"created\": datetime.fromtimestamp(post.created),\n",
    "            \"url\": post.url\n",
    "        })\n",
    "\n",
    "# üìÑ In DataFrame umwandeln & speichern\n",
    "df = pd.DataFrame(all_posts)\n",
    "df.to_csv(\"../data/raw/reddit_data.csv\", index=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8152499",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Instagram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125499c",
   "metadata": {},
   "source": [
    "was nicht funktioniert hat: instaloader, playwright"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4076b412",
   "metadata": {},
   "source": [
    "step 1: log in & extract urls from for you page\n",
    "\n",
    "step 2: Look up urls without beeing signed in and extract captions & content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb71d64",
   "metadata": {},
   "source": [
    "setp 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa05b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e7df8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log in data\n",
    "USERNAME =  os.getenv(\"INSTA_USERNAME\")\n",
    "PASSWORD =  os.getenv(\"INSTA_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd225ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 20 URLs to 'explore_results.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Start browser and login\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "wait = WebDriverWait(driver, 15)\n",
    "\n",
    "driver.get(\"https://www.instagram.com/accounts/login/\")\n",
    "time.sleep(3)\n",
    "\n",
    "# Accept cookies (if shown)\n",
    "try:\n",
    "    decline_button = wait.until(EC.element_to_be_clickable(\n",
    "        (By.XPATH, '//button[contains(text(), \"Nur essentielle Cookies erlauben\") or contains(text(), \"Decline optional cookies\")]')))\n",
    "    decline_button.click()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Fill in login form\n",
    "wait.until(EC.presence_of_element_located((By.NAME, 'username')))\n",
    "driver.find_element(By.NAME, 'username').send_keys(USERNAME)\n",
    "driver.find_element(By.NAME, 'password').send_keys(PASSWORD)\n",
    "driver.find_element(By.NAME, 'password').send_keys(Keys.RETURN)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Go to Explore page\n",
    "driver.get(\"https://www.instagram.com/explore/\")\n",
    "time.sleep(5)\n",
    "\n",
    "# Scroll and collect post URLs\n",
    "post_urls = set()\n",
    "scrolls = 0\n",
    "while len(post_urls) < 20 and scrolls < 10:\n",
    "    elements = driver.find_elements(By.XPATH, \"//a[contains(@href, '/p/')]\")\n",
    "    for elem in elements:\n",
    "        href = elem.get_attribute(\"href\")\n",
    "        if href and href.startswith(\"https://www.instagram.com/p/\"):\n",
    "            post_urls.add(href)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "    scrolls += 1\n",
    "\n",
    "# Save URLs to CSV\n",
    "df = pd.DataFrame({\"Post URL\": list(post_urls)})\n",
    "df.to_csv(\"../raw/explore_results.csv\", index=False)\n",
    "print(f\"‚úÖ Saved {len(post_urls)} URLs to 'explore_results.csv'.\")\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31d210c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Beitrag nicht verf√ºgbar oder 18+: https://www.instagram.com/p/DIKaU11Rwj0/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === CONFIGURATION ===\n",
    "csv_path = \"../data/raw/explore_results.csv\"\n",
    "\n",
    "# === READ CSV AND PREPARE DATAFRAME ===\n",
    "# Ensure columns are interpreted as strings\n",
    "df = pd.read_csv(csv_path, dtype={\n",
    "    \"timestamp\": \"string\",\n",
    "    \"datum\": \"string\",\n",
    "    \"inhalt\": \"string\",\n",
    "    \"username\": \"string\",\n",
    "    \"caption\": \"string\",\n",
    "    \"likes\": \"string\"\n",
    "})\n",
    "\n",
    "# Ensure necessary columns exist\n",
    "required_columns = [\"timestamp\", \"datum\", \"inhalt\", \"username\", \"caption\", \"likes\"]\n",
    "for col in required_columns:\n",
    "    if col not in df.columns:\n",
    "        df[col] = pd.Series(dtype=\"string\")\n",
    "\n",
    "# === SETUP SELENIUM BROWSER ===\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "wait = WebDriverWait(driver, 15)\n",
    "\n",
    "# === DISMISS POPUPS ===\n",
    "def dismiss_popups():\n",
    "    # Cookie-Banner\n",
    "    cookie_buttons = [\n",
    "        '//button[text()=\"Nur essentielle Cookies erlauben\"]',\n",
    "        '//button[text()=\"Decline optional cookies\"]',\n",
    "    ]\n",
    "\n",
    "    # Login-Overlay schlie√üen\n",
    "    login_close_xpaths = [\n",
    "        '//div[@role=\"dialog\"]//div[@aria-label=\"Schlie√üen\"]',\n",
    "        '//div[@role=\"dialog\"]//div[@aria-label=\"Close\"]',\n",
    "        '//button[@aria-label=\"Schlie√üen\"]',\n",
    "        '//button[@aria-label=\"Close\"]',\n",
    "    ]\n",
    "\n",
    "    for xpath in cookie_buttons + login_close_xpaths:\n",
    "        try:\n",
    "            btn = WebDriverWait(driver, 3).until(EC.element_to_be_clickable((By.XPATH, xpath)))\n",
    "            btn.click()\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# === MAIN LOOP: PROCESS EACH POST ===\n",
    "for i, row in df.iterrows():\n",
    "    url = row['Post URL']\n",
    "    if pd.notna(row.get(\"timestamp\")):\n",
    "        continue  # Skip already processed rows\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "        dismiss_popups()\n",
    "\n",
    "        # Check if article appears ‚Äì if not: skip (possibly 18+ or deleted)\n",
    "        try:\n",
    "            wait.until(EC.presence_of_element_located((By.TAG_NAME, \"article\")))\n",
    "        except:\n",
    "            print(f\" Beitrag nicht verf√ºgbar oder 18+: {url}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Username\n",
    "        try:\n",
    "            username_elem = driver.find_element(By.XPATH, \"//a[contains(@href, '/') and contains(@class, 'notranslate')]//span\").text.strip()\n",
    "            username = username_elem.text.strip()\n",
    "        except:\n",
    "            username = \"\"\n",
    "        \n",
    "        # Caption (try newer & older structure)\n",
    "        try:\n",
    "            all_spans = driver.find_elements(By.XPATH, \"//div[contains(@class, '_a9zs')]/span\").text.strip()\n",
    "            caption = \"\"\n",
    "            for span in all_spans:\n",
    "                text = span.text.strip()\n",
    "                if text and username not in text and not re.search(r\"Gef√§llt|Kommentieren|Speichern\", text):\n",
    "                    caption = text\n",
    "                    break\n",
    "        except:\n",
    "            caption = \"\"\n",
    "\n",
    "        # Alt-Text (Image Description)\n",
    "        try:\n",
    "            img = driver.find_element(By.XPATH, \"//article//img\")\n",
    "            image_alt = img.get_attribute(\"alt\").strip()\n",
    "        except:\n",
    "            image_alt = \"\"\n",
    "\n",
    "        # Datum\n",
    "        try:\n",
    "            date_elem = driver.find_element(By.XPATH, \"//time\")\n",
    "            post_date = date_elem.get_attribute(\"datetime\")[:10]\n",
    "        except:\n",
    "            post_date = \"\"\n",
    "\n",
    "        # Likes (only if visible)\n",
    "        try:\n",
    "            likes_elem = driver.find_element(By.XPATH, \"//section//span[contains(text(),'Gef√§llt')]\").text.strip()\n",
    "            likes_text = likes_elem.text\n",
    "            likes_number = re.findall(r\"\\d[\\d\\.\\,]*\", likes_text)\n",
    "            likes = int(likes_number[0].replace(\".\", \"\").replace(\",\", \"\")) if likes_number else \"\"\n",
    "        except:\n",
    "            likes = \"\"\n",
    "\n",
    "        # Save to DataFrame\n",
    "        df.at[i, \"timestamp\"] = datetime.now().isoformat()\n",
    "        df.at[i, \"datum\"] = post_date\n",
    "        df.at[i, \"inhalt\"] = image_alt\n",
    "        df.at[i, \"username\"] = username\n",
    "        df.at[i, \"caption\"] = caption\n",
    "        df.at[i, \"likes\"] = str(likes)\n",
    "\n",
    "        print(f\"‚úÖ Saved: {username} | {caption[:30]}...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with {url}: {e}\")\n",
    "        continue\n",
    "\n",
    "# === FINAL SAVE ===\n",
    "df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "driver.quit()\n",
    "print(\"‚úÖ All data saved to explore_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "602447ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post URL</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>datum</th>\n",
       "      <th>inhalt</th>\n",
       "      <th>username</th>\n",
       "      <th>caption</th>\n",
       "      <th>likes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.instagram.com/p/DIKaU11Rwj0/</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.instagram.com/p/DHW6TrpR9UE/</td>\n",
       "      <td>2025-04-14T12:47:05.156202</td>\n",
       "      <td>2025-03-18</td>\n",
       "      <td>Photo by BernieGirl on March 18, 2025. Ist m√∂g...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.instagram.com/p/DHOLtvLtJKL/</td>\n",
       "      <td>2025-04-14T12:47:26.154501</td>\n",
       "      <td>2025-03-15</td>\n",
       "      <td>Photo by Die Welt hinter der Leinwand on March...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.instagram.com/p/DH-iv39t-RD/</td>\n",
       "      <td>2025-04-14T12:47:55.882011</td>\n",
       "      <td>2025-04-03</td>\n",
       "      <td>aiwonderlab.eu's profile picture</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.instagram.com/p/DIJwgPKpxTN/</td>\n",
       "      <td>2025-04-14T12:48:16.988772</td>\n",
       "      <td>2025-04-07</td>\n",
       "      <td>sonya_styless's profile picture</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Post URL                   timestamp  \\\n",
       "0  https://www.instagram.com/p/DIKaU11Rwj0/                        <NA>   \n",
       "1  https://www.instagram.com/p/DHW6TrpR9UE/  2025-04-14T12:47:05.156202   \n",
       "2  https://www.instagram.com/p/DHOLtvLtJKL/  2025-04-14T12:47:26.154501   \n",
       "3  https://www.instagram.com/p/DH-iv39t-RD/  2025-04-14T12:47:55.882011   \n",
       "4  https://www.instagram.com/p/DIJwgPKpxTN/  2025-04-14T12:48:16.988772   \n",
       "\n",
       "        datum                                             inhalt username  \\\n",
       "0        <NA>                                               <NA>     <NA>   \n",
       "1  2025-03-18  Photo by BernieGirl on March 18, 2025. Ist m√∂g...     <NA>   \n",
       "2  2025-03-15  Photo by Die Welt hinter der Leinwand on March...     <NA>   \n",
       "3  2025-04-03                   aiwonderlab.eu's profile picture     <NA>   \n",
       "4  2025-04-07                    sonya_styless's profile picture     <NA>   \n",
       "\n",
       "  caption likes  \n",
       "0    <NA>  <NA>  \n",
       "1    <NA>  <NA>  \n",
       "2    <NA>  <NA>  \n",
       "3    <NA>  <NA>  \n",
       "4    <NA>  <NA>  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafbbd32",
   "metadata": {},
   "source": [
    "# 1. TWITTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0abcb5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c082e7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8909893b",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Request failed: 429\n{\"title\":\"Too Many Requests\",\"detail\":\"Too Many Requests\",\"type\":\"about:blank\",\"status\":429}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Check response\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Extract data\u001b[39;00m\n",
      "\u001b[1;31mException\u001b[0m: Request failed: 429\n{\"title\":\"Too Many Requests\",\"detail\":\"Too Many Requests\",\"type\":\"about:blank\",\"status\":429}"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Your bearer token from Twitter Developer Portal\n",
    "BEARER_TOKEN = os.getenv(\"X_BEARER_TOKEN\")\n",
    "\n",
    "# Twitter API endpoint for recent tweets\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "\n",
    "# Search parameters ‚Äì open topic\n",
    "query_params = {\n",
    "    'query': 'lang:de -is:retweet',  # No keyword, just German tweets\n",
    "    'max_results': 50,  # Max per request (10‚Äì100)\n",
    "    'tweet.fields': 'created_at,public_metrics,text,author_id',\n",
    "    'expansions': 'author_id',\n",
    "    'user.fields': 'username,name'\n",
    "}\n",
    "\n",
    "# Set headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {BEARER_TOKEN}\"\n",
    "}\n",
    "\n",
    "# Send request\n",
    "response = requests.get(search_url, headers=headers, params=query_params)\n",
    "\n",
    "# Check response\n",
    "if response.status_code != 200:\n",
    "    raise Exception(f\"Request failed: {response.status_code}\\n{response.text}\")\n",
    "\n",
    "data = response.json()\n",
    "\n",
    "# Extract data\n",
    "tweets = data.get(\"data\", [])\n",
    "users = {u[\"id\"]: u for u in data.get(\"includes\", {}).get(\"users\", [])}\n",
    "\n",
    "# Prepare data rows\n",
    "results = []\n",
    "for tweet in tweets:\n",
    "    user = users.get(tweet[\"author_id\"], {})\n",
    "    metrics = tweet.get(\"public_metrics\", {})\n",
    "    results.append({\n",
    "        \"url\": f\"https://twitter.com/{user.get('username')}/status/{tweet['id']}\",\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"datum\": tweet.get(\"created_at\", \"\"),\n",
    "        \"username\": user.get(\"username\", \"\"),\n",
    "        \"name\": user.get(\"name\", \"\"),\n",
    "        \"caption\": tweet.get(\"text\", \"\"),\n",
    "        \"likes\": metrics.get(\"like_count\", 0),\n",
    "        \"retweets\": metrics.get(\"retweet_count\", 0),\n",
    "        \"replies\": metrics.get(\"reply_count\", 0)\n",
    "    })\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(results)\n",
    "df.sort_values(by=\"likes\", ascending=False, inplace=True)  # Sort by popularity\n",
    "df.to_csv(\"../raw/twitter_api_top_tweets.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Saved {len(df)} tweets to twitter_api_top_tweets.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
