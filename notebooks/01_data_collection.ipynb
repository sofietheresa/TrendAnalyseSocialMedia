{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bdbda90",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 16px;\">\n",
    "    <strong>Course:</strong> Machine Learning Operations |\n",
    "    <strong>Lecturer:</strong> Prof. Dr. Klotz |\n",
    "    <strong>Date:</strong> 17.05.2025 |\n",
    "    <strong>Name:</strong> Sofie Pischl\n",
    "</div>\n",
    "\n",
    "# <center>Data Collection </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a95362a",
   "metadata": {},
   "source": [
    "Konzept & Inhalt:\n",
    "\n",
    "Daten von den gr√∂√üten Social media Apps sollen abgegriffen werden. besonderer Fokus auf Texten.\n",
    "\n",
    "1. Setup & Imports\n",
    "2. Reddit: Hot Posts aus Subreddits\n",
    "3. Instagram: Top Posts per Scraping/API (light)\n",
    "4. Twitter: Aktuelle Tweets via snscrape oder Tweepy\n",
    "5. TikTok: Trending Videos\n",
    "6. YouTube: Trending Videos (API/Scraping)\n",
    "7. Fazit & Learnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b780b59",
   "metadata": {},
   "source": [
    "----\n",
    "# 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dceb2f7",
   "metadata": {},
   "source": [
    "Zun√§chst werden alle ben√∂tigten Libraries importiert:\n",
    "- `praw` f√ºr den Reddit-Zugriff\n",
    "- `pandas` f√ºr Datenverarbeitung\n",
    "- `datetime` f√ºr Timestamps\n",
    "- `dotenv` f√ºr Umgebungsvariablen\n",
    "- `pathlib` f√ºr saubere Pfadangaben\n",
    "- `logging` f√ºr Fehlerprotokollierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96b79d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Load from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa66e0a",
   "metadata": {},
   "source": [
    "# 2. Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089b93d3",
   "metadata": {},
   "source": [
    "### Funktionen:\n",
    "- Authentifizierung √ºber OAuth2 via `praw`\n",
    "- Abruf der `hot`-Beitr√§ge aus ausgew√§hlten Subreddits\n",
    "- Speicherung als `.csv` unter `/data/raw/reddit_data.csv`\n",
    "- Fehler-Handling und Logging integriert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d039ab76",
   "metadata": {},
   "source": [
    "**Authentifizierung**\n",
    "\n",
    "Zur Authentifizierung an der Reddit-API wird ein Reddit-Objekt der Bibliothek praw initialisiert. Die ben√∂tigten Zugangsdaten ‚Äì client_id, client_secret und user_agent ‚Äì werden aus einer .env-Datei geladen, um die Trennung von Code und Konfiguration zu gew√§hrleisten und Sicherheitsrisiken zu minimieren.\n",
    "\n",
    "Diese Parameter dienen der eindeutigen Identifikation der Anwendung gegen√ºber der API und sind notwendig, um Zugriff auf Reddit-Inhalte zu erhalten. Der user_agent erm√∂glicht zudem die R√ºckverfolgbarkeit von API-Anfragen seitens Reddit. Ohne diese Authentifizierung ist ein reguliertes, automatisiertes Scraping nicht zul√§ssig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5789b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=(os.getenv(\"REDDIT_ID\")),\n",
    "    client_secret=(os.getenv(\"REDDIT_SECRET\")),\n",
    "    user_agent=(os.getenv(\"USER_AGENT\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c34f9d7",
   "metadata": {},
   "source": [
    "**Logging-Konfiguration**\n",
    "\n",
    "Bevor das Reddit-Scraping startet, wird ein Logging-System eingerichtet. Dazu wird zun√§chst ein Pfad zur Log-Datei definiert ‚Äì in diesem Fall `logs/reddit.log`. Falls das Verzeichnis `logs/` noch nicht existiert, wird es automatisch erstellt. Anschlie√üend wird das Python-Logging so konfiguriert, dass alle Log-Meldungen in diese Datei geschrieben werden.\n",
    "\n",
    "Die Konfiguration legt fest, dass nur Meldungen ab dem Schweregrad `INFO` gespeichert werden. Au√üerdem wird das Format der Eintr√§ge so definiert, dass jeder Log-Eintrag einen Zeitstempel, den Log-Level (wie `INFO` oder `ERROR`) sowie die eigentliche Nachricht enth√§lt. So entsteht eine nachvollziehbare Chronik √ºber den Ablauf und m√∂gliche Fehler des Scripts.\n",
    "\n",
    "Ein typischer Eintrag k√∂nnte zum Beispiel so aussehen:\n",
    "\n",
    "```\n",
    "2025-04-19 14:33:07,512 - INFO - Starte Reddit-Scraping...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b20f953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Logging aktiv unter: C:\\Users\\SofiePischl\\Documents\\01_HdM\\10_ML_OPS\\TrendAnalyse Social Media\\logs\\reddit.log\n"
     ]
    }
   ],
   "source": [
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "# Logging einrichten\n",
    "log_path = Path(\"../logs/reddit.log\")\n",
    "log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    filename=log_path,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "print(f\" Logging aktiv unter: {log_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6781e111",
   "metadata": {},
   "source": [
    "**Reddit-Datensammlung mittels Python und PRAW**\n",
    "\n",
    "Die Funktion `scrape_reddit()` dient der systematischen Erhebung textbasierter Inhalte aus der Social-Media-Plattform **Reddit**. Ziel ist es, strukturierte Daten zur Analyse von Trendthemen zu generieren. Zur Umsetzung wird die Bibliothek `praw` (Python Reddit API Wrapper) verwendet, die eine komfortable Schnittstelle zur Reddit-API bereitstellt.\n",
    "\n",
    "Nach der Initialisierung der Protokollierung via `logging` erfolgt die Authentifizierung an der Reddit-API. Hierf√ºr wird ein `Reddit`-Objekt instanziiert, wobei sensible Zugangsdaten wie `client_id`, `client_secret` und `user_agent` aus einer `.env`-Datei geladen werden. Dieses Vorgehen erm√∂glicht eine sichere Trennung von Konfiguration und Codebasis und sch√ºtzt vor dem unbeabsichtigten Leaken von API-Schl√ºsseln.\n",
    "\n",
    "Im Anschluss wird eine Liste von Subreddits definiert, die sowohl popul√§re als auch als ‚Äûtrending‚Äú markierte Communities umfasst. Aus diesen Subreddits werden jeweils bis zu 100 Beitr√§ge aus dem Hot-Feed abgerufen. Dieses Verfahren stellt sicher, dass aktuelle, stark diskutierte Inhalte gesammelt werden, die ein hohes Relevanzpotenzial f√ºr Trendanalysen aufweisen.\n",
    "\n",
    "Die Datenextraktion erfolgt √ºber eine doppelte Schleife: F√ºr jedes Subreddit werden Hot-Beitr√§ge iteriert, wobei ausschlie√ülich ‚Äûself-posts‚Äú ber√ºcksichtigt werden. Diese beinhalten keine externen Links und erm√∂glichen dadurch eine fokussierte Analyse des vom Nutzer selbst verfassten Textinhalts. Pro Beitrag werden zentrale Metriken wie Titel, Text, Anzahl der Kommentare, Upvotes, Erstellungszeitpunkt sowie die URL gespeichert. Zur besseren zeitlichen Einordnung wird au√üerdem ein einheitlicher Zeitstempel f√ºr alle Eintr√§ge vergeben.\n",
    "\n",
    "Die gesammelten Beitr√§ge werden in einem `pandas.DataFrame` strukturiert und anschlie√üend unter `data/raw/reddit_data.csv` abgespeichert. Dabei wird sichergestellt, dass ben√∂tigte Verzeichnisse automatisch erstellt werden. Falls bereits Daten vorhanden sind, werden die neuen Eintr√§ge angeh√§ngt und anschlie√üend Duplikate basierend auf Titel, Textinhalt und Subreddit entfernt. Die finale Version wird ohne Index in die CSV-Datei geschrieben.\n",
    "\n",
    "Abschlie√üend wird die Anzahl der gespeicherten Beitr√§ge im Logfile vermerkt. Etwaige Fehler werden w√§hrend der Ausf√ºhrung abgefangen und entsprechend protokolliert. Die Funktion kann sowohl als Modul importiert als auch direkt per Skriptausf√ºhrung genutzt werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f757739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Gespeichert unter: C:\\Users\\SofiePischl\\Documents\\01_HdM\\10_ML_OPS\\TrendAnalyse Social Media\\app\\data\\raw\\reddit_data.csv\n"
     ]
    }
   ],
   "source": [
    "def scrape_reddit():\n",
    "    try:\n",
    "        logging.info(\"Starte Reddit-Scraping...\")\n",
    "\n",
    "        subreddits = [\"all\", \"popular\", \"trendingreddits\", \"trendingsubreddits\"]\n",
    "        post_limit = 100\n",
    "        all_posts = []\n",
    "        scrape_time = datetime.now()\n",
    "\n",
    "        for sub in subreddits:\n",
    "            subreddit = reddit.subreddit(sub)\n",
    "            for post in subreddit.hot(limit=post_limit):\n",
    "                if post.is_self:\n",
    "                    all_posts.append({\n",
    "                        \"subreddit\": sub,\n",
    "                        \"title\": post.title,\n",
    "                        \"text\": post.selftext,\n",
    "                        \"score\": post.score,\n",
    "                        \"comments\": post.num_comments,\n",
    "                        \"created\": datetime.fromtimestamp(post.created),\n",
    "                        \"url\": post.url,\n",
    "                        \"scraped_at\": scrape_time\n",
    "                    })\n",
    "\n",
    "        df = pd.DataFrame(all_posts)\n",
    "        csv_path = Path(\"../app/data/raw/reddit_data.csv\")\n",
    "        csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if csv_path.exists():\n",
    "            df_existing = pd.read_csv(csv_path)\n",
    "            df = pd.concat([df_existing, df], ignore_index=True)\n",
    "\n",
    "        df.drop_duplicates(subset=[\"title\", \"text\", \"subreddit\"], inplace=True)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\" Gespeichert unter: {csv_path.resolve()}\")\n",
    "\n",
    "        logging.info(f\"{len(df)} Eintr√§ge gespeichert unter {csv_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fehler beim Reddit-Scraping: {e}\")\n",
    "\n",
    "# Ausf√ºhrung bei direktem Aufruf\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_reddit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba082732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>comments</th>\n",
       "      <th>created</th>\n",
       "      <th>url</th>\n",
       "      <th>scraped_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>popular</td>\n",
       "      <td>Is anyone else getting irritated with the new ...</td>\n",
       "      <td>I get it, it‚Äôs something I can put in my prefe...</td>\n",
       "      <td>2474</td>\n",
       "      <td>643</td>\n",
       "      <td>2025-04-19 01:53:11</td>\n",
       "      <td>https://www.reddit.com/r/ChatGPT/comments/1k2j...</td>\n",
       "      <td>2025-04-19 14:20:21.514955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>popular</td>\n",
       "      <td>What is the first thing you‚Äôd buy if you get f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5116</td>\n",
       "      <td>7975</td>\n",
       "      <td>2025-04-18 21:51:33</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/1k...</td>\n",
       "      <td>2025-04-19 14:23:21.324902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>popular</td>\n",
       "      <td>Under current law, the Social Security payroll...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9256</td>\n",
       "      <td>1062</td>\n",
       "      <td>2025-04-18 18:47:13</td>\n",
       "      <td>https://www.reddit.com/r/SocialSecurity/commen...</td>\n",
       "      <td>2025-04-19 14:23:21.324902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>trendingreddits</td>\n",
       "      <td>We are open again!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2024-09-27 20:14:11</td>\n",
       "      <td>https://www.reddit.com/r/TrendingReddits/comme...</td>\n",
       "      <td>2025-04-19 14:23:21.324902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>trendingreddits</td>\n",
       "      <td>Hi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-04-02 21:13:09</td>\n",
       "      <td>https://www.reddit.com/r/TrendingReddits/comme...</td>\n",
       "      <td>2025-04-19 14:23:21.324902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           subreddit                                              title  \\\n",
       "162          popular  Is anyone else getting irritated with the new ...   \n",
       "163          popular  What is the first thing you‚Äôd buy if you get f...   \n",
       "164          popular  Under current law, the Social Security payroll...   \n",
       "165  trendingreddits                                 We are open again!   \n",
       "166  trendingreddits                                                 Hi   \n",
       "\n",
       "                                                  text  score  comments  \\\n",
       "162  I get it, it‚Äôs something I can put in my prefe...   2474       643   \n",
       "163                                                NaN   5116      7975   \n",
       "164                                                NaN   9256      1062   \n",
       "165                                                NaN      4         2   \n",
       "166                                                NaN      1         4   \n",
       "\n",
       "                 created                                                url  \\\n",
       "162  2025-04-19 01:53:11  https://www.reddit.com/r/ChatGPT/comments/1k2j...   \n",
       "163  2025-04-18 21:51:33  https://www.reddit.com/r/AskReddit/comments/1k...   \n",
       "164  2025-04-18 18:47:13  https://www.reddit.com/r/SocialSecurity/commen...   \n",
       "165  2024-09-27 20:14:11  https://www.reddit.com/r/TrendingReddits/comme...   \n",
       "166  2025-04-02 21:13:09  https://www.reddit.com/r/TrendingReddits/comme...   \n",
       "\n",
       "                     scraped_at  \n",
       "162  2025-04-19 14:20:21.514955  \n",
       "163  2025-04-19 14:23:21.324902  \n",
       "164  2025-04-19 14:23:21.324902  \n",
       "165  2025-04-19 14:23:21.324902  \n",
       "166  2025-04-19 14:23:21.324902  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zeuge neuste Eintr√§ge\n",
    "df = pd.read_csv(\"../app/data/raw/reddit_data.csv\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c86300dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8152499",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Instagram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125499c",
   "metadata": {},
   "source": [
    "was nicht funktioniert hat: instaloader, playwright"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4076b412",
   "metadata": {},
   "source": [
    "step 1: log in & extract urls from for you page\n",
    "\n",
    "step 2: Look up urls without beeing signed in and extract captions & content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb71d64",
   "metadata": {},
   "source": [
    "setp 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74fa05b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e7df8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log in data\n",
    "USERNAME =  os.getenv(\"INSTA_USERNAME\")\n",
    "PASSWORD =  os.getenv(\"INSTA_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd225ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 5 URLs to 'explore_results.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Start browser and login\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "wait = WebDriverWait(driver, 15)\n",
    "\n",
    "driver.get(\"https://www.instagram.com/accounts/login/\")\n",
    "time.sleep(3)\n",
    "\n",
    "# Accept cookies (if shown)\n",
    "try:\n",
    "    decline_button = wait.until(EC.element_to_be_clickable(\n",
    "        (By.XPATH, '//button[contains(text(), \"Nur essentielle Cookies erlauben\") or contains(text(), \"Decline optional cookies\")]')))\n",
    "    decline_button.click()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Fill in login form\n",
    "wait.until(EC.presence_of_element_located((By.NAME, 'username')))\n",
    "driver.find_element(By.NAME, 'username').send_keys(USERNAME)\n",
    "driver.find_element(By.NAME, 'password').send_keys(PASSWORD)\n",
    "driver.find_element(By.NAME, 'password').send_keys(Keys.RETURN)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Go to Explore page\n",
    "driver.get(\"https://www.instagram.com/explore/\")\n",
    "time.sleep(5)\n",
    "\n",
    "# Scroll and collect post URLs\n",
    "post_urls = set()\n",
    "scrolls = 0\n",
    "while len(post_urls) < 20 and scrolls < 10:\n",
    "    elements = driver.find_elements(By.XPATH, \"//a[contains(@href, '/p/')]\")\n",
    "    for elem in elements:\n",
    "        href = elem.get_attribute(\"href\")\n",
    "        if href and href.startswith(\"https://www.instagram.com/p/\"):\n",
    "            post_urls.add(href)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "    scrolls += 1\n",
    "\n",
    "# Save URLs to CSV\n",
    "df = pd.DataFrame({\"Post URL\": list(post_urls)})\n",
    "df.to_csv(\"../data/raw/explore_results_3.csv\", index=False)\n",
    "print(f\"‚úÖ Saved {len(post_urls)} URLs to 'explore_results.csv'.\")\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40e42b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f308ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SETUP ===\n",
    "csv_path = \"../data/raw/explore_results_3.csv\"\n",
    "df = pd.read_csv(csv_path, dtype=\"string\")\n",
    "\n",
    "# Sicherstellen, dass alle Zielspalten existieren\n",
    "for col in [\"timestamp\", \"datum\", \"inhalt\", \"username\", \"caption\", \"likes\"]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = \"\"\n",
    "\n",
    "# Browser starten\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "wait = WebDriverWait(driver, 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "666b738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.instagram.com/p/DICWDtYNq7E/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edbad6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dismiss_popups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "190b7f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dismiss_popups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1bed21",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)\n",
    "        print(\"Looked up url\")\n",
    "        dismiss_popups()\n",
    "        time.sleep(1)\n",
    "        dismiss_popups()\n",
    "        print(\"Dismissed pop ups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "830d3ba4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//a[contains(@href, '/') and contains(@class, 'notranslate')]//span[1]\"}\n  (Session info: chrome=135.0.7049.85); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x010280E3+60707]\n\tGetHandleVerifier [0x01028124+60772]\n\t(No symbol) [0x00E50683]\n\t(No symbol) [0x00E98660]\n\t(No symbol) [0x00E989FB]\n\t(No symbol) [0x00EE1022]\n\t(No symbol) [0x00EBD094]\n\t(No symbol) [0x00EDE824]\n\t(No symbol) [0x00EBCE46]\n\t(No symbol) [0x00E8C5D3]\n\t(No symbol) [0x00E8D424]\n\tGetHandleVerifier [0x0126BBC3+2435075]\n\tGetHandleVerifier [0x01267163+2416035]\n\tGetHandleVerifier [0x0128350C+2531660]\n\tGetHandleVerifier [0x0103F1B5+155125]\n\tGetHandleVerifier [0x01045B5D+182173]\n\tGetHandleVerifier [0x0102F9B8+91640]\n\tGetHandleVerifier [0x0102FB60+92064]\n\tGetHandleVerifier [0x0101A620+4704]\n\tBaseThreadInitThunk [0x761F5D49+25]\n\tRtlInitializeExceptionChain [0x77D1CE3B+107]\n\tRtlGetAppContainerNamedObjectPath [0x77D1CDC1+561]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m username_elem \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m//a[contains(@href, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m) and contains(@class, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnotranslate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)]//span[1]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(username_elem)\n",
      "File \u001b[1;32mc:\\Users\\SofiePischl\\Documents\\01_HdM\\10_ML_OPS\\TrendAnalyse Social Media\\.venv\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:898\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    895\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NoSuchElementException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot locate relative element with: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mby\u001b[38;5;241m.\u001b[39mroot\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elements[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\SofiePischl\\Documents\\01_HdM\\10_ML_OPS\\TrendAnalyse Social Media\\.venv\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:429\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    427\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 429\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    430\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\SofiePischl\\Documents\\01_HdM\\10_ML_OPS\\TrendAnalyse Social Media\\.venv\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:232\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    230\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//a[contains(@href, '/') and contains(@class, 'notranslate')]//span[1]\"}\n  (Session info: chrome=135.0.7049.85); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x010280E3+60707]\n\tGetHandleVerifier [0x01028124+60772]\n\t(No symbol) [0x00E50683]\n\t(No symbol) [0x00E98660]\n\t(No symbol) [0x00E989FB]\n\t(No symbol) [0x00EE1022]\n\t(No symbol) [0x00EBD094]\n\t(No symbol) [0x00EDE824]\n\t(No symbol) [0x00EBCE46]\n\t(No symbol) [0x00E8C5D3]\n\t(No symbol) [0x00E8D424]\n\tGetHandleVerifier [0x0126BBC3+2435075]\n\tGetHandleVerifier [0x01267163+2416035]\n\tGetHandleVerifier [0x0128350C+2531660]\n\tGetHandleVerifier [0x0103F1B5+155125]\n\tGetHandleVerifier [0x01045B5D+182173]\n\tGetHandleVerifier [0x0102F9B8+91640]\n\tGetHandleVerifier [0x0102FB60+92064]\n\tGetHandleVerifier [0x0101A620+4704]\n\tBaseThreadInitThunk [0x761F5D49+25]\n\tRtlInitializeExceptionChain [0x77D1CE3B+107]\n\tRtlGetAppContainerNamedObjectPath [0x77D1CDC1+561]\n"
     ]
    }
   ],
   "source": [
    "username_elem = driver.find_element(By.XPATH, \"//a[contains(@href, '/') and contains(@class, 'notranslate')]//span[1]\")\n",
    "print(username_elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc978fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_fida.n.zati_'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username_elem.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7be236d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"(//div[contains(@class, '_a9zs')]/span)[2]\"}\n  (Session info: chrome=135.0.7049.85); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x010280E3+60707]\n\tGetHandleVerifier [0x01028124+60772]\n\t(No symbol) [0x00E50683]\n\t(No symbol) [0x00E98660]\n\t(No symbol) [0x00E989FB]\n\t(No symbol) [0x00EE1022]\n\t(No symbol) [0x00EBD094]\n\t(No symbol) [0x00EDE824]\n\t(No symbol) [0x00EBCE46]\n\t(No symbol) [0x00E8C5D3]\n\t(No symbol) [0x00E8D424]\n\tGetHandleVerifier [0x0126BBC3+2435075]\n\tGetHandleVerifier [0x01267163+2416035]\n\tGetHandleVerifier [0x0128350C+2531660]\n\tGetHandleVerifier [0x0103F1B5+155125]\n\tGetHandleVerifier [0x01045B5D+182173]\n\tGetHandleVerifier [0x0102F9B8+91640]\n\tGetHandleVerifier [0x0102FB60+92064]\n\tGetHandleVerifier [0x0101A620+4704]\n\tBaseThreadInitThunk [0x761F5D49+25]\n\tRtlInitializeExceptionChain [0x77D1CE3B+107]\n\tRtlGetAppContainerNamedObjectPath [0x77D1CDC1+561]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m caption_elem \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m(//div[contains(@class, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_a9zs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)]/span)[2]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m caption_elem\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32mc:\\Users\\SofiePischl\\Documents\\01_HdM\\10_ML_OPS\\TrendAnalyse Social Media\\.venv\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:898\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    895\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NoSuchElementException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot locate relative element with: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mby\u001b[38;5;241m.\u001b[39mroot\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elements[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\SofiePischl\\Documents\\01_HdM\\10_ML_OPS\\TrendAnalyse Social Media\\.venv\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:429\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    427\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 429\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    430\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\SofiePischl\\Documents\\01_HdM\\10_ML_OPS\\TrendAnalyse Social Media\\.venv\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:232\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    230\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"(//div[contains(@class, '_a9zs')]/span)[2]\"}\n  (Session info: chrome=135.0.7049.85); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x010280E3+60707]\n\tGetHandleVerifier [0x01028124+60772]\n\t(No symbol) [0x00E50683]\n\t(No symbol) [0x00E98660]\n\t(No symbol) [0x00E989FB]\n\t(No symbol) [0x00EE1022]\n\t(No symbol) [0x00EBD094]\n\t(No symbol) [0x00EDE824]\n\t(No symbol) [0x00EBCE46]\n\t(No symbol) [0x00E8C5D3]\n\t(No symbol) [0x00E8D424]\n\tGetHandleVerifier [0x0126BBC3+2435075]\n\tGetHandleVerifier [0x01267163+2416035]\n\tGetHandleVerifier [0x0128350C+2531660]\n\tGetHandleVerifier [0x0103F1B5+155125]\n\tGetHandleVerifier [0x01045B5D+182173]\n\tGetHandleVerifier [0x0102F9B8+91640]\n\tGetHandleVerifier [0x0102FB60+92064]\n\tGetHandleVerifier [0x0101A620+4704]\n\tBaseThreadInitThunk [0x761F5D49+25]\n\tRtlInitializeExceptionChain [0x77D1CE3B+107]\n\tRtlGetAppContainerNamedObjectPath [0x77D1CDC1+561]\n"
     ]
    }
   ],
   "source": [
    "caption_elem = driver.find_element(By.XPATH, \"(//div[contains(@class, '_a9zs')]/span)[2]\")\n",
    "caption_elem.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6229c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "like_spans = driver.find_elements(By.XPATH, \"//section//span[contains(text(), 'likes')]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f82da36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"2794535543ca7a8f5811614e0ca17064\", element=\"f.FA5CB9931F1124A41F7FC0F4B96F38C7.d.9EF78FA623897B6F8821F18CB253A2F5.e.1324\")>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "like_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0ac68da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'251,547 likes'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "like_spans[-1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f55365b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['251,547']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"\\d[\\d\\.\\,]*\", like_spans[-1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332dc9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                    like_text = like_spans[-1].text\n",
    "                    like_num = re.findall(r\"\\d[\\d\\.\\,]*\", like_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9b4ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ike_spans = driver.find_elements(By.XPATH, \"//section//span[contains(text(), 'Gef√§llt')]\")\n",
    "                if like_spans:\n",
    "                    like_text = like_spans[-1].text\n",
    "                    like_num = re.findall(r\"\\d[\\d\\.\\,]*\", like_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3b47dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_elem = driver.find_element(By.XPATH, \"//time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cac13f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2025-04-04'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_elem.get_attribute(\"datetime\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6dd380be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SETUP ===\n",
    "csv_path = \"../data/raw/explore_results_3.csv\"\n",
    "df = pd.read_csv(csv_path, dtype=\"string\")\n",
    "\n",
    "# Sicherstellen, dass alle Zielspalten existieren\n",
    "for col in [\"timestamp\", \"datum\", \"inhalt\", \"username\", \"caption\", \"likes\"]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = \"\"\n",
    "\n",
    "# Browser starten\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "wait = WebDriverWait(driver, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ef306e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Popups abfangen ===\n",
    "def dismiss_popups():\n",
    "    xpaths = [\n",
    "        '//button[contains(text(), \"Nur essentielle Cookies erlauben\")]',\n",
    "        '//button[contains(text(), \"Decline optional cookies\")]',\n",
    "        '//div[@role=\"dialog\"]//div[@aria-label=\"Schlie√üen\"]',\n",
    "        '//div[@role=\"dialog\"]//div[@aria-label=\"Close\"]',\n",
    "         \"//div[@role='dialog']//button\",\n",
    "        '//button[@aria-label=\"Schlie√üen\"]',\n",
    "    ]\n",
    "    for xpath in xpaths:\n",
    "        try:\n",
    "            btn = WebDriverWait(driver, 2).until(EC.element_to_be_clickable((By.XPATH, xpath)))\n",
    "            btn.click()\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c18761d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Beitrag geladen.\n",
      "‚úÖ Kein Popup oder konnte nicht geklickt werden.\n",
      "‚ùå Username nicht gefunden.\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Schritt 1: Seite aufrufen\n",
    "url = 'https://www.instagram.com/p/DICWDtYNq7E/'\n",
    "driver.get(url)\n",
    "\n",
    "# Schritt 2: Warten, bis der Beitrag (Artikel) geladen ist\n",
    "try:\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.TAG_NAME, \"article\"))\n",
    "    )\n",
    "    print(\"üìÑ Beitrag geladen.\")\n",
    "except:\n",
    "    print(\"‚ùå Beitrag nicht gefunden.\")\n",
    "\n",
    "# Schritt 3: Versuche, das Login-Popup zu schlie√üen\n",
    "try:\n",
    "    close_btn = WebDriverWait(driver, 5).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//div[@role='dialog']//button\"))\n",
    "    )\n",
    "    close_btn.click()\n",
    "    print(\"‚ùé Login-Popup geschlossen.\")\n",
    "except:\n",
    "    print(\"‚úÖ Kein Popup oder konnte nicht geklickt werden.\")\n",
    "\n",
    "# Schritt 4: Jetzt warte, bis Username sichtbar ist\n",
    "try:\n",
    "    username_elem = WebDriverWait(driver, 10).until(\n",
    "        EC.visibility_of_element_located(By.XPATH, \"//a[contains(@href, '/') and contains(@class, 'notranslate')]//span[1]\")\n",
    "    )\n",
    "    username = username_elem.text.strip()\n",
    "    print(\"üë§ Username:\", username)\n",
    "except:\n",
    "    print(\"‚ùå Username nicht gefunden.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "421c82f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//a[contains(@href, '/') and contains(@class, 'notranslate')]//span[1]\"}\n  (Session info: chrome=135.0.7049.85); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x010280E3+60707]\n\tGetHandleVerifier [0x01028124+60772]\n\t(No symbol) [0x00E50683]\n\t(No symbol) [0x00E98660]\n\t(No symbol) [0x00E989FB]\n\t(No symbol) [0x00EE1022]\n\t(No symbol) [0x00EBD094]\n\t(No symbol) [0x00EDE824]\n\t(No symbol) [0x00EBCE46]\n\t(No symbol) [0x00E8C5D3]\n\t(No symbol) [0x00E8D424]\n\tGetHandleVerifier [0x0126BBC3+2435075]\n\tGetHandleVerifier [0x01267163+2416035]\n\tGetHandleVerifier [0x0128350C+2531660]\n\tGetHandleVerifier [0x0103F1B5+155125]\n\tGetHandleVerifier [0x01045B5D+182173]\n\tGetHandleVerifier [0x0102F9B8+91640]\n\tGetHandleVerifier [0x0102FB60+92064]\n\tGetHandleVerifier [0x0101A620+4704]\n\tBaseThreadInitThunk [0x761F5D49+25]\n\tRtlInitializeExceptionChain [0x77D1CE3B+107]\n\tRtlGetAppContainerNamedObjectPath [0x77D1CDC1+561]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m username_elem \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m//a[contains(@href, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m) and contains(@class, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnotranslate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)]//span[1]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(username_elem)\n",
      "File \u001b[1;32mc:\\Users\\SofiePischl\\Documents\\01_HdM\\10_ML_OPS\\TrendAnalyse Social Media\\.venv\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:898\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    895\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NoSuchElementException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot locate relative element with: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mby\u001b[38;5;241m.\u001b[39mroot\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elements[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\SofiePischl\\Documents\\01_HdM\\10_ML_OPS\\TrendAnalyse Social Media\\.venv\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:429\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    427\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 429\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    430\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\SofiePischl\\Documents\\01_HdM\\10_ML_OPS\\TrendAnalyse Social Media\\.venv\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:232\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    230\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//a[contains(@href, '/') and contains(@class, 'notranslate')]//span[1]\"}\n  (Session info: chrome=135.0.7049.85); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x010280E3+60707]\n\tGetHandleVerifier [0x01028124+60772]\n\t(No symbol) [0x00E50683]\n\t(No symbol) [0x00E98660]\n\t(No symbol) [0x00E989FB]\n\t(No symbol) [0x00EE1022]\n\t(No symbol) [0x00EBD094]\n\t(No symbol) [0x00EDE824]\n\t(No symbol) [0x00EBCE46]\n\t(No symbol) [0x00E8C5D3]\n\t(No symbol) [0x00E8D424]\n\tGetHandleVerifier [0x0126BBC3+2435075]\n\tGetHandleVerifier [0x01267163+2416035]\n\tGetHandleVerifier [0x0128350C+2531660]\n\tGetHandleVerifier [0x0103F1B5+155125]\n\tGetHandleVerifier [0x01045B5D+182173]\n\tGetHandleVerifier [0x0102F9B8+91640]\n\tGetHandleVerifier [0x0102FB60+92064]\n\tGetHandleVerifier [0x0101A620+4704]\n\tBaseThreadInitThunk [0x761F5D49+25]\n\tRtlInitializeExceptionChain [0x77D1CE3B+107]\n\tRtlGetAppContainerNamedObjectPath [0x77D1CDC1+561]\n"
     ]
    }
   ],
   "source": [
    "username_elem = driver.find_element(By.XPATH, \"//a[contains(@href, '/') and contains(@class, 'notranslate')]//span[1]\")\n",
    "print(username_elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d6148a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Lade Beitrag: https://www.instagram.com/p/DICWDtYNq7E/\n",
      "Looked up url\n",
      "Dismissed pop ups\n",
      "Entering try loop username\n",
      "Looking for username\n",
      "‚ùå Username nicht gefunden.\n",
      "\n",
      " Lade Beitrag: https://www.instagram.com/p/DHLad6VIRES/\n",
      "Looked up url\n",
      "Dismissed pop ups\n",
      "Entering try loop username\n",
      "Looking for username\n",
      "‚ùå Username nicht gefunden.\n",
      "\n",
      " Lade Beitrag: https://www.instagram.com/p/DIOqCE7TKSu/\n",
      "m\n",
      "\n",
      " Lade Beitrag: https://www.instagram.com/p/DHQFK3CMfRe/\n",
      "m\n",
      "\n",
      " Lade Beitrag: https://www.instagram.com/p/DHWe5czOGta/\n",
      "m\n"
     ]
    }
   ],
   "source": [
    "# === Hauptloop ===\n",
    "for i, row in df.iterrows():\n",
    "    url = row['Post URL']\n",
    "    if not isinstance(url, str) or not url.startswith(\"http\"):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        print(f\"\\n Lade Beitrag: {url}\")\n",
    "        driver.get(url)\n",
    "        print(\"Looked up url\")\n",
    "        dismiss_popups()\n",
    "        time.sleep(1)\n",
    "        dismiss_popups()\n",
    "        print(\"Dismissed pop ups\")\n",
    "\n",
    "\n",
    "        # Zeitstempel immer setzen\n",
    "        df.at[i, \"timestamp\"] = datetime.now().isoformat()\n",
    "\n",
    "        # üë§ Username (erster Treffer)\n",
    "        if not pd.notna(row.get(\"username\")) or row.get(\"username\") == \"\":\n",
    "            print(\"Entering try loop username\")\n",
    "            try:\n",
    "                print(\"Looking for username\")\n",
    "                time.sleep(2)\n",
    "                username_elem = driver.find_element(By.XPATH, \"//a[contains(@href, '/') and contains(@class, 'notranslate')]//span[1]\")\n",
    "                print(username_elem)\n",
    "                #username_elem = driver.find_element(By.XPATH, \"(//a[contains(@href, '/') and contains(@class, 'notranslate')]//span)[1]\")\n",
    "                df.at[i, \"username\"] = username_elem.text.strip()\n",
    "                print(\"üë§ Username:\", df.at[i, \"username\"])\n",
    "                print(username_elem)\n",
    "            except:\n",
    "                print(\"‚ùå Username nicht gefunden.\")\n",
    "\n",
    "    except:\n",
    "        print(\"m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c5679a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Lade Beitrag: https://www.instagram.com/p/DICWDtYNq7E/\n",
      "Looked up url\n",
      "Dismissed pop ups\n",
      "Entering try loop username\n",
      "Looking for username\n",
      "‚ùå Username nicht gefunden.\n",
      "Caption not found\n",
      "üñºÔ∏è Bildbeschreibung: _fida.n.zati_'s profile picture\n",
      "\n",
      " Lade Beitrag: https://www.instagram.com/p/DHLad6VIRES/\n",
      "Looked up url\n",
      "Dismissed pop ups\n",
      "Entering try loop username\n",
      "Looking for username\n",
      "‚ùå Username nicht gefunden.\n",
      "Caption not found\n",
      "\n",
      " Lade Beitrag: https://www.instagram.com/p/DIOqCE7TKSu/\n",
      "Looked up url\n",
      "Dismissed pop ups\n",
      "Entering try loop username\n",
      "Looking for username\n",
      "‚ùå Username nicht gefunden.\n",
      "Caption not found\n",
      "‚ùå Likes nicht gefunden.\n",
      "\n",
      " Lade Beitrag: https://www.instagram.com/p/DHQFK3CMfRe/\n",
      "‚ùå Fehler bei https://www.instagram.com/p/DHQFK3CMfRe/: Message: invalid session id\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x010280E3+60707]\n",
      "\tGetHandleVerifier [0x01028124+60772]\n",
      "\t(No symbol) [0x00E504FE]\n",
      "\t(No symbol) [0x00E8B898]\n",
      "\t(No symbol) [0x00EBCF06]\n",
      "\t(No symbol) [0x00EB89D5]\n",
      "\t(No symbol) [0x00EB7F66]\n",
      "\t(No symbol) [0x00E236E5]\n",
      "\t(No symbol) [0x00E23C3E]\n",
      "\t(No symbol) [0x00E240CD]\n",
      "\tGetHandleVerifier [0x0126BBC3+2435075]\n",
      "\tGetHandleVerifier [0x01267163+2416035]\n",
      "\tGetHandleVerifier [0x0128350C+2531660]\n",
      "\tGetHandleVerifier [0x0103F1B5+155125]\n",
      "\tGetHandleVerifier [0x01045B5D+182173]\n",
      "\t(No symbol) [0x00E233B0]\n",
      "\t(No symbol) [0x00E22BC3]\n",
      "\tGetHandleVerifier [0x0138D2AC+3620588]\n",
      "\tBaseThreadInitThunk [0x761F5D49+25]\n",
      "\tRtlInitializeExceptionChain [0x77D1CE3B+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D1CDC1+561]\n",
      "\n",
      "\n",
      " Lade Beitrag: https://www.instagram.com/p/DHWe5czOGta/\n",
      "‚ùå Fehler bei https://www.instagram.com/p/DHWe5czOGta/: Message: invalid session id\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x010280E3+60707]\n",
      "\tGetHandleVerifier [0x01028124+60772]\n",
      "\t(No symbol) [0x00E504FE]\n",
      "\t(No symbol) [0x00E8B898]\n",
      "\t(No symbol) [0x00EBCF06]\n",
      "\t(No symbol) [0x00EB89D5]\n",
      "\t(No symbol) [0x00EB7F66]\n",
      "\t(No symbol) [0x00E236E5]\n",
      "\t(No symbol) [0x00E23C3E]\n",
      "\t(No symbol) [0x00E240CD]\n",
      "\tGetHandleVerifier [0x0126BBC3+2435075]\n",
      "\tGetHandleVerifier [0x01267163+2416035]\n",
      "\tGetHandleVerifier [0x0128350C+2531660]\n",
      "\tGetHandleVerifier [0x0103F1B5+155125]\n",
      "\tGetHandleVerifier [0x01045B5D+182173]\n",
      "\t(No symbol) [0x00E233B0]\n",
      "\t(No symbol) [0x00E22BC3]\n",
      "\tGetHandleVerifier [0x0138D2AC+3620588]\n",
      "\tBaseThreadInitThunk [0x761F5D49+25]\n",
      "\tRtlInitializeExceptionChain [0x77D1CE3B+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D1CDC1+561]\n",
      "\n",
      "\n",
      "‚úÖ Alle Daten aktualisiert und gespeichert.\n"
     ]
    }
   ],
   "source": [
    "# === SETUP ===\n",
    "csv_path = \"../data/raw/explore_results_3.csv\"\n",
    "df = pd.read_csv(csv_path, dtype=\"string\")\n",
    "\n",
    "# Sicherstellen, dass alle Zielspalten existieren\n",
    "for col in [\"timestamp\", \"datum\", \"inhalt\", \"username\", \"caption\", \"likes\"]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = \"\"\n",
    "\n",
    "# Browser starten\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "wait = WebDriverWait(driver, 15)\n",
    "\n",
    "# === Popups abfangen ===\n",
    "def dismiss_popups():\n",
    "    xpaths = [\n",
    "        '//button[contains(text(), \"Nur essentielle Cookies erlauben\")]',\n",
    "        '//button[contains(text(), \"Decline optional cookies\")]',\n",
    "        '//div[@role=\"dialog\"]//div[@aria-label=\"Schlie√üen\"]',\n",
    "        '//div[@role=\"dialog\"]//div[@aria-label=\"Close\"]',\n",
    "         \"//div[@role='dialog']//button\",\n",
    "        '//button[@aria-label=\"Schlie√üen\"]',\n",
    "    ]\n",
    "    for xpath in xpaths:\n",
    "        try:\n",
    "            btn = WebDriverWait(driver, 2).until(EC.element_to_be_clickable((By.XPATH, xpath)))\n",
    "            btn.click()\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# === Hauptloop ===\n",
    "for i, row in df.iterrows():\n",
    "    url = row['Post URL']\n",
    "    if not isinstance(url, str) or not url.startswith(\"http\"):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        print(f\"\\n Lade Beitrag: {url}\")\n",
    "        driver.get(url)\n",
    "        print(\"Looked up url\")\n",
    "        dismiss_popups()\n",
    "        time.sleep(1)\n",
    "        dismiss_popups()\n",
    "        print(\"Dismissed pop ups\")\n",
    "\n",
    "\n",
    "        # Zeitstempel immer setzen\n",
    "        df.at[i, \"timestamp\"] = datetime.now().isoformat()\n",
    "\n",
    "        # üë§ Username (erster Treffer)\n",
    "        if not pd.notna(row.get(\"username\")) or row.get(\"username\") == \"\":\n",
    "            print(\"Entering try loop username\")\n",
    "            try:\n",
    "                time.sleep(2)\n",
    "                print(\"Looking for username\")\n",
    "                username_elem = driver.find_element(By.XPATH, \"//a[contains(@href, '/') and contains(@class, 'notranslate')]//span[1]\")\n",
    "                print(username_elem)\n",
    "                #username_elem = driver.find_element(By.XPATH, \"(//a[contains(@href, '/') and contains(@class, 'notranslate')]//span)[1]\")\n",
    "                df.at[i, \"username\"] = username_elem.text.strip()\n",
    "                print(\"üë§ Username:\", df.at[i, \"username\"])\n",
    "                print(username_elem)\n",
    "            except:\n",
    "                print(\"‚ùå Username nicht gefunden.\")\n",
    "\n",
    "        # üìù Caption (zweiter Treffer)\n",
    "        if not pd.notna(row.get(\"caption\")) or row.get(\"caption\") == \"\":\n",
    "            try:\n",
    "                caption_elem = driver.find_element(By.XPATH, \"(//div[contains(@class, '_a9zs')]/span)[2]\")\n",
    "                df.at[i, \"caption\"] = caption_elem.text.strip()\n",
    "                print(\"üìù Caption:\", df.at[i, \"caption\"])\n",
    "            except:\n",
    "                print(\"Caption not found\")\n",
    "\n",
    "        # ‚ù§Ô∏è Likes (letzter Treffer)\n",
    "        if not pd.notna(row.get(\"likes\")) or row.get(\"likes\") == \"\":\n",
    "            try:\n",
    "                like_spans = driver.find_elements(By.XPATH, \"//section//span[contains(text(), 'Gef√§llt')]\")\n",
    "                if like_spans:\n",
    "                    like_text = like_spans[-1].text\n",
    "                    like_num = re.findall(r\"\\d[\\d\\.\\,]*\", like_text)\n",
    "                    if like_num:\n",
    "                        likes = int(like_num[0].replace(\".\", \"\").replace(\",\", \"\"))\n",
    "                        df.at[i, \"likes\"] = str(likes)\n",
    "                        print(\"‚ù§Ô∏è Likes:\", likes)\n",
    "            except:\n",
    "                print(\"‚ùå Likes nicht gefunden.\")\n",
    "\n",
    "        # üóìÔ∏è Ver√∂ffentlichungsdatum\n",
    "        if not pd.notna(row.get(\"datum\")) or row.get(\"datum\") == \"\":\n",
    "            try:\n",
    "                date_elem = driver.find_element(By.XPATH, \"//time\")\n",
    "                df.at[i, \"datum\"] = date_elem.get_attribute(\"datetime\")[:10]\n",
    "                print(\"üìÖ Datum:\", df.at[i, \"datum\"])\n",
    "            except:\n",
    "                print(\"‚ùå Datum nicht gefunden.\")\n",
    "\n",
    "        # üñºÔ∏è Bildbeschreibung\n",
    "        if not pd.notna(row.get(\"inhalt\")) or row.get(\"inhalt\") == \"\":\n",
    "            try:\n",
    "                image = driver.find_element(By.XPATH, \"//article//img\")\n",
    "                df.at[i, \"inhalt\"] = image.get_attribute(\"alt\").strip()\n",
    "                print(\"üñºÔ∏è Bildbeschreibung:\", df.at[i, \"inhalt\"])\n",
    "            except:\n",
    "                print(\"‚ùå Bildbeschreibung nicht gefunden.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fehler bei {url}: {e}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "# Speichern\n",
    "df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "driver.quit()\n",
    "print(\"\\n‚úÖ Alle Daten aktualisiert und gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602447ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post URL</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>datum</th>\n",
       "      <th>inhalt</th>\n",
       "      <th>username</th>\n",
       "      <th>caption</th>\n",
       "      <th>likes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.instagram.com/p/DIKaU11Rwj0/</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.instagram.com/p/DHW6TrpR9UE/</td>\n",
       "      <td>2025-04-14T12:47:05.156202</td>\n",
       "      <td>2025-03-18</td>\n",
       "      <td>Photo by BernieGirl on March 18, 2025. Ist m√∂g...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.instagram.com/p/DHOLtvLtJKL/</td>\n",
       "      <td>2025-04-14T12:47:26.154501</td>\n",
       "      <td>2025-03-15</td>\n",
       "      <td>Photo by Die Welt hinter der Leinwand on March...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.instagram.com/p/DH-iv39t-RD/</td>\n",
       "      <td>2025-04-14T12:47:55.882011</td>\n",
       "      <td>2025-04-03</td>\n",
       "      <td>aiwonderlab.eu's profile picture</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.instagram.com/p/DIJwgPKpxTN/</td>\n",
       "      <td>2025-04-14T12:48:16.988772</td>\n",
       "      <td>2025-04-07</td>\n",
       "      <td>sonya_styless's profile picture</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Post URL                   timestamp  \\\n",
       "0  https://www.instagram.com/p/DIKaU11Rwj0/                        <NA>   \n",
       "1  https://www.instagram.com/p/DHW6TrpR9UE/  2025-04-14T12:47:05.156202   \n",
       "2  https://www.instagram.com/p/DHOLtvLtJKL/  2025-04-14T12:47:26.154501   \n",
       "3  https://www.instagram.com/p/DH-iv39t-RD/  2025-04-14T12:47:55.882011   \n",
       "4  https://www.instagram.com/p/DIJwgPKpxTN/  2025-04-14T12:48:16.988772   \n",
       "\n",
       "        datum                                             inhalt username  \\\n",
       "0        <NA>                                               <NA>     <NA>   \n",
       "1  2025-03-18  Photo by BernieGirl on March 18, 2025. Ist m√∂g...     <NA>   \n",
       "2  2025-03-15  Photo by Die Welt hinter der Leinwand on March...     <NA>   \n",
       "3  2025-04-03                   aiwonderlab.eu's profile picture     <NA>   \n",
       "4  2025-04-07                    sonya_styless's profile picture     <NA>   \n",
       "\n",
       "  caption likes  \n",
       "0    <NA>  <NA>  \n",
       "1    <NA>  <NA>  \n",
       "2    <NA>  <NA>  \n",
       "3    <NA>  <NA>  \n",
       "4    <NA>  <NA>  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafbbd32",
   "metadata": {},
   "source": [
    "# 1. TWITTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0abcb5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c082e7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8909893b",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Request failed: 429\n{\"title\":\"Too Many Requests\",\"detail\":\"Too Many Requests\",\"type\":\"about:blank\",\"status\":429}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Check response\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Extract data\u001b[39;00m\n",
      "\u001b[1;31mException\u001b[0m: Request failed: 429\n{\"title\":\"Too Many Requests\",\"detail\":\"Too Many Requests\",\"type\":\"about:blank\",\"status\":429}"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Your bearer token from Twitter Developer Portal\n",
    "BEARER_TOKEN = os.getenv(\"X_BEARER_TOKEN\")\n",
    "\n",
    "# Twitter API endpoint for recent tweets\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "\n",
    "# Search parameters ‚Äì open topic\n",
    "query_params = {\n",
    "    'query': 'Twitter lang:de -is:retweet',  # No keyword, just German tweets\n",
    "    'max_results': 50,  # Max per request (10‚Äì100)\n",
    "    'tweet.fields': 'created_at,public_metrics,text,author_id',\n",
    "    'expansions': 'author_id',\n",
    "    'user.fields': 'username,name'\n",
    "}\n",
    "\n",
    "# Set headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {BEARER_TOKEN}\"\n",
    "}\n",
    "\n",
    "# Send request\n",
    "response = requests.get(search_url, headers=headers, params=query_params)\n",
    "\n",
    "# Check response\n",
    "if response.status_code != 200:\n",
    "    raise Exception(f\"Request failed: {response.status_code}\\n{response.text}\")\n",
    "\n",
    "data = response.json()\n",
    "\n",
    "# Extract data\n",
    "tweets = data.get(\"data\", [])\n",
    "users = {u[\"id\"]: u for u in data.get(\"includes\", {}).get(\"users\", [])}\n",
    "\n",
    "# Prepare data rows\n",
    "results = []\n",
    "for tweet in tweets:\n",
    "    user = users.get(tweet[\"author_id\"], {})\n",
    "    metrics = tweet.get(\"public_metrics\", {})\n",
    "    results.append({\n",
    "        \"url\": f\"https://twitter.com/{user.get('username')}/status/{tweet['id']}\",\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"datum\": tweet.get(\"created_at\", \"\"),\n",
    "        \"username\": user.get(\"username\", \"\"),\n",
    "        \"name\": user.get(\"name\", \"\"),\n",
    "        \"caption\": tweet.get(\"text\", \"\"),\n",
    "        \"likes\": metrics.get(\"like_count\", 0),\n",
    "        \"retweets\": metrics.get(\"retweet_count\", 0),\n",
    "        \"replies\": metrics.get(\"reply_count\", 0)\n",
    "    })\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(results)\n",
    "df.sort_values(by=\"likes\", ascending=False, inplace=True)  # Sort by popularity\n",
    "df.to_csv(\"../raw/twitter_api_top_tweets.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Saved {len(df)} tweets to twitter_api_top_tweets.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39bac9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1744658724\n"
     ]
    }
   ],
   "source": [
    "print(response.headers.get(\"x-rate-limit-remaining\"))\n",
    "print(response.headers.get(\"x-rate-limit-reset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b392924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ntscraper\n",
      "  Downloading ntscraper-0.3.18-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: requests>=2.28 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from ntscraper) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from ntscraper) (4.13.3)\n",
      "Requirement already satisfied: lxml>=4.9 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from ntscraper) (5.3.2)\n",
      "Requirement already satisfied: tqdm>=4.66 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from ntscraper) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from beautifulsoup4>=4.11->ntscraper) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from beautifulsoup4>=4.11->ntscraper) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from requests>=2.28->ntscraper) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from requests>=2.28->ntscraper) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from requests>=2.28->ntscraper) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from requests>=2.28->ntscraper) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from tqdm>=4.66->ntscraper) (0.4.6)\n",
      "Downloading ntscraper-0.3.18-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: ntscraper\n",
      "Successfully installed ntscraper-0.3.18\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ntscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b94a488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing instances: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.86it/s]\n"
     ]
    }
   ],
   "source": [
    "from ntscraper import Nitter\n",
    "\n",
    "scraper = Nitter(log_level=1, skip_instance_check=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7baae61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snscrape in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (0.7.0.20230622)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from snscrape) (2.32.3)\n",
      "Requirement already satisfied: lxml in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from snscrape) (5.3.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from snscrape) (4.13.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from snscrape) (3.18.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from beautifulsoup4->snscrape) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from beautifulsoup4->snscrape) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from requests[socks]->snscrape) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from requests[socks]->snscrape) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from requests[socks]->snscrape) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from requests[socks]->snscrape) (2025.1.31)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from requests[socks]->snscrape) (1.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install snscrape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcc62110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14-Apr-25 21:22:10 - Retrieving scroll page None\n",
      "14-Apr-25 21:22:10 - Retrieving https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22Klima%20lang%3Ade%20since%3A2024-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D\n",
      "14-Apr-25 21:22:10 - Retrieved https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22Klima%20lang%3Ade%20since%3A2024-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D: 404\n",
      "14-Apr-25 21:22:10 - Retrieving guest token\n",
      "14-Apr-25 21:22:10 - Retrieving https://twitter.com/search?f=live&lang=en&q=Klima+lang%3Ade+since%3A2024-01-01&src=spelling_expansion_revert_click\n",
      "14-Apr-25 21:22:10 - Retrieved https://twitter.com/search?f=live&lang=en&q=Klima+lang%3Ade+since%3A2024-01-01&src=spelling_expansion_revert_click (redirected to https://x.com/search?f=live&lang=en&q=Klima+lang%3Ade+since%3A2024-01-01&src=spelling_expansion_revert_click): 200\n",
      "14-Apr-25 21:22:10 - Retrieving guest token via API\n",
      "14-Apr-25 21:22:10 - Retrieving https://api.twitter.com/1.1/guest/activate.json\n",
      "14-Apr-25 21:22:10 - Retrieved https://api.twitter.com/1.1/guest/activate.json: 200\n",
      "14-Apr-25 21:22:10 - Error retrieving https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22Klima%20lang%3Ade%20since%3A2024-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D: blocked (404), retrying\n",
      "14-Apr-25 21:22:10 - Waiting 1 seconds\n",
      "14-Apr-25 21:22:12 - Retrieving https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22Klima%20lang%3Ade%20since%3A2024-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D\n",
      "14-Apr-25 21:22:12 - Retrieved https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22Klima%20lang%3Ade%20since%3A2024-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D: 404\n",
      "14-Apr-25 21:22:12 - Retrieving guest token\n",
      "14-Apr-25 21:22:12 - Retrieving https://twitter.com/search?f=live&lang=en&q=Klima+lang%3Ade+since%3A2024-01-01&src=spelling_expansion_revert_click\n",
      "14-Apr-25 21:22:12 - Retrieved https://twitter.com/search?f=live&lang=en&q=Klima+lang%3Ade+since%3A2024-01-01&src=spelling_expansion_revert_click (redirected to https://x.com/search?f=live&lang=en&q=Klima+lang%3Ade+since%3A2024-01-01&src=spelling_expansion_revert_click): 200\n",
      "14-Apr-25 21:22:12 - Retrieving guest token via API\n",
      "14-Apr-25 21:22:12 - Retrieving https://api.twitter.com/1.1/guest/activate.json\n",
      "14-Apr-25 21:22:12 - Retrieved https://api.twitter.com/1.1/guest/activate.json: 200\n",
      "14-Apr-25 21:22:12 - Error retrieving https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22Klima%20lang%3Ade%20since%3A2024-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D: blocked (404), retrying\n",
      "14-Apr-25 21:22:12 - Waiting 2 seconds\n",
      "14-Apr-25 21:22:14 - Retrieving https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22Klima%20lang%3Ade%20since%3A2024-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D\n",
      "14-Apr-25 21:22:14 - Retrieved https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22Klima%20lang%3Ade%20since%3A2024-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D: 404\n",
      "14-Apr-25 21:22:14 - Retrieving guest token\n",
      "14-Apr-25 21:22:14 - Retrieving https://twitter.com/search?f=live&lang=en&q=Klima+lang%3Ade+since%3A2024-01-01&src=spelling_expansion_revert_click\n",
      "14-Apr-25 21:22:15 - Retrieved https://twitter.com/search?f=live&lang=en&q=Klima+lang%3Ade+since%3A2024-01-01&src=spelling_expansion_revert_click (redirected to https://x.com/search?f=live&lang=en&q=Klima+lang%3Ade+since%3A2024-01-01&src=spelling_expansion_revert_click): 200\n",
      "14-Apr-25 21:22:15 - Retrieving guest token via API\n",
      "14-Apr-25 21:22:15 - Retrieving https://api.twitter.com/1.1/guest/activate.json\n",
      "14-Apr-25 21:22:15 - Retrieved https://api.twitter.com/1.1/guest/activate.json: 200\n",
      "14-Apr-25 21:22:15 - Error retrieving https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22Klima%20lang%3Ade%20since%3A2024-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D: blocked (404), retrying\n",
      "14-Apr-25 21:22:15 - Waiting 4 seconds\n",
      "14-Apr-25 21:22:19 - Retrieving https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22Klima%20lang%3Ade%20since%3A2024-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D\n",
      "14-Apr-25 21:22:19 - Retrieved https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22Klima%20lang%3Ade%20since%3A2024-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D: 404\n",
      "14-Apr-25 21:22:19 - Retrieving guest token\n",
      "14-Apr-25 21:22:19 - Retrieving https://twitter.com/search?f=live&lang=en&q=Klima+lang%3Ade+since%3A2024-01-01&src=spelling_expansion_revert_click\n",
      "14-Apr-25 21:22:19 - Retrieved https://twitter.com/search?f=live&lang=en&q=Klima+lang%3Ade+since%3A2024-01-01&src=spelling_expansion_revert_click (redirected to https://x.com/search?f=live&lang=en&q=Klima+lang%3Ade+since%3A2024-01-01&src=spelling_expansion_revert_click): 200\n",
      "14-Apr-25 21:22:19 - Retrieving guest token via API\n",
      "14-Apr-25 21:22:19 - Retrieving https://api.twitter.com/1.1/guest/activate.json\n",
      "14-Apr-25 21:22:20 - Retrieved https://api.twitter.com/1.1/guest/activate.json: 200\n",
      "14-Apr-25 21:22:20 - Error retrieving https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22Klima%20lang%3Ade%20since%3A2024-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D: blocked (404)\n",
      "14-Apr-25 21:22:20 - 4 requests to https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22Klima%20lang%3Ade%20since%3A2024-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D failed, giving up.\n",
      "14-Apr-25 21:22:20 - Errors: blocked (404), blocked (404), blocked (404), blocked (404)\n",
      "Fehler beim Scrapen: 4 requests to https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22Klima%20lang%3Ade%20since%3A2024-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D failed, giving up.\n",
      "Tweets gespeichert in: tweets_scrape_output.csv\n"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "query = 'Klima lang:de since:2024-01-01'\n",
    "max_tweets = 50\n",
    "tweets = []\n",
    "\n",
    "try:\n",
    "    for i, tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):\n",
    "        if i >= max_tweets:\n",
    "            break\n",
    "        tweets.append({\n",
    "            'Datum': tweet.date,\n",
    "            'User': tweet.user.username,\n",
    "            'Name': tweet.user.displayname,\n",
    "            'Text': tweet.content,\n",
    "            'Likes': tweet.likeCount,\n",
    "            'Retweets': tweet.retweetCount,\n",
    "            'Replies': tweet.replyCount,\n",
    "            'URL': tweet.url\n",
    "        })\n",
    "    print(f\"{len(tweets)} Tweets erfolgreich gesammelt.\")\n",
    "except Exception as e:\n",
    "    print(f\"Fehler beim Scrapen: {e}\")\n",
    "finally:\n",
    "    df = pd.DataFrame(tweets)\n",
    "    output_path = \"tweets_scrape_output.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Tweets gespeichert in: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9af1f68",
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tweets_data\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# üîÑ Ausf√ºhren & speichern\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKlima\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tweets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n\u001b[0;32m     56\u001b[0m df\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtweets_playwright_scrape.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[16], line 8\u001b[0m, in \u001b[0;36mscrape_tweets\u001b[1;34m(keyword, max_tweets)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mscrape_tweets\u001b[39m(keyword\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKlima\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_tweets\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m      6\u001b[0m     tweets_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m sync_playwright() \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m      9\u001b[0m         browser \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mchromium\u001b[38;5;241m.\u001b[39mlaunch(headless\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m         page \u001b[38;5;241m=\u001b[39m browser\u001b[38;5;241m.\u001b[39mnew_page()\n",
      "File \u001b[1;32mc:\\Users\\SofiePischl\\Documents\\01_HdM\\10_ML_OPS\\TrendAnalyse Social Media\\.venv\\lib\\site-packages\\playwright\\sync_api\\_context_manager.py:47\u001b[0m, in \u001b[0;36mPlaywrightContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_own_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m---> 47\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Error(\n\u001b[0;32m     48\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"It looks like you are using Playwright Sync API inside the asyncio loop.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03mPlease use the Async API instead.\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m             )\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;66;03m# Create a new fiber for the protocol dispatcher. It will be pumping events\u001b[39;00m\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;66;03m# until the end of times. We will pass control to that fiber every time we\u001b[39;00m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;66;03m# block while waiting for a response.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgreenlet_main\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mError\u001b[0m: It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead."
     ]
    }
   ],
   "source": [
    "from playwright.sync_api import sync_playwright\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def scrape_tweets(keyword=\"Klima\", max_tweets=20):\n",
    "    tweets_data = []\n",
    "\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=True)\n",
    "        page = browser.new_page()\n",
    "\n",
    "        search_url = f\"https://x.com/search?q={keyword}%20lang%3Ade&f=live\"\n",
    "        page.goto(search_url)\n",
    "        time.sleep(5)\n",
    "\n",
    "        last_height = 0\n",
    "        while len(tweets_data) < max_tweets:\n",
    "            tweet_elements = page.query_selector_all('article')\n",
    "\n",
    "            for tweet in tweet_elements:\n",
    "                try:\n",
    "                    content = tweet.inner_text()\n",
    "                    lines = content.split('\\n')\n",
    "                    username = lines[0] if lines else \"\"\n",
    "                    text = '\\n'.join(lines[2:-4]) if len(lines) > 6 else content\n",
    "                    timestamp = tweet.query_selector('time').get_attribute('datetime') if tweet.query_selector('time') else ''\n",
    "                    tweet_url = tweet.query_selector('a:has(time)').get_attribute('href') if tweet.query_selector('a:has(time)') else ''\n",
    "                    full_url = f\"https://x.com{tweet_url}\" if tweet_url else ''\n",
    "\n",
    "                    if any(d['url'] == full_url for d in tweets_data):\n",
    "                        continue  # Already captured\n",
    "\n",
    "                    tweets_data.append({\n",
    "                        \"username\": username,\n",
    "                        \"text\": text,\n",
    "                        \"timestamp\": timestamp,\n",
    "                        \"url\": full_url\n",
    "                    })\n",
    "\n",
    "                    if len(tweets_data) >= max_tweets:\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "\n",
    "            # Scroll down\n",
    "            page.mouse.wheel(0, 2000)\n",
    "            time.sleep(2)\n",
    "\n",
    "        browser.close()\n",
    "\n",
    "    return tweets_data\n",
    "\n",
    "# üîÑ Ausf√ºhren & speichern\n",
    "data = scrape_tweets(\"Klima\", max_tweets=30)\n",
    "df = pd.DataFrame(data)\n",
    "df.to_excel(\"tweets_playwright_scrape.xlsx\", index=False)\n",
    "print(f\"{len(df)} Tweets gespeichert in 'tweets_playwright_scrape.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2717e9e3",
   "metadata": {},
   "source": [
    "# 4. TikTok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dce2d26",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Trending() takes no arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mTikTokApi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TikTokApi\n\u001b[0;32m      3\u001b[0m api \u001b[38;5;241m=\u001b[39m TikTokApi()\n\u001b[1;32m----> 4\u001b[0m trending \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrending\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video \u001b[38;5;129;01min\u001b[39;00m trending:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthor: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muniqueId\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Trending() takes no arguments"
     ]
    }
   ],
   "source": [
    "from TikTokApi import TikTokApi\n",
    "\n",
    "api = TikTokApi()\n",
    "trending = api.trending(count=10)\n",
    "\n",
    "for video in trending:\n",
    "    print(f\"Author: {video['author']['uniqueId']}\")\n",
    "    print(f\"Desc: {video['desc']}\")\n",
    "    print(f\"Video URL: {video['video']['downloadAddr']}\")\n",
    "    print('-' * 30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
