{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bdbda90",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 16px;\">\n",
    "    <strong>Course:</strong> Machine Learning Operations |\n",
    "    <strong>Lecturer:</strong> Prof. Dr. Klotz |\n",
    "    <strong>Date:</strong> 17.05.2025 |\n",
    "    <strong>Name:</strong> Sofie Pischl\n",
    "</div>\n",
    "\n",
    "# <center>Data Collection </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a95362a",
   "metadata": {},
   "source": [
    "Konzept & Inhalt:\n",
    "\n",
    "Daten von den größten Social media Apps sollen abgegriffen werden. besonderer Fokus auf Texten.\n",
    "\n",
    "1. Setup & Imports\n",
    "2. Reddit: Hot Posts aus Subreddits\n",
    "3. Instagram: Top Posts per Scraping/API (light)\n",
    "4. Twitter: Aktuelle Tweets via snscrape oder Tweepy\n",
    "5. TikTok: Trending Videos\n",
    "6. YouTube: Trending Videos (API/Scraping)\n",
    "7. Fazit & Learnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b780b59",
   "metadata": {},
   "source": [
    "----\n",
    "# 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dceb2f7",
   "metadata": {},
   "source": [
    "Zunächst werden alle benötigten Libraries importiert:\n",
    "- `praw` für den Reddit-Zugriff\n",
    "- `pandas` für Datenverarbeitung\n",
    "- `datetime` für Timestamps\n",
    "- `dotenv` für Umgebungsvariablen\n",
    "- `pathlib` für saubere Pfadangaben\n",
    "- `logging` für Fehlerprotokollierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96b79d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Load from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa66e0a",
   "metadata": {},
   "source": [
    "# 2. Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089b93d3",
   "metadata": {},
   "source": [
    "### Funktionen:\n",
    "- Authentifizierung über OAuth2 via `praw`\n",
    "- Abruf der `hot`-Beiträge aus ausgewählten Subreddits\n",
    "- Speicherung als `.csv` unter `/data/raw/reddit_data.csv`\n",
    "- Fehler-Handling und Logging integriert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d039ab76",
   "metadata": {},
   "source": [
    "**Authentifizierung**\n",
    "\n",
    "Zur Authentifizierung an der Reddit-API wird ein Reddit-Objekt der Bibliothek praw initialisiert. Die benötigten Zugangsdaten – client_id, client_secret und user_agent – werden aus einer .env-Datei geladen, um die Trennung von Code und Konfiguration zu gewährleisten und Sicherheitsrisiken zu minimieren.\n",
    "\n",
    "Diese Parameter dienen der eindeutigen Identifikation der Anwendung gegenüber der API und sind notwendig, um Zugriff auf Reddit-Inhalte zu erhalten. Der user_agent ermöglicht zudem die Rückverfolgbarkeit von API-Anfragen seitens Reddit. Ohne diese Authentifizierung ist ein reguliertes, automatisiertes Scraping nicht zulässig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5789b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=(os.getenv(\"REDDIT_ID\")),\n",
    "    client_secret=(os.getenv(\"REDDIT_SECRET\")),\n",
    "    user_agent=(os.getenv(\"USER_AGENT\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c34f9d7",
   "metadata": {},
   "source": [
    "**Logging-Konfiguration**\n",
    "\n",
    "Bevor das Reddit-Scraping startet, wird ein Logging-System eingerichtet. Dazu wird zunächst ein Pfad zur Log-Datei definiert – in diesem Fall `logs/reddit.log`. Falls das Verzeichnis `logs/` noch nicht existiert, wird es automatisch erstellt. Anschließend wird das Python-Logging so konfiguriert, dass alle Log-Meldungen in diese Datei geschrieben werden.\n",
    "\n",
    "Die Konfiguration legt fest, dass nur Meldungen ab dem Schweregrad `INFO` gespeichert werden. Außerdem wird das Format der Einträge so definiert, dass jeder Log-Eintrag einen Zeitstempel, den Log-Level (wie `INFO` oder `ERROR`) sowie die eigentliche Nachricht enthält. So entsteht eine nachvollziehbare Chronik über den Ablauf und mögliche Fehler des Scripts.\n",
    "\n",
    "Ein typischer Eintrag könnte zum Beispiel so aussehen:\n",
    "\n",
    "```\n",
    "2025-04-19 14:33:07,512 - INFO - Starte Reddit-Scraping...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b20f953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Logging aktiv unter: C:\\Users\\SofiePischl\\Documents\\01_HdM\\10_ML_OPS\\TrendAnalyse Social Media\\logs\\reddit.log\n"
     ]
    }
   ],
   "source": [
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "# Logging einrichten\n",
    "log_path = Path(\"../logs/reddit.log\")\n",
    "log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    filename=log_path,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "print(f\" Logging aktiv unter: {log_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6781e111",
   "metadata": {},
   "source": [
    "**Reddit-Datensammlung mittels Python und PRAW**\n",
    "\n",
    "Die Funktion `scrape_reddit()` dient der systematischen Erhebung textbasierter Inhalte aus der Social-Media-Plattform **Reddit**. Ziel ist es, strukturierte Daten zur Analyse von Trendthemen zu generieren. Zur Umsetzung wird die Bibliothek `praw` (Python Reddit API Wrapper) verwendet, die eine komfortable Schnittstelle zur Reddit-API bereitstellt.\n",
    "\n",
    "Nach der Initialisierung der Protokollierung via `logging` erfolgt die Authentifizierung an der Reddit-API. Hierfür wird ein `Reddit`-Objekt instanziiert, wobei sensible Zugangsdaten wie `client_id`, `client_secret` und `user_agent` aus einer `.env`-Datei geladen werden. Dieses Vorgehen ermöglicht eine sichere Trennung von Konfiguration und Codebasis und schützt vor dem unbeabsichtigten Leaken von API-Schlüsseln.\n",
    "\n",
    "Im Anschluss wird eine Liste von Subreddits definiert, die sowohl populäre als auch als „trending“ markierte Communities umfasst. Aus diesen Subreddits werden jeweils bis zu 100 Beiträge aus dem Hot-Feed abgerufen. Dieses Verfahren stellt sicher, dass aktuelle, stark diskutierte Inhalte gesammelt werden, die ein hohes Relevanzpotenzial für Trendanalysen aufweisen.\n",
    "\n",
    "Die Datenextraktion erfolgt über eine doppelte Schleife: Für jedes Subreddit werden Hot-Beiträge iteriert, wobei ausschließlich „self-posts“ berücksichtigt werden. Diese beinhalten keine externen Links und ermöglichen dadurch eine fokussierte Analyse des vom Nutzer selbst verfassten Textinhalts. Pro Beitrag werden zentrale Metriken wie Titel, Text, Anzahl der Kommentare, Upvotes, Erstellungszeitpunkt sowie die URL gespeichert. Zur besseren zeitlichen Einordnung wird außerdem ein einheitlicher Zeitstempel für alle Einträge vergeben.\n",
    "\n",
    "Die gesammelten Beiträge werden in einem `pandas.DataFrame` strukturiert und anschließend unter `data/raw/reddit_data.csv` abgespeichert. Dabei wird sichergestellt, dass benötigte Verzeichnisse automatisch erstellt werden. Falls bereits Daten vorhanden sind, werden die neuen Einträge angehängt und anschließend Duplikate basierend auf Titel, Textinhalt und Subreddit entfernt. Die finale Version wird ohne Index in die CSV-Datei geschrieben.\n",
    "\n",
    "Abschließend wird die Anzahl der gespeicherten Beiträge im Logfile vermerkt. Etwaige Fehler werden während der Ausführung abgefangen und entsprechend protokolliert. Die Funktion kann sowohl als Modul importiert als auch direkt per Skriptausführung genutzt werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f757739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Gespeichert unter: C:\\Users\\SofiePischl\\Documents\\01_HdM\\10_ML_OPS\\TrendAnalyse Social Media\\app\\data\\raw\\reddit_data.csv\n"
     ]
    }
   ],
   "source": [
    "def scrape_reddit():\n",
    "    try:\n",
    "        logging.info(\"Starte Reddit-Scraping...\")\n",
    "\n",
    "        subreddits = [\"all\", \"popular\", \"trendingreddits\", \"trendingsubreddits\"]\n",
    "        post_limit = 100\n",
    "        all_posts = []\n",
    "        scrape_time = datetime.now()\n",
    "\n",
    "        for sub in subreddits:\n",
    "            subreddit = reddit.subreddit(sub)\n",
    "            for post in subreddit.hot(limit=post_limit):\n",
    "                if post.is_self:\n",
    "                    all_posts.append({\n",
    "                        \"subreddit\": sub,\n",
    "                        \"title\": post.title,\n",
    "                        \"text\": post.selftext,\n",
    "                        \"score\": post.score,\n",
    "                        \"comments\": post.num_comments,\n",
    "                        \"created\": datetime.fromtimestamp(post.created),\n",
    "                        \"url\": post.url,\n",
    "                        \"scraped_at\": scrape_time\n",
    "                    })\n",
    "\n",
    "        df = pd.DataFrame(all_posts)\n",
    "        csv_path = Path(\"../app/data/raw/reddit_data.csv\")\n",
    "        csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if csv_path.exists():\n",
    "            df_existing = pd.read_csv(csv_path)\n",
    "            df = pd.concat([df_existing, df], ignore_index=True)\n",
    "\n",
    "        df.drop_duplicates(subset=[\"title\", \"text\", \"subreddit\"], inplace=True)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\" Gespeichert unter: {csv_path.resolve()}\")\n",
    "\n",
    "        logging.info(f\"{len(df)} Einträge gespeichert unter {csv_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fehler beim Reddit-Scraping: {e}\")\n",
    "\n",
    "# Ausführung bei direktem Aufruf\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_reddit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba082732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>comments</th>\n",
       "      <th>created</th>\n",
       "      <th>url</th>\n",
       "      <th>scraped_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>popular</td>\n",
       "      <td>Is anyone else getting irritated with the new ...</td>\n",
       "      <td>I get it, it’s something I can put in my prefe...</td>\n",
       "      <td>2474</td>\n",
       "      <td>643</td>\n",
       "      <td>2025-04-19 01:53:11</td>\n",
       "      <td>https://www.reddit.com/r/ChatGPT/comments/1k2j...</td>\n",
       "      <td>2025-04-19 14:20:21.514955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>popular</td>\n",
       "      <td>What is the first thing you’d buy if you get f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5116</td>\n",
       "      <td>7975</td>\n",
       "      <td>2025-04-18 21:51:33</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/1k...</td>\n",
       "      <td>2025-04-19 14:23:21.324902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>popular</td>\n",
       "      <td>Under current law, the Social Security payroll...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9256</td>\n",
       "      <td>1062</td>\n",
       "      <td>2025-04-18 18:47:13</td>\n",
       "      <td>https://www.reddit.com/r/SocialSecurity/commen...</td>\n",
       "      <td>2025-04-19 14:23:21.324902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>trendingreddits</td>\n",
       "      <td>We are open again!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2024-09-27 20:14:11</td>\n",
       "      <td>https://www.reddit.com/r/TrendingReddits/comme...</td>\n",
       "      <td>2025-04-19 14:23:21.324902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>trendingreddits</td>\n",
       "      <td>Hi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-04-02 21:13:09</td>\n",
       "      <td>https://www.reddit.com/r/TrendingReddits/comme...</td>\n",
       "      <td>2025-04-19 14:23:21.324902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           subreddit                                              title  \\\n",
       "162          popular  Is anyone else getting irritated with the new ...   \n",
       "163          popular  What is the first thing you’d buy if you get f...   \n",
       "164          popular  Under current law, the Social Security payroll...   \n",
       "165  trendingreddits                                 We are open again!   \n",
       "166  trendingreddits                                                 Hi   \n",
       "\n",
       "                                                  text  score  comments  \\\n",
       "162  I get it, it’s something I can put in my prefe...   2474       643   \n",
       "163                                                NaN   5116      7975   \n",
       "164                                                NaN   9256      1062   \n",
       "165                                                NaN      4         2   \n",
       "166                                                NaN      1         4   \n",
       "\n",
       "                 created                                                url  \\\n",
       "162  2025-04-19 01:53:11  https://www.reddit.com/r/ChatGPT/comments/1k2j...   \n",
       "163  2025-04-18 21:51:33  https://www.reddit.com/r/AskReddit/comments/1k...   \n",
       "164  2025-04-18 18:47:13  https://www.reddit.com/r/SocialSecurity/commen...   \n",
       "165  2024-09-27 20:14:11  https://www.reddit.com/r/TrendingReddits/comme...   \n",
       "166  2025-04-02 21:13:09  https://www.reddit.com/r/TrendingReddits/comme...   \n",
       "\n",
       "                     scraped_at  \n",
       "162  2025-04-19 14:20:21.514955  \n",
       "163  2025-04-19 14:23:21.324902  \n",
       "164  2025-04-19 14:23:21.324902  \n",
       "165  2025-04-19 14:23:21.324902  \n",
       "166  2025-04-19 14:23:21.324902  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zeuge neuste Einträge\n",
    "df = pd.read_csv(\"../app/data/raw/reddit_data.csv\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c86300dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8152499",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Instagram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125499c",
   "metadata": {},
   "source": [
    "was nicht funktioniert hat: instaloader, playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00ba8d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a93c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e585a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTA_USERNAME= os.getenv(\"INSTA_USERNAME\")\n",
    "INSTA_PASSWORD= os.getenv(\"INSTA_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9727179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging aktiv unter: C:\\Users\\SofiePischl\\Documents\\01_HdM\\10_ML_OPS\\TrendAnalyse Social Media\\logs\\instagram.log\n"
     ]
    }
   ],
   "source": [
    "# Logging initialisieren\n",
    "log_path = Path(\"../logs/instagram.log\")\n",
    "log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    filename=log_path,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "print(f\"Logging aktiv unter: {log_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c344738",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_instagram_explore():\n",
    "    try:\n",
    "        logging.info(\"Starte Instagram-Explore-Scraping...\")\n",
    "\n",
    "        # Headless-Browser konfigurieren\n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless=new\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--window-size=1920x1080\")\n",
    "\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        driver.get(\"https://www.instagram.com/accounts/login/\")\n",
    "\n",
    "        # Login\n",
    "        username = os.getenv(\"INSTA_USERNAME\")\n",
    "        password = os.getenv(\"INSTA_PASSWORD\")\n",
    "\n",
    "        time.sleep(3)\n",
    "        driver.find_element(By.NAME, \"username\").send_keys(username)\n",
    "        driver.find_element(By.NAME, \"password\").send_keys(password)\n",
    "        driver.find_element(By.XPATH, \"//button[@type='submit']\").click()\n",
    "\n",
    "        time.sleep(7)  # Warte auf Login\n",
    "\n",
    "        # Gehe zur Explore-Seite\n",
    "        driver.get(\"https://www.instagram.com/explore/\")\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Seite scrollen, um mehr Posts zu laden\n",
    "        for _ in range(3):  # 3x scrollen → kannst du erhöhen\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(3)\n",
    "\n",
    "        # Beiträge extrahieren\n",
    "        posts = driver.find_elements(By.XPATH, '//a[contains(@href, \"/p/\")]')\n",
    "        post_data = []\n",
    "\n",
    "        for post in posts:\n",
    "            try:\n",
    "                url = post.get_attribute(\"href\")\n",
    "                img = post.find_element(By.TAG_NAME, \"img\")\n",
    "                img_url = img.get_attribute(\"src\")\n",
    "                description = img.get_attribute(\"alt\")\n",
    "\n",
    "                post_data.append({\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"url\": url,\n",
    "                    \"image\": img_url,\n",
    "                    \"description\": description\n",
    "                })\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Fehler beim Extrahieren eines Posts: {e}\")\n",
    "                continue\n",
    "\n",
    "        # In CSV speichern\n",
    "        csv_path = Path(\"../app/data/raw/instagram_data.csv\")\n",
    "        csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        df = pd.DataFrame(post_data)\n",
    "\n",
    "        if csv_path.exists():\n",
    "            df_existing = pd.read_csv(csv_path)\n",
    "            df = pd.concat([df_existing, df], ignore_index=True)\n",
    "\n",
    "        df.drop_duplicates(subset=[\"url\", \"image\", \"description\"], inplace=True)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "\n",
    "        logging.info(f\"{len(df)} Einträge gespeichert unter {csv_path}\")\n",
    "        driver.quit()\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fehler beim Instagram-Explore-Scraping: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e397062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jetzt ausführen\n",
    "scrape_instagram_explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c60440bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_instagram_post(url):\n",
    "    try:\n",
    "        logging.info(f\"Starte Instagram-Scraping für URL: {url}\")\n",
    "        \n",
    "        # Zielpfad vorbereiten\n",
    "        csv_path = Path(\"../app/data/raw/instagram_data.csv\")\n",
    "        csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Browser initialisieren\n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless=new\")  # im Hintergrund ausführen\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--window-size=1920x1080\")\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        wait = WebDriverWait(driver, 15)\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        # Beitrag laden\n",
    "        wait.until(EC.presence_of_element_located((By.TAG_NAME, \"article\")))\n",
    "\n",
    "        # Login-Popup schließen (falls vorhanden)\n",
    "        try:\n",
    "            close_btn = wait.until(EC.element_to_be_clickable((By.XPATH, \"//div[@role='dialog']//button\")))\n",
    "            close_btn.click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Username extrahieren\n",
    "        try:\n",
    "            username_elem = wait.until(EC.visibility_of_element_located(\n",
    "                (By.XPATH, \"//a[contains(@href, '/') and contains(@class, 'notranslate')]//span[1]\")\n",
    "            ))\n",
    "            username = username_elem.text.strip()\n",
    "        except:\n",
    "            username = \"\"\n",
    "\n",
    "        # Caption extrahieren\n",
    "        try:\n",
    "            caption_elem = wait.until(EC.visibility_of_element_located(\n",
    "                (By.XPATH, \"//div[@data-testid='post-comment-root']//span\")\n",
    "            ))\n",
    "            caption = caption_elem.text.strip()\n",
    "        except:\n",
    "            caption = \"\"\n",
    "\n",
    "        # Likes (optional)\n",
    "        try:\n",
    "            likes_elem = wait.until(EC.visibility_of_element_located(\n",
    "                (By.XPATH, \"//section//span[contains(text(), 'Gefällt')]\")\n",
    "            ))\n",
    "            likes = likes_elem.text.strip()\n",
    "        except:\n",
    "            likes = \"\"\n",
    "\n",
    "        # Timestamp + Struktur\n",
    "        data = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"url\": url,\n",
    "            \"username\": username,\n",
    "            \"caption\": caption,\n",
    "            \"likes\": likes\n",
    "        }\n",
    "\n",
    "        df_new = pd.DataFrame([data])\n",
    "\n",
    "        if csv_path.exists():\n",
    "            df_existing = pd.read_csv(csv_path)\n",
    "            df_new = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "\n",
    "        df_new.drop_duplicates(subset=[\"url\", \"caption\", \"username\"], inplace=True)\n",
    "        df_new.to_csv(csv_path, index=False)\n",
    "\n",
    "        logging.info(f\"Erfolgreich gespeichert unter {csv_path}\")\n",
    "        driver.quit()\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fehler beim Scrapen von {url}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9f1d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispielaufruf\n",
    "scrape_instagram_post(\"https://www.instagram.com/p/DICWDtYNq7E/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafbbd32",
   "metadata": {},
   "source": [
    "# 1. TWITTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0abcb5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c082e7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8909893b",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Request failed: 429\n{\"title\":\"Too Many Requests\",\"detail\":\"Too Many Requests\",\"type\":\"about:blank\",\"status\":429}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Check response\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Extract data\u001b[39;00m\n",
      "\u001b[1;31mException\u001b[0m: Request failed: 429\n{\"title\":\"Too Many Requests\",\"detail\":\"Too Many Requests\",\"type\":\"about:blank\",\"status\":429}"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Your bearer token from Twitter Developer Portal\n",
    "BEARER_TOKEN = os.getenv(\"X_BEARER_TOKEN\")\n",
    "\n",
    "# Twitter API endpoint for recent tweets\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "\n",
    "# Search parameters – open topic\n",
    "query_params = {\n",
    "    'query': 'Twitter lang:de -is:retweet',  # No keyword, just German tweets\n",
    "    'max_results': 50,  # Max per request (10–100)\n",
    "    'tweet.fields': 'created_at,public_metrics,text,author_id',\n",
    "    'expansions': 'author_id',\n",
    "    'user.fields': 'username,name'\n",
    "}\n",
    "\n",
    "# Set headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {BEARER_TOKEN}\"\n",
    "}\n",
    "\n",
    "# Send request\n",
    "response = requests.get(search_url, headers=headers, params=query_params)\n",
    "\n",
    "# Check response\n",
    "if response.status_code != 200:\n",
    "    raise Exception(f\"Request failed: {response.status_code}\\n{response.text}\")\n",
    "\n",
    "data = response.json()\n",
    "\n",
    "# Extract data\n",
    "tweets = data.get(\"data\", [])\n",
    "users = {u[\"id\"]: u for u in data.get(\"includes\", {}).get(\"users\", [])}\n",
    "\n",
    "# Prepare data rows\n",
    "results = []\n",
    "for tweet in tweets:\n",
    "    user = users.get(tweet[\"author_id\"], {})\n",
    "    metrics = tweet.get(\"public_metrics\", {})\n",
    "    results.append({\n",
    "        \"url\": f\"https://twitter.com/{user.get('username')}/status/{tweet['id']}\",\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"datum\": tweet.get(\"created_at\", \"\"),\n",
    "        \"username\": user.get(\"username\", \"\"),\n",
    "        \"name\": user.get(\"name\", \"\"),\n",
    "        \"caption\": tweet.get(\"text\", \"\"),\n",
    "        \"likes\": metrics.get(\"like_count\", 0),\n",
    "        \"retweets\": metrics.get(\"retweet_count\", 0),\n",
    "        \"replies\": metrics.get(\"reply_count\", 0)\n",
    "    })\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(results)\n",
    "df.sort_values(by=\"likes\", ascending=False, inplace=True)  # Sort by popularity\n",
    "df.to_csv(\"../raw/twitter_api_top_tweets.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Saved {len(df)} tweets to twitter_api_top_tweets.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39bac9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1744658724\n"
     ]
    }
   ],
   "source": [
    "print(response.headers.get(\"x-rate-limit-remaining\"))\n",
    "print(response.headers.get(\"x-rate-limit-reset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b392924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ntscraper\n",
      "  Downloading ntscraper-0.3.18-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: requests>=2.28 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from ntscraper) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from ntscraper) (4.13.3)\n",
      "Requirement already satisfied: lxml>=4.9 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from ntscraper) (5.3.2)\n",
      "Requirement already satisfied: tqdm>=4.66 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from ntscraper) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from beautifulsoup4>=4.11->ntscraper) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from beautifulsoup4>=4.11->ntscraper) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from requests>=2.28->ntscraper) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from requests>=2.28->ntscraper) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from requests>=2.28->ntscraper) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from requests>=2.28->ntscraper) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from tqdm>=4.66->ntscraper) (0.4.6)\n",
      "Downloading ntscraper-0.3.18-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: ntscraper\n",
      "Successfully installed ntscraper-0.3.18\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ntscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b94a488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing instances: 100%|██████████| 5/5 [00:01<00:00,  2.86it/s]\n"
     ]
    }
   ],
   "source": [
    "from ntscraper import Nitter\n",
    "\n",
    "scraper = Nitter(log_level=1, skip_instance_check=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7baae61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snscrape in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (0.7.0.20230622)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from snscrape) (2.32.3)\n",
      "Requirement already satisfied: lxml in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from snscrape) (5.3.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from snscrape) (4.13.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from snscrape) (3.18.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from beautifulsoup4->snscrape) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from beautifulsoup4->snscrape) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from requests[socks]->snscrape) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from requests[socks]->snscrape) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from requests[socks]->snscrape) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from requests[socks]->snscrape) (2025.1.31)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\sofiepischl\\documents\\01_hdm\\10_ml_ops\\trendanalyse social media\\.venv\\lib\\site-packages (from requests[socks]->snscrape) (1.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install snscrape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcc62110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14-Apr-25 21:22:10 - Retrieving scroll page None\n",
      "14-Apr-25 21:22:10 - Retrieving https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22Klima%20lang%3Ade%20since%3A2024-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D\n",
      "14-Apr-25 21:22:10 - Retrieved https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22Klima%20lang%3Ade%20since%3A2024-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D: 404\n",
      "14-Apr-25 21:22:10 - Retrieving guest token\n",
      "14-Apr-25 21:22:10 - Retrieving https://twitter.com/search?f=live&lang=en&q=Klima+lang%3Ade+since%3A2024-01-01&src=spelling_expansion_revert_click\n",
      "14-Apr-25 21:22:10 - Retrieved https://twitter.com/search?f=live&lang=en&q=Klima+lang%3Ade+since%3A2024-01-01&src=spelling_expansion_revert_click (redirected to https://x.com/search?f=live&lang=en&q=Klima+lang%3Ade+since%3A2024-01-01&src=spelling_expansion_revert_click): 200\n",
      "14-Apr-25 21:22:10 - Retrieving guest token via API\n",
      "14-Apr-25 21:22:10 - Retrieving https://api.twitter.com/1.1/guest/activate.json\n",
      "14-Apr-25 21:22:10 - Retrieved https://api.twitter.com/1.1/guest/activate.json: 200\n",
      "14-Apr-25 21:22:10 - Error retrieving https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22Klima%20lang%3Ade%20since%3A2024-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D: blocked (404), retrying\n",
      "14-Apr-25 21:22:10 - Waiting 1 seconds\n",
      "14-Apr-25 21:22:12 - Retrieving https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22Klima%20lang%3Ade%20since%3A2024-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D\n",
      "14-Apr-25 21:22:12 - Retrieved https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22Klima%20lang%3Ade%20since%3A2024-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D: 404\n",
      "14-Apr-25 21:22:12 - Retrieving guest token\n",
      "14-Apr-25 21:22:12 - Retrieving https://twitter.com/search?f=live&lang=en&q=Klima+lang%3Ade+since%3A2024-01-01&src=spelling_expansion_revert_click\n",
      "14-Apr-25 21:22:12 - Retrieved https://twitter.com/search?f=live&lang=en&q=Klima+lang%3Ade+since%3A2024-01-01&src=spelling_expansion_revert_click (redirected to https://x.com/search?f=live&lang=en&q=Klima+lang%3Ade+since%3A2024-01-01&src=spelling_expansion_revert_click): 200\n",
      "14-Apr-25 21:22:12 - Retrieving guest token via API\n",
      "14-Apr-25 21:22:12 - Retrieving https://api.twitter.com/1.1/guest/activate.json\n",
      "14-Apr-25 21:22:12 - Retrieved https://api.twitter.com/1.1/guest/activate.json: 200\n",
      "14-Apr-25 21:22:12 - Error retrieving https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22Klima%20lang%3Ade%20since%3A2024-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D: blocked (404), retrying\n",
      "14-Apr-25 21:22:12 - Waiting 2 seconds\n",
      "14-Apr-25 21:22:14 - Retrieving https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22Klima%20lang%3Ade%20since%3A2024-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D\n",
      "14-Apr-25 21:22:14 - Retrieved https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22Klima%20lang%3Ade%20since%3A2024-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D: 404\n",
      "14-Apr-25 21:22:14 - Retrieving guest token\n",
      "14-Apr-25 21:22:14 - Retrieving https://twitter.com/search?f=live&lang=en&q=Klima+lang%3Ade+since%3A2024-01-01&src=spelling_expansion_revert_click\n",
      "14-Apr-25 21:22:15 - Retrieved https://twitter.com/search?f=live&lang=en&q=Klima+lang%3Ade+since%3A2024-01-01&src=spelling_expansion_revert_click (redirected to https://x.com/search?f=live&lang=en&q=Klima+lang%3Ade+since%3A2024-01-01&src=spelling_expansion_revert_click): 200\n",
      "14-Apr-25 21:22:15 - Retrieving guest token via API\n",
      "14-Apr-25 21:22:15 - Retrieving https://api.twitter.com/1.1/guest/activate.json\n",
      "14-Apr-25 21:22:15 - Retrieved https://api.twitter.com/1.1/guest/activate.json: 200\n",
      "14-Apr-25 21:22:15 - Error retrieving https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22Klima%20lang%3Ade%20since%3A2024-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D: blocked (404), retrying\n",
      "14-Apr-25 21:22:15 - Waiting 4 seconds\n",
      "14-Apr-25 21:22:19 - Retrieving https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22Klima%20lang%3Ade%20since%3A2024-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D\n",
      "14-Apr-25 21:22:19 - Retrieved https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22Klima%20lang%3Ade%20since%3A2024-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D: 404\n",
      "14-Apr-25 21:22:19 - Retrieving guest token\n",
      "14-Apr-25 21:22:19 - Retrieving https://twitter.com/search?f=live&lang=en&q=Klima+lang%3Ade+since%3A2024-01-01&src=spelling_expansion_revert_click\n",
      "14-Apr-25 21:22:19 - Retrieved https://twitter.com/search?f=live&lang=en&q=Klima+lang%3Ade+since%3A2024-01-01&src=spelling_expansion_revert_click (redirected to https://x.com/search?f=live&lang=en&q=Klima+lang%3Ade+since%3A2024-01-01&src=spelling_expansion_revert_click): 200\n",
      "14-Apr-25 21:22:19 - Retrieving guest token via API\n",
      "14-Apr-25 21:22:19 - Retrieving https://api.twitter.com/1.1/guest/activate.json\n",
      "14-Apr-25 21:22:20 - Retrieved https://api.twitter.com/1.1/guest/activate.json: 200\n",
      "14-Apr-25 21:22:20 - Error retrieving https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22Klima%20lang%3Ade%20since%3A2024-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D: blocked (404)\n",
      "14-Apr-25 21:22:20 - 4 requests to https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22Klima%20lang%3Ade%20since%3A2024-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D failed, giving up.\n",
      "14-Apr-25 21:22:20 - Errors: blocked (404), blocked (404), blocked (404), blocked (404)\n",
      "Fehler beim Scrapen: 4 requests to https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22Klima%20lang%3Ade%20since%3A2024-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D failed, giving up.\n",
      "Tweets gespeichert in: tweets_scrape_output.csv\n"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "query = 'Klima lang:de since:2024-01-01'\n",
    "max_tweets = 50\n",
    "tweets = []\n",
    "\n",
    "try:\n",
    "    for i, tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):\n",
    "        if i >= max_tweets:\n",
    "            break\n",
    "        tweets.append({\n",
    "            'Datum': tweet.date,\n",
    "            'User': tweet.user.username,\n",
    "            'Name': tweet.user.displayname,\n",
    "            'Text': tweet.content,\n",
    "            'Likes': tweet.likeCount,\n",
    "            'Retweets': tweet.retweetCount,\n",
    "            'Replies': tweet.replyCount,\n",
    "            'URL': tweet.url\n",
    "        })\n",
    "    print(f\"{len(tweets)} Tweets erfolgreich gesammelt.\")\n",
    "except Exception as e:\n",
    "    print(f\"Fehler beim Scrapen: {e}\")\n",
    "finally:\n",
    "    df = pd.DataFrame(tweets)\n",
    "    output_path = \"tweets_scrape_output.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Tweets gespeichert in: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9af1f68",
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tweets_data\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# 🔄 Ausführen & speichern\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKlima\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tweets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n\u001b[0;32m     56\u001b[0m df\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtweets_playwright_scrape.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[16], line 8\u001b[0m, in \u001b[0;36mscrape_tweets\u001b[1;34m(keyword, max_tweets)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mscrape_tweets\u001b[39m(keyword\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKlima\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_tweets\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m      6\u001b[0m     tweets_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m sync_playwright() \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m      9\u001b[0m         browser \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mchromium\u001b[38;5;241m.\u001b[39mlaunch(headless\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m         page \u001b[38;5;241m=\u001b[39m browser\u001b[38;5;241m.\u001b[39mnew_page()\n",
      "File \u001b[1;32mc:\\Users\\SofiePischl\\Documents\\01_HdM\\10_ML_OPS\\TrendAnalyse Social Media\\.venv\\lib\\site-packages\\playwright\\sync_api\\_context_manager.py:47\u001b[0m, in \u001b[0;36mPlaywrightContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_own_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m---> 47\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Error(\n\u001b[0;32m     48\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"It looks like you are using Playwright Sync API inside the asyncio loop.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03mPlease use the Async API instead.\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m             )\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;66;03m# Create a new fiber for the protocol dispatcher. It will be pumping events\u001b[39;00m\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;66;03m# until the end of times. We will pass control to that fiber every time we\u001b[39;00m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;66;03m# block while waiting for a response.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgreenlet_main\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mError\u001b[0m: It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead."
     ]
    }
   ],
   "source": [
    "from playwright.sync_api import sync_playwright\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def scrape_tweets(keyword=\"Klima\", max_tweets=20):\n",
    "    tweets_data = []\n",
    "\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=True)\n",
    "        page = browser.new_page()\n",
    "\n",
    "        search_url = f\"https://x.com/search?q={keyword}%20lang%3Ade&f=live\"\n",
    "        page.goto(search_url)\n",
    "        time.sleep(5)\n",
    "\n",
    "        last_height = 0\n",
    "        while len(tweets_data) < max_tweets:\n",
    "            tweet_elements = page.query_selector_all('article')\n",
    "\n",
    "            for tweet in tweet_elements:\n",
    "                try:\n",
    "                    content = tweet.inner_text()\n",
    "                    lines = content.split('\\n')\n",
    "                    username = lines[0] if lines else \"\"\n",
    "                    text = '\\n'.join(lines[2:-4]) if len(lines) > 6 else content\n",
    "                    timestamp = tweet.query_selector('time').get_attribute('datetime') if tweet.query_selector('time') else ''\n",
    "                    tweet_url = tweet.query_selector('a:has(time)').get_attribute('href') if tweet.query_selector('a:has(time)') else ''\n",
    "                    full_url = f\"https://x.com{tweet_url}\" if tweet_url else ''\n",
    "\n",
    "                    if any(d['url'] == full_url for d in tweets_data):\n",
    "                        continue  # Already captured\n",
    "\n",
    "                    tweets_data.append({\n",
    "                        \"username\": username,\n",
    "                        \"text\": text,\n",
    "                        \"timestamp\": timestamp,\n",
    "                        \"url\": full_url\n",
    "                    })\n",
    "\n",
    "                    if len(tweets_data) >= max_tweets:\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "\n",
    "            # Scroll down\n",
    "            page.mouse.wheel(0, 2000)\n",
    "            time.sleep(2)\n",
    "\n",
    "        browser.close()\n",
    "\n",
    "    return tweets_data\n",
    "\n",
    "# 🔄 Ausführen & speichern\n",
    "data = scrape_tweets(\"Klima\", max_tweets=30)\n",
    "df = pd.DataFrame(data)\n",
    "df.to_excel(\"tweets_playwright_scrape.xlsx\", index=False)\n",
    "print(f\"{len(df)} Tweets gespeichert in 'tweets_playwright_scrape.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2717e9e3",
   "metadata": {},
   "source": [
    "# 4. TikTok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1415a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TikTokApi import TikTokApi\n",
    "import asyncio\n",
    "import os\n",
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env values\n",
    "load_dotenv()\n",
    "\n",
    "ms_token = os.getenv(\"MS_TOKEN\")\n",
    "csv_path = os.getenv(\"OUTPUT_PATH\", \"trending_videos.csv\")\n",
    "\n",
    "\n",
    "async def trending_videos():\n",
    "    async with TikTokApi() as api:\n",
    "        await api.create_sessions(\n",
    "            ms_tokens=[ms_token],\n",
    "            num_sessions=1,\n",
    "            sleep_after=3,\n",
    "            browser=os.getenv(\"TIKTOK_BROWSER\", \"chromium\")\n",
    "        )\n",
    "\n",
    "        data = []\n",
    "\n",
    "        async for video in api.trending.videos(count=30):\n",
    "            info = video.as_dict\n",
    "            data.append({\n",
    "                \"id\": info.get(\"id\"),\n",
    "                \"description\": info.get(\"desc\"),\n",
    "                \"author_username\": info.get(\"author\", {}).get(\"uniqueId\"),\n",
    "                \"author_id\": info.get(\"author\", {}).get(\"id\"),\n",
    "                \"likes\": info.get(\"stats\", {}).get(\"diggCount\"),\n",
    "                \"shares\": info.get(\"stats\", {}).get(\"shareCount\"),\n",
    "                \"comments\": info.get(\"stats\", {}).get(\"commentCount\"),\n",
    "                \"plays\": info.get(\"stats\", {}).get(\"playCount\"),\n",
    "                \"video_url\": info.get(\"video\", {}).get(\"downloadAddr\"),\n",
    "                \"created_time\": info.get(\"createTime\"),\n",
    "            })\n",
    "\n",
    "        os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "\n",
    "        file_exists = os.path.isfile(csv_path)\n",
    "        with open(csv_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=data[0].keys())\n",
    "            if not file_exists:\n",
    "                writer.writeheader()\n",
    "            writer.writerows(data)\n",
    "\n",
    "        print(f\"\\n✅ Erfolgreich {len(data)} Videos in '{csv_path}' gespeichert.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(trending_videos())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc307378",
   "metadata": {},
   "source": [
    "# YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc6c0041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>published_at</th>\n",
       "      <th>view_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>url</th>\n",
       "      <th>scraped_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4sHs9ujo1eg</td>\n",
       "      <td>Die Heuchelei der Stars &amp; Influencer auf dem C...</td>\n",
       "      <td>Das Coachella Festival ist grade bei vielen In...</td>\n",
       "      <td>Alicia Joe</td>\n",
       "      <td>2025-04-21T16:02:49Z</td>\n",
       "      <td>322656</td>\n",
       "      <td>19853</td>\n",
       "      <td>1352</td>\n",
       "      <td>https://www.youtube.com/watch?v=4sHs9ujo1eg</td>\n",
       "      <td>2025-04-22 22:06:12.302112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gTA6gQIC39A</td>\n",
       "      <td>Warum ausgerechnet Katy Perry jetzt ins All fl...</td>\n",
       "      <td>Anzeige | Ladet euch für eure nächste Reise Ai...</td>\n",
       "      <td>Desy</td>\n",
       "      <td>2025-04-21T17:30:01Z</td>\n",
       "      <td>172254</td>\n",
       "      <td>10729</td>\n",
       "      <td>1183</td>\n",
       "      <td>https://www.youtube.com/watch?v=gTA6gQIC39A</td>\n",
       "      <td>2025-04-22 22:06:12.302112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ncXAUBGV8JI</td>\n",
       "      <td>Problemmotoren von VW, Ford, Stellantis und Co...</td>\n",
       "      <td>Ein nasser, in Öl geführter Zahnriemen schien ...</td>\n",
       "      <td>auto motor und sport</td>\n",
       "      <td>2025-04-21T15:01:16Z</td>\n",
       "      <td>159413</td>\n",
       "      <td>2657</td>\n",
       "      <td>768</td>\n",
       "      <td>https://www.youtube.com/watch?v=ncXAUBGV8JI</td>\n",
       "      <td>2025-04-22 22:06:12.302112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dFKCoqPjJ28</td>\n",
       "      <td>Fynn Kliemanns Comeback ist ein Fiebertraum</td>\n",
       "      <td>Fynn Kliemann ist zurück. Der DIY-Künstler und...</td>\n",
       "      <td>Der Dunkle Parabelritter</td>\n",
       "      <td>2025-04-21T17:30:02Z</td>\n",
       "      <td>169827</td>\n",
       "      <td>10718</td>\n",
       "      <td>1113</td>\n",
       "      <td>https://www.youtube.com/watch?v=dFKCoqPjJ28</td>\n",
       "      <td>2025-04-22 22:06:12.302112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gzCXZID3AlY</td>\n",
       "      <td>Streit mit PAULA ? Kontakt mit leiblichen ELTE...</td>\n",
       "      <td>Anzeige | \\nHey, wie versprochen kommen hier a...</td>\n",
       "      <td>Beauty Benzz</td>\n",
       "      <td>2025-04-21T14:00:58Z</td>\n",
       "      <td>70563</td>\n",
       "      <td>2880</td>\n",
       "      <td>293</td>\n",
       "      <td>https://www.youtube.com/watch?v=gzCXZID3AlY</td>\n",
       "      <td>2025-04-22 22:06:12.302112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                              title  \\\n",
       "0  4sHs9ujo1eg  Die Heuchelei der Stars & Influencer auf dem C...   \n",
       "1  gTA6gQIC39A  Warum ausgerechnet Katy Perry jetzt ins All fl...   \n",
       "2  ncXAUBGV8JI  Problemmotoren von VW, Ford, Stellantis und Co...   \n",
       "3  dFKCoqPjJ28        Fynn Kliemanns Comeback ist ein Fiebertraum   \n",
       "4  gzCXZID3AlY  Streit mit PAULA ? Kontakt mit leiblichen ELTE...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Das Coachella Festival ist grade bei vielen In...   \n",
       "1  Anzeige | Ladet euch für eure nächste Reise Ai...   \n",
       "2  Ein nasser, in Öl geführter Zahnriemen schien ...   \n",
       "3  Fynn Kliemann ist zurück. Der DIY-Künstler und...   \n",
       "4  Anzeige | \\nHey, wie versprochen kommen hier a...   \n",
       "\n",
       "              channel_title          published_at view_count like_count  \\\n",
       "0                Alicia Joe  2025-04-21T16:02:49Z     322656      19853   \n",
       "1                      Desy  2025-04-21T17:30:01Z     172254      10729   \n",
       "2      auto motor und sport  2025-04-21T15:01:16Z     159413       2657   \n",
       "3  Der Dunkle Parabelritter  2025-04-21T17:30:02Z     169827      10718   \n",
       "4              Beauty Benzz  2025-04-21T14:00:58Z      70563       2880   \n",
       "\n",
       "  comment_count                                          url  \\\n",
       "0          1352  https://www.youtube.com/watch?v=4sHs9ujo1eg   \n",
       "1          1183  https://www.youtube.com/watch?v=gTA6gQIC39A   \n",
       "2           768  https://www.youtube.com/watch?v=ncXAUBGV8JI   \n",
       "3          1113  https://www.youtube.com/watch?v=dFKCoqPjJ28   \n",
       "4           293  https://www.youtube.com/watch?v=gzCXZID3AlY   \n",
       "\n",
       "                   scraped_at  \n",
       "0  2025-04-22 22:06:12.302112  \n",
       "1  2025-04-22 22:06:12.302112  \n",
       "2  2025-04-22 22:06:12.302112  \n",
       "3  2025-04-22 22:06:12.302112  \n",
       "4  2025-04-22 22:06:12.302112  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# === ENV & Logging ===\n",
    "load_dotenv()\n",
    "\n",
    "log_path = Path(\"../logs/youtube.log\")\n",
    "log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    filename=log_path,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# === YouTube API Setup ===\n",
    "API_KEY = os.getenv(\"YT_KEY\")\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "def scrape_youtube_trending(region=\"DE\", max_results=50):\n",
    "    try:\n",
    "        logging.info(\"Starte YouTube-Scraping...\")\n",
    "\n",
    "        request = youtube.videos().list(\n",
    "            part=\"snippet,statistics\",\n",
    "            chart=\"mostPopular\",\n",
    "            regionCode=region,\n",
    "            maxResults=max_results\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        videos = []\n",
    "        scrape_time = datetime.now()\n",
    "\n",
    "        for item in response.get(\"items\", []):\n",
    "            snippet = item[\"snippet\"]\n",
    "            stats = item.get(\"statistics\", {})\n",
    "\n",
    "            videos.append({\n",
    "                \"video_id\": item[\"id\"],\n",
    "                \"title\": snippet.get(\"title\"),\n",
    "                \"description\": snippet.get(\"description\"),\n",
    "                \"channel_title\": snippet.get(\"channelTitle\"),\n",
    "                \"published_at\": snippet.get(\"publishedAt\"),\n",
    "                \"view_count\": stats.get(\"viewCount\"),\n",
    "                \"like_count\": stats.get(\"likeCount\"),\n",
    "                \"comment_count\": stats.get(\"commentCount\"),\n",
    "                \"url\": f\"https://www.youtube.com/watch?v={item['id']}\",\n",
    "                \"scraped_at\": scrape_time\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(videos)\n",
    "        csv_path = Path(\"../app/data/raw/youtube_data.csv\")\n",
    "        csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if csv_path.exists():\n",
    "            df_existing = pd.read_csv(csv_path)\n",
    "            df = pd.concat([df_existing, df], ignore_index=True)\n",
    "\n",
    "        df.drop_duplicates(subset=[\"video_id\"], inplace=True)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "\n",
    "        display(df.head())\n",
    "\n",
    "        logging.info(f\"{len(df)} Videos gespeichert unter {csv_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fehler beim YouTube-Scraping: {e}\")\n",
    "        print(f\" Fehler: {e}\")\n",
    "\n",
    "# Direkter Aufruf\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_youtube_trending()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
