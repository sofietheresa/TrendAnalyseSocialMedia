{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bdbda90",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 16px;\">\n",
    "    <strong>Course:</strong> Machine Learning Operations |\n",
    "    <strong>Lecturer:</strong> Prof. Dr. Klotz |\n",
    "    <strong>Date:</strong> 17.05.2025 |\n",
    "    <strong>Name:</strong> Sofie Pischl\n",
    "</div>\n",
    "\n",
    "# <center>Data Collection </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a95362a",
   "metadata": {},
   "source": [
    "Konzept & Inhalt:\n",
    "\n",
    "Daten von den größten Social media Apps sollen abgegriffen werden. besonderer Fokus auf Texten.\n",
    "\n",
    "1. Setup & Imports\n",
    "2. Reddit: Hot Posts aus Subreddits\n",
    "3. Instagram: Top Posts per Scraping/API (light)\n",
    "4. Twitter: Aktuelle Tweets via snscrape oder Tweepy\n",
    "5. TikTok: Trending Videos\n",
    "6. YouTube: Trending Videos (API/Scraping)\n",
    "7. Fazit & Learnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b780b59",
   "metadata": {},
   "source": [
    "----\n",
    "# 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d312ad",
   "metadata": {},
   "source": [
    "Import allgemeiner Bibliotheken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef2f679e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Standardbibliotheken\n",
    "import asyncio            # Für asynchrone Programmierung (z. B. async/await)\n",
    "import csv                # Zum Lesen/Schreiben von CSV-Dateien auf niedriger Ebene\n",
    "import logging            # Für Logging von Status- und Fehlermeldungen\n",
    "import os                 # Für Zugriff auf Umgebungsvariablen, Dateipfade etc.\n",
    "import time               # Für Zeitfunktionen wie sleep()\n",
    "from datetime import datetime  # Für aktuelle Zeit und Datumsmanipulation\n",
    "from pathlib import Path  # Für objektorientierten Umgang mit Dateipfaden\n",
    "\n",
    "#Drittanbieter-Bibliotheken\n",
    "import pandas as pd       # Für Datenanalyse und CSV-Verarbeitung\n",
    "from dotenv import load_dotenv  # Zum Laden von Umgebungsvariablen aus .env-Dateien\n",
    "\n",
    "# Load from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa66e0a",
   "metadata": {},
   "source": [
    "# 2. Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b79d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import praw  # Reddit API Wrapper für den Zugriff auf Subreddits, Posts und Kommentare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089b93d3",
   "metadata": {},
   "source": [
    "### Funktionen:\n",
    "- Authentifizierung über OAuth2 via `praw`\n",
    "- Abruf der `hot`-Beiträge aus ausgewählten Subreddits\n",
    "- Speicherung als `.csv` unter `/data/raw/reddit_data.csv`\n",
    "- Fehler-Handling und Logging integriert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d039ab76",
   "metadata": {},
   "source": [
    "**Authentifizierung**\n",
    "\n",
    "Zur Authentifizierung an der Reddit-API wird ein Reddit-Objekt der Bibliothek praw initialisiert. Die benötigten Zugangsdaten – client_id, client_secret und user_agent – werden aus einer .env-Datei geladen, um die Trennung von Code und Konfiguration zu gewährleisten und Sicherheitsrisiken zu minimieren.\n",
    "\n",
    "Diese Parameter dienen der eindeutigen Identifikation der Anwendung gegenüber der API und sind notwendig, um Zugriff auf Reddit-Inhalte zu erhalten. Der user_agent ermöglicht zudem die Rückverfolgbarkeit von API-Anfragen seitens Reddit. Ohne diese Authentifizierung ist ein reguliertes, automatisiertes Scraping nicht zulässig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5789b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=(os.getenv(\"REDDIT_ID\")),\n",
    "    client_secret=(os.getenv(\"REDDIT_SECRET\")),\n",
    "    user_agent=(os.getenv(\"USER_AGENT\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c34f9d7",
   "metadata": {},
   "source": [
    "**Logging-Konfiguration**\n",
    "\n",
    "Bevor das Reddit-Scraping startet, wird ein Logging-System eingerichtet. Dazu wird zunächst ein Pfad zur Log-Datei definiert – in diesem Fall `logs/reddit.log`. Falls das Verzeichnis `logs/` noch nicht existiert, wird es automatisch erstellt. Anschließend wird das Python-Logging so konfiguriert, dass alle Log-Meldungen in diese Datei geschrieben werden.\n",
    "\n",
    "Die Konfiguration legt fest, dass nur Meldungen ab dem Schweregrad `INFO` gespeichert werden. Außerdem wird das Format der Einträge so definiert, dass jeder Log-Eintrag einen Zeitstempel, den Log-Level (wie `INFO` oder `ERROR`) sowie die eigentliche Nachricht enthält. So entsteht eine nachvollziehbare Chronik über den Ablauf und mögliche Fehler des Scripts.\n",
    "\n",
    "Ein typischer Eintrag könnte zum Beispiel so aussehen:\n",
    "\n",
    "```\n",
    "2025-04-19 14:33:07,512 - INFO - Starte Reddit-Scraping...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b20f953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Logging aktiv unter: C:\\Users\\SofiePischl\\Documents\\01_HdM\\10_ML_OPS\\TrendAnalyse Social Media\\logs\\reddit.log\n"
     ]
    }
   ],
   "source": [
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "# Logging einrichten\n",
    "log_path = Path(\"../logs/reddit.log\")\n",
    "log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    filename=log_path,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "print(f\" Logging aktiv unter: {log_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6781e111",
   "metadata": {},
   "source": [
    "**Reddit-Datensammlung mittels Python und PRAW**\n",
    "\n",
    "Die Funktion `scrape_reddit()` dient der systematischen Erhebung textbasierter Inhalte aus der Social-Media-Plattform **Reddit**. Ziel ist es, strukturierte Daten zur Analyse von Trendthemen zu generieren. Zur Umsetzung wird die Bibliothek `praw` (Python Reddit API Wrapper) verwendet, die eine komfortable Schnittstelle zur Reddit-API bereitstellt.\n",
    "\n",
    "Nach der Initialisierung der Protokollierung via `logging` erfolgt die Authentifizierung an der Reddit-API. Hierfür wird ein `Reddit`-Objekt instanziiert, wobei sensible Zugangsdaten wie `client_id`, `client_secret` und `user_agent` aus einer `.env`-Datei geladen werden. Dieses Vorgehen ermöglicht eine sichere Trennung von Konfiguration und Codebasis und schützt vor dem unbeabsichtigten Leaken von API-Schlüsseln.\n",
    "\n",
    "Im Anschluss wird eine Liste von Subreddits definiert, die sowohl populäre als auch als „trending“ markierte Communities umfasst. Aus diesen Subreddits werden jeweils bis zu 100 Beiträge aus dem Hot-Feed abgerufen. Dieses Verfahren stellt sicher, dass aktuelle, stark diskutierte Inhalte gesammelt werden, die ein hohes Relevanzpotenzial für Trendanalysen aufweisen.\n",
    "\n",
    "Die Datenextraktion erfolgt über eine doppelte Schleife: Für jedes Subreddit werden Hot-Beiträge iteriert, wobei ausschließlich „self-posts“ berücksichtigt werden. Diese beinhalten keine externen Links und ermöglichen dadurch eine fokussierte Analyse des vom Nutzer selbst verfassten Textinhalts. Pro Beitrag werden zentrale Metriken wie Titel, Text, Anzahl der Kommentare, Upvotes, Erstellungszeitpunkt sowie die URL gespeichert. Zur besseren zeitlichen Einordnung wird außerdem ein einheitlicher Zeitstempel für alle Einträge vergeben.\n",
    "\n",
    "Die gesammelten Beiträge werden in einem `pandas.DataFrame` strukturiert und anschließend unter `data/raw/reddit_data.csv` abgespeichert. Dabei wird sichergestellt, dass benötigte Verzeichnisse automatisch erstellt werden. Falls bereits Daten vorhanden sind, werden die neuen Einträge angehängt und anschließend Duplikate basierend auf Titel, Textinhalt und Subreddit entfernt. Die finale Version wird ohne Index in die CSV-Datei geschrieben.\n",
    "\n",
    "Abschließend wird die Anzahl der gespeicherten Beiträge im Logfile vermerkt. Etwaige Fehler werden während der Ausführung abgefangen und entsprechend protokolliert. Die Funktion kann sowohl als Modul importiert als auch direkt per Skriptausführung genutzt werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f757739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Gespeichert unter: C:\\Users\\SofiePischl\\Documents\\01_HdM\\10_ML_OPS\\TrendAnalyse Social Media\\app\\data\\raw\\reddit_data.csv\n"
     ]
    }
   ],
   "source": [
    "def scrape_reddit():\n",
    "    try:\n",
    "        logging.info(\"Starte Reddit-Scraping...\")\n",
    "\n",
    "        subreddits = [\"all\", \"popular\", \"trendingreddits\", \"trendingsubreddits\"]\n",
    "        post_limit = 100\n",
    "        all_posts = []\n",
    "        scrape_time = datetime.now()\n",
    "\n",
    "        for sub in subreddits:\n",
    "            subreddit = reddit.subreddit(sub)\n",
    "            for post in subreddit.hot(limit=post_limit):\n",
    "                if post.is_self:\n",
    "                    all_posts.append({\n",
    "                        \"subreddit\": sub,\n",
    "                        \"title\": post.title,\n",
    "                        \"text\": post.selftext,\n",
    "                        \"score\": post.score,\n",
    "                        \"comments\": post.num_comments,\n",
    "                        \"created\": datetime.fromtimestamp(post.created),\n",
    "                        \"url\": post.url,\n",
    "                        \"scraped_at\": scrape_time\n",
    "                    })\n",
    "\n",
    "        df = pd.DataFrame(all_posts)\n",
    "        csv_path = Path(\"../app/data/raw/reddit_data.csv\")\n",
    "        csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if csv_path.exists():\n",
    "            df_existing = pd.read_csv(csv_path)\n",
    "            df = pd.concat([df_existing, df], ignore_index=True)\n",
    "\n",
    "        df.drop_duplicates(subset=[\"title\", \"text\", \"subreddit\"], inplace=True)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\" Gespeichert unter: {csv_path.resolve()}\")\n",
    "\n",
    "        logging.info(f\"{len(df)} Einträge gespeichert unter {csv_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fehler beim Reddit-Scraping: {e}\")\n",
    "\n",
    "# Ausführung bei direktem Aufruf\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_reddit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba082732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>comments</th>\n",
       "      <th>created</th>\n",
       "      <th>url</th>\n",
       "      <th>scraped_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "      <td>AITAH for telling my best friend her marriage ...</td>\n",
       "      <td>This weekend was a disaster...\\n\\nI 27F have b...</td>\n",
       "      <td>17380</td>\n",
       "      <td>1914</td>\n",
       "      <td>2025-05-07 02:03:00</td>\n",
       "      <td>https://www.reddit.com/r/AITAH/comments/1kgjqg...</td>\n",
       "      <td>2025-05-07 11:03:58.051533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all</td>\n",
       "      <td>[Post Game Thread] The Indiana Pacers head bac...</td>\n",
       "      <td>\\n||\\n|:-:|\\n|[](/IND) **120 -  119** [](/CLE)...</td>\n",
       "      <td>6946</td>\n",
       "      <td>2069</td>\n",
       "      <td>2025-05-07 03:48:31</td>\n",
       "      <td>https://www.reddit.com/r/nba/comments/1kgltqu/...</td>\n",
       "      <td>2025-05-07 11:03:58.051533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>popular</td>\n",
       "      <td>Americans, how do you feel about the firing of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21007</td>\n",
       "      <td>5167</td>\n",
       "      <td>2025-05-06 19:46:16</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/1k...</td>\n",
       "      <td>2025-05-07 11:03:58.051533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>popular</td>\n",
       "      <td>Claim your Trailer 2 OG flair here!</td>\n",
       "      <td>Out of the blue we got the second trailer...FI...</td>\n",
       "      <td>12751</td>\n",
       "      <td>46078</td>\n",
       "      <td>2025-05-06 16:43:19</td>\n",
       "      <td>https://www.reddit.com/r/GTA6/comments/1kg68js...</td>\n",
       "      <td>2025-05-07 11:03:58.051533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>popular</td>\n",
       "      <td>AITA for refusing to let my daughter’s fiancé ...</td>\n",
       "      <td>So I (M49) might be in the wrong here, but I h...</td>\n",
       "      <td>8064</td>\n",
       "      <td>4265</td>\n",
       "      <td>2025-05-06 18:29:17</td>\n",
       "      <td>https://www.reddit.com/r/AmItheAsshole/comment...</td>\n",
       "      <td>2025-05-07 11:03:58.051533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit                                              title  \\\n",
       "0       all  AITAH for telling my best friend her marriage ...   \n",
       "1       all  [Post Game Thread] The Indiana Pacers head bac...   \n",
       "2   popular  Americans, how do you feel about the firing of...   \n",
       "3   popular                Claim your Trailer 2 OG flair here!   \n",
       "4   popular  AITA for refusing to let my daughter’s fiancé ...   \n",
       "\n",
       "                                                text  score  comments  \\\n",
       "0  This weekend was a disaster...\\n\\nI 27F have b...  17380      1914   \n",
       "1  \\n||\\n|:-:|\\n|[](/IND) **120 -  119** [](/CLE)...   6946      2069   \n",
       "2                                                NaN  21007      5167   \n",
       "3  Out of the blue we got the second trailer...FI...  12751     46078   \n",
       "4  So I (M49) might be in the wrong here, but I h...   8064      4265   \n",
       "\n",
       "               created                                                url  \\\n",
       "0  2025-05-07 02:03:00  https://www.reddit.com/r/AITAH/comments/1kgjqg...   \n",
       "1  2025-05-07 03:48:31  https://www.reddit.com/r/nba/comments/1kgltqu/...   \n",
       "2  2025-05-06 19:46:16  https://www.reddit.com/r/AskReddit/comments/1k...   \n",
       "3  2025-05-06 16:43:19  https://www.reddit.com/r/GTA6/comments/1kg68js...   \n",
       "4  2025-05-06 18:29:17  https://www.reddit.com/r/AmItheAsshole/comment...   \n",
       "\n",
       "                   scraped_at  \n",
       "0  2025-05-07 11:03:58.051533  \n",
       "1  2025-05-07 11:03:58.051533  \n",
       "2  2025-05-07 11:03:58.051533  \n",
       "3  2025-05-07 11:03:58.051533  \n",
       "4  2025-05-07 11:03:58.051533  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zeuge neuste Einträge\n",
    "df = pd.read_csv(\"../app/data/raw/reddit_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8152499",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Instagram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125499c",
   "metadata": {},
   "source": [
    "Im Rahmen der Datenerhebung für Instagram wurden zunächst etablierte Bibliotheken wie *Instaloader* und *Playwright* getestet. Beide Ansätze erwiesen sich jedoch als unzureichend: *Instaloader* ermöglicht nur den Zugriff auf öffentlich verfügbare Inhalte registrierter Nutzer und bietet keinen Zugriff auf die dynamisch generierte Explore-Seite. *Playwright*, obwohl für modernes Web-Scraping geeignet, stieß auf technische Einschränkungen hinsichtlich der Authentifizierung und der zuverlässigen Navigation innerhalb der dynamischen Inhalte Instagrams.\n",
    "\n",
    "Aus diesem Grund wurde ein alternativer Ansatz mittels *Selenium* implementiert. Dieser verfolgt die Strategie, das Nutzerverhalten im Browser möglichst realitätsnah zu simulieren. Der automatisierte Prozess umfasst dabei das Aufrufen der Instagram-Loginseite, die Anmeldung über Umgebungsvariablen gespeicherte Zugangsdaten, die Navigation zur Explore-Seite sowie das Extrahieren von Beitragsinformationen (z. B. Bildquelle, Beschreibung, URL) über XPath-Selektoren.\n",
    "\n",
    "Grundsätzlich zeigte sich der Selenium-basierte Ansatz als funktional, jedoch mit erheblichen Stabilitätsproblemen. Da die DOM-Struktur der Explore-Seite nicht statisch ist, können sich XPath-Referenzen zwischenzeitlich ändern oder variieren, was zu fehleranfälligen Selektionsoperationen führt. Zudem ist die Explore-Seite stark dynamisch und in hohem Maße vom Nutzerverhalten und den Algorithmus-basierten Inhalten abhängig, was eine konsistente und reproduzierbare Datenextraktion erschwert. Somit stellt die Nutzung von Selenium eine technisch mögliche, jedoch fragile und wartungsintensive Lösung für das Scraping dynamischer Instagram-Inhalte dar.\n",
    "\n",
    "Deshalb wird der Ansatz im folgenden dargestellt, wurde aber im weiteren Verlauf des Projekts verworden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ba8d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver  # Startet und steuert Browser-Instanzen (z. B. Chrome)\n",
    "from selenium.webdriver.chrome.service import Service  # Verwaltet den ChromeDriver-Dienst\n",
    "from selenium.webdriver.common.by import By  # Selektoren zum Finden von Elementen (z. B. By.ID, By.XPATH)\n",
    "from selenium.webdriver.chrome.options import Options  # Konfiguration für den Chrome-Browser (z. B. headless-Modus)\n",
    "from selenium.webdriver.support.ui import WebDriverWait  # Ermöglicht explizite Wartezeiten für Elemente\n",
    "from selenium.webdriver.support import expected_conditions as EC  # Vordefinierte Bedingungen zum Warten (z. B. Sichtbarkeit von Elementen)\n",
    "from webdriver_manager.chrome import ChromeDriverManager  # Automatische Verwaltung und Installation des ChromeDrivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e585a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credentials aus der .env Datei laden\n",
    "INSTA_USERNAME= os.getenv(\"INSTA_USERNAME\")\n",
    "INSTA_PASSWORD= os.getenv(\"INSTA_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9727179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging aktiv unter: C:\\Users\\SofiePischl\\Documents\\01_HdM\\10_ML_OPS\\TrendAnalyse Social Media\\logs\\instagram.log\n"
     ]
    }
   ],
   "source": [
    "# Logging initialisieren\n",
    "log_path = Path(\"../logs/instagram.log\")\n",
    "log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "# Logger holen (Root-Logger)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Bestehende Handler entfernen (z. B. alte Datei-Handler)\n",
    "logger.handlers.clear()\n",
    "\n",
    "# Neuen FileHandler hinzufügen\n",
    "file_handler = logging.FileHandler(log_path)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=log_path,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "print(f\"Logging aktiv unter: {log_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31c0d83",
   "metadata": {},
   "source": [
    "Die folgende Funktion scrape_instagram_explore() automatisiert den Zugriff auf die Instagram-Explore-Seite, um dort eingebettete Beiträge zu extrahieren und zu speichern. Zunächst wird ein Headless-Browser mittels Selenium konfiguriert und gestartet, um Instagram ohne sichtbares Browserfenster zu laden. Anschließend erfolgt ein Login mit Benutzerdaten, die aus Umgebungsvariablen gelesen werden. Nach erfolgreicher Anmeldung wird zur Explore-Seite navigiert, wo durch wiederholtes Scrollen weitere Inhalte dynamisch nachgeladen werden. Die so sichtbaren Beiträge werden dann anhand von XPath-Selektoren identifiziert, und von jedem Beitrag werden die URL, das zugehörige Bild sowie die Bildbeschreibung ausgelesen. Diese Informationen werden zusammen mit einem Zeitstempel gesammelt und in eine CSV-Datei gespeichert. Bestehende Daten in der Datei werden dabei berücksichtigt und Duplikate entfernt. Die Funktion schließt mit dem Speichern der kombinierten Daten und dem Beenden des Browsers. Fehler während des Prozesses werden per Logging dokumentiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c344738",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_instagram_explore():\n",
    "    try:\n",
    "        logging.info(\"Starte Instagram-Explore-Scraping...\")\n",
    "\n",
    "        # Headless-Browser konfigurieren\n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless=new\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--window-size=1920x1080\")\n",
    "\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        driver.get(\"https://www.instagram.com/accounts/login/\")\n",
    "\n",
    "        # Login\n",
    "        username = os.getenv(\"INSTA_USERNAME\")\n",
    "        password = os.getenv(\"INSTA_PASSWORD\")\n",
    "\n",
    "        time.sleep(3)\n",
    "        driver.find_element(By.NAME, \"username\").send_keys(username)\n",
    "        driver.find_element(By.NAME, \"password\").send_keys(password)\n",
    "        driver.find_element(By.XPATH, \"//button[@type='submit']\").click()\n",
    "\n",
    "        time.sleep(7)  # Warte auf Login\n",
    "\n",
    "        # Gehe zur Explore-Seite\n",
    "        driver.get(\"https://www.instagram.com/explore/\")\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Seite scrollen, um mehr Posts zu laden\n",
    "        for _ in range(3):  # 3x scrollen → kannst du erhöhen\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(3)\n",
    "\n",
    "        # Beiträge extrahieren\n",
    "        posts = driver.find_elements(By.XPATH, '//a[contains(@href, \"/p/\")]')\n",
    "        post_data = []\n",
    "\n",
    "        for post in posts:\n",
    "            try:\n",
    "                url = post.get_attribute(\"href\")\n",
    "                img = post.find_element(By.TAG_NAME, \"img\")\n",
    "                img_url = img.get_attribute(\"src\")\n",
    "                description = img.get_attribute(\"alt\")\n",
    "\n",
    "                post_data.append({\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"url\": url,\n",
    "                    \"image\": img_url,\n",
    "                    \"description\": description\n",
    "                })\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Fehler beim Extrahieren eines Posts: {e}\")\n",
    "                continue\n",
    "\n",
    "        # In CSV speichern\n",
    "        csv_path = Path(\"../app/data/raw/instagram_data.csv\")\n",
    "        csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        df = pd.DataFrame(post_data)\n",
    "\n",
    "        if csv_path.exists():\n",
    "            df_existing = pd.read_csv(csv_path)\n",
    "            df = pd.concat([df_existing, df], ignore_index=True)\n",
    "\n",
    "        df.drop_duplicates(subset=[\"url\", \"image\", \"description\"], inplace=True)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "\n",
    "        logging.info(f\"{len(df)} Einträge gespeichert unter {csv_path}\")\n",
    "        driver.quit()\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fehler beim Instagram-Explore-Scraping: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e397062e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Instagram-Explore-Scraping...\n"
     ]
    }
   ],
   "source": [
    "# Jetzt ausführen\n",
    "scrape_instagram_explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f83d79c",
   "metadata": {},
   "source": [
    "Zur gezielten Extraktion von Informationen aus einzelnen Instagram-Posts wurde ein browsergestützter Ansatz mittels Selenium implementiert. Die Funktion scrape_instagram_post(url) nimmt eine URL zu einem öffentlichen Instagram-Beitrag entgegen und automatisiert den Zugriff darauf. Dazu wird ein Headless-Chrome-Browser im Hintergrund gestartet, um die angegebene Seite zu laden und die dort eingebetteten Inhalte auszulesen. Die Funktion wartet zunächst auf das vollständige Laden des Beitrags und versucht anschließend, ein ggf. erscheinendes Login-Popup automatisiert zu schließen, um eine ungehinderte Datenerfassung zu ermöglichen.\n",
    "\n",
    "Es werden folgende Informationen extrahiert: der Benutzername des veröffentlichenden Accounts, die Bildunterschrift (Caption) sowie – sofern vorhanden – die Anzahl der Likes. Diese Daten werden zusammen mit der aufgerufenen URL und einem Zeitstempel in strukturierter Form gespeichert. Die Speicherung erfolgt in einer zentralen CSV-Datei, wobei bestehende Daten berücksichtigt und potenzielle Duplikate entfernt werden. Insgesamt erlaubt dieser Ansatz eine punktuelle Analyse individueller Inhalte, ist jedoch wie andere Selenium-basierte Verfahren anfällig für strukturelle Änderungen in der Instagram-Oberfläche, insbesondere bei dynamischen Komponenten und nicht stabilen XPath-Referenzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c60440bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_instagram_post(url):\n",
    "    try:\n",
    "        logging.info(f\"Starte Instagram-Scraping für URL: {url}\")\n",
    "        \n",
    "        # Zielpfad vorbereiten\n",
    "        csv_path = Path(\"../app/data/raw/instagram_data.csv\")\n",
    "        csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Browser initialisieren\n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless=new\")  # im Hintergrund ausführen\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--window-size=1920x1080\")\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        wait = WebDriverWait(driver, 15)\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        # Beitrag laden\n",
    "        wait.until(EC.presence_of_element_located((By.TAG_NAME, \"article\")))\n",
    "\n",
    "        # Login-Popup schließen (falls vorhanden)\n",
    "        try:\n",
    "            close_btn = wait.until(EC.element_to_be_clickable((By.XPATH, \"//div[@role='dialog']//button\")))\n",
    "            close_btn.click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Username extrahieren\n",
    "        try:\n",
    "            username_elem = wait.until(EC.visibility_of_element_located(\n",
    "                (By.XPATH, \"//a[contains(@href, '/') and contains(@class, 'notranslate')]//span[1]\")\n",
    "            ))\n",
    "            username = username_elem.text.strip()\n",
    "        except:\n",
    "            username = \"\"\n",
    "\n",
    "        # Caption extrahieren\n",
    "        try:\n",
    "            caption_elem = wait.until(EC.visibility_of_element_located(\n",
    "                (By.XPATH, \"//div[@data-testid='post-comment-root']//span\")\n",
    "            ))\n",
    "            caption = caption_elem.text.strip()\n",
    "        except:\n",
    "            caption = \"\"\n",
    "\n",
    "        # Likes (optional)\n",
    "        try:\n",
    "            likes_elem = wait.until(EC.visibility_of_element_located(\n",
    "                (By.XPATH, \"//section//span[contains(text(), 'Gefällt')]\")\n",
    "            ))\n",
    "            likes = likes_elem.text.strip()\n",
    "        except:\n",
    "            likes = \"\"\n",
    "\n",
    "        # Timestamp + Struktur\n",
    "        data = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"url\": url,\n",
    "            \"username\": username,\n",
    "            \"caption\": caption,\n",
    "            \"likes\": likes\n",
    "        }\n",
    "\n",
    "        df_new = pd.DataFrame([data])\n",
    "\n",
    "        if csv_path.exists():\n",
    "            df_existing = pd.read_csv(csv_path)\n",
    "            df_new = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "\n",
    "        df_new.drop_duplicates(subset=[\"url\", \"caption\", \"username\"], inplace=True)\n",
    "        df_new.to_csv(csv_path, index=False)\n",
    "\n",
    "        logging.info(f\"Erfolgreich gespeichert unter {csv_path}\")\n",
    "        driver.quit()\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fehler beim Scrapen von {url}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9f1d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispielaufruf\n",
    "scrape_instagram_post(\"https://www.instagram.com/p/DICWDtYNq7E/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafbbd32",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. TWITTER / X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25574e53",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Versuch mit der offiziellen Twitter API (v2)\n",
    "\n",
    "Zunächst wurde ein Zugriff über die offizielle Twitter Developer API (v2) realisiert, wobei ein Bearer Token über ein entsprechendes Developer-Konto eingebunden wurde. Die Verbindung wurde in der Regel erfolgreich hergestellt, jedoch resultierten sämtliche Anfragen bereits nach wenigen Requests in einem **HTTP 429 – Too Many Requests** Fehler. Selbst bei einer Limitierung auf nur drei Tweets wurde die Anfrage durch die API geblockt. Dies deutet auf eine äußerst restriktive Ratenbegrenzung hin, selbst im Rahmen von „Essential Access“-Leveln.\n",
    "\n",
    "Außerdem sind in der kostenlos erstellbaren Developer App nur 100 Tweets abgreifbar, sodass diese Lösung, selbst wenn sie funktionieren würde, nicht sillführend währe.\n",
    "\n",
    "### 2. Nutzung alternativer Scraping-Bibliotheken (z. B. `snscrape`)\n",
    "\n",
    "In einem zweiten Schritt wurde die populäre Bibliothek [`snscrape`](https://github.com/JustAnotherArchivist/snscrape) eingesetzt, welche ohne API-Zugang direkt über die öffentliche Webschnittstelle von Twitter (u. a. GraphQL-Endpunkte) operiert. Dieses Vorgehen war früher eine gängige Methode zur Umgehung von Authentifizierungsbeschränkungen.\n",
    "\n",
    "Aktuell führt jedoch auch dieser Ansatz regelmäßig zu Fehlern. Konkret wurde mehrfach der folgende Exception ausgelöst:\n",
    "\n",
    "```\n",
    "ScraperException: 4 requests to .../SearchTimeline failed, giving up.\n",
    "```\n",
    "\n",
    "Diese Meldung ist ein Hinweis darauf, dass Twitter die internen API-Endpunkte (GraphQL) entweder modifiziert oder stark abgesichert hat. `snscrape` kann infolgedessen die erwarteten Objekte nicht mehr extrahieren. Entsprechende Issues sind auch in der GitHub-Community der Bibliothek dokumentiert, eine stabile Lösung existiert derzeit nicht.\n",
    "\n",
    "\n",
    "\n",
    "### 3. Headless-Scraping mit Playwright\n",
    "\n",
    "Ein weiterer Ansatz zur Umgehung offizieller API-Beschränkungen bestand im Einsatz der Automatisierungsbibliothek [`Playwright`](https://playwright.dev/python/), die über einen echten Chromium-Browser Webinhalte visuell bzw. DOM-basiert extrahieren kann. Diese Methode ist grundsätzlich vielversprechend, da sie wie ein menschlicher Nutzer agiert und damit keine API-Limits verletzt.\n",
    "\n",
    "Im konkreten Fall wurde Playwright zunächst als **synchrones Skript** innerhalb eines Jupyter Notebooks getestet. Dabei trat jedoch ein technisches Problem auf: Da Jupyter selbst auf einem laufenden `asyncio`-Event-Loop basiert, ist die Verwendung von `sync_playwright()` nicht erlaubt. Dies führte unmittelbar zu folgender Fehlermeldung:\n",
    "\n",
    "```\n",
    "Error: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
    "```\n",
    "\n",
    "Als Reaktion darauf wurde auf die empfohlene **asynchrone API-Variante** (`async_playwright()`) umgestellt. Auch dieser Ansatz scheiterte jedoch – insbesondere unter Windows – mit folgendem Fehler:\n",
    "\n",
    "```\n",
    "NotImplementedError: asyncio subprocess transport is not implemented\n",
    "```\n",
    "\n",
    "Diese Meldung verweist auf eine bekannte Einschränkung der Playwright-Implementierung im Zusammenspiel mit dem Event-Loop von Windows und Jupyter. Der Start des internen Chromium-Drivers schlägt aufgrund fehlender Unterstützung für `asyncio.create_subprocess_exec()` fehl. Damit ist Playwright im Rahmen einer Notebook-Umgebung faktisch **nicht lauffähig**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2717e9e3",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. TikTok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8604b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TikTokApi ist eine Bibliothek zum Abrufen von TikTok-Daten (inoffizielle API)\n",
    "from TikTokApi import TikTokApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6cff74e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token für die Session – muss aus Cookies der TikTok website gezogen werden\n",
    "ms_token = os.getenv(\"MS_TOKEN\")  \n",
    "\n",
    "# Pfad zur Ausgabedatei (CSV), Standardwert: \"trending_videos.csv\"\n",
    "csv_path = os.getenv(\"OUTPUT_PATH\", \"trending_videos.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef890c15",
   "metadata": {},
   "source": [
    "Der nachfolgende Code nutzt nun die inoffizielle TikTokApi, um die aktuell beliebtesten TikTok-Videos (Trending-Videos) automatisiert abzurufen. Mithilfe des gültigen ms_token (Session-Token) wird eine Browser-Sitzung erstellt, die den Zugriff auf öffentliche TikTok-Inhalte ermöglicht. Die Funktion extrahiert Informationen zu 30 populären Videos – darunter Beschreibung, Autor, Likes, Views und URL – und speichert die Ergebnisse in einer CSV-Datei. Die gesamte Logik läuft asynchron ab, um die Effizienz bei der Netzwerkkommunikation zu verbessern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1415a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def trending_videos():\n",
    "    # TikTok-API-Session asynchron starten\n",
    "    async with TikTokApi() as api:\n",
    "        await api.create_sessions(\n",
    "            ms_tokens=[ms_token],  # Session-Token für Authentifizierung\n",
    "            num_sessions=1,        # Anzahl gleichzeitiger Sessions\n",
    "            sleep_after=3,         # Wartezeit zwischen Session-Anfragen\n",
    "            browser=os.getenv(\"TIKTOK_BROWSER\", \"chromium\")  # Standardbrowser\n",
    "        )\n",
    "\n",
    "        data = []\n",
    "\n",
    "        # Schleife über die 30 beliebtesten Trending-Videos\n",
    "        async for video in api.trending.videos(count=30):\n",
    "            info = video.as_dict  # Rohdaten des Videos als Dictionary\n",
    "\n",
    "            # Relevante Felder extrahieren und speichern\n",
    "            data.append({\n",
    "                \"id\": info.get(\"id\"),\n",
    "                \"description\": info.get(\"desc\"),\n",
    "                \"author_username\": info.get(\"author\", {}).get(\"uniqueId\"),\n",
    "                \"author_id\": info.get(\"author\", {}).get(\"id\"),\n",
    "                \"likes\": info.get(\"stats\", {}).get(\"diggCount\"),\n",
    "                \"shares\": info.get(\"stats\", {}).get(\"shareCount\"),\n",
    "                \"comments\": info.get(\"stats\", {}).get(\"commentCount\"),\n",
    "                \"plays\": info.get(\"stats\", {}).get(\"playCount\"),\n",
    "                \"video_url\": info.get(\"video\", {}).get(\"downloadAddr\"),\n",
    "                \"created_time\": info.get(\"createTime\"),\n",
    "            })\n",
    "\n",
    "        # Sicherstellen, dass der Zielordner existiert\n",
    "        os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "\n",
    "        # Ergebnisse in CSV-Datei schreiben (append-Modus)\n",
    "        file_exists = os.path.isfile(csv_path)\n",
    "        with open(csv_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=data[0].keys())\n",
    "\n",
    "            if not file_exists:\n",
    "                writer.writeheader()  # Schreibe Header nur, wenn Datei neu ist\n",
    "            writer.writerows(data)   # Anhängen der neuen Zeilen\n",
    "\n",
    "        print(f\"\\n✅ Erfolgreich {len(data)} Videos in '{csv_path}' gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc307378",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. YouTube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ab09fe",
   "metadata": {},
   "source": [
    "Um YouTube zu scrapen wird die offizielle YouTube Data API v3 genutzt, um die derzeit beliebtesten Videos (Trending) aus einer bestimmten Region abzurufen. Es extrahiert Titel, Beschreibung, Kanalname, Veröffentlichungsdatum sowie Statistiken wie Aufrufe, Likes und Kommentare. Die Daten werden als Pandas DataFrame verarbeitet und in einer CSV-Datei gespeichert. Bestehende Daten werden berücksichtigt, Duplikate entfernt und das Ergebnis ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a1a7fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#offizieller Google API Client für Python\n",
    "from googleapiclient.discovery import build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3984f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Set-up\n",
    "API_KEY = os.getenv(\"YT_KEY\")  # API-Key aus Umgebungsvariablen\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)  # YouTube-API-Client erzeugen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc6c0041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_youtube_trending(region=\"DE\", max_results=50):\n",
    "    try:\n",
    "        print(\"🔍 Starte YouTube-Scraping...\")\n",
    "\n",
    "        # API-Anfrage vorbereiten (meistgesehene Videos in gegebener Region)\n",
    "        request = youtube.videos().list(\n",
    "            part=\"snippet,statistics\",\n",
    "            chart=\"mostPopular\",\n",
    "            regionCode=region,\n",
    "            maxResults=max_results\n",
    "        )\n",
    "        response = request.execute()  # Anfrage ausführen\n",
    "\n",
    "        videos = []\n",
    "        scrape_time = datetime.now()  # Zeitstempel des Abrufs\n",
    "\n",
    "        # Schleife über alle zurückgegebenen Videos\n",
    "        for item in response.get(\"items\", []):\n",
    "            snippet = item[\"snippet\"]\n",
    "            stats = item.get(\"statistics\", {})\n",
    "\n",
    "            videos.append({\n",
    "                \"video_id\": item[\"id\"],\n",
    "                \"title\": snippet.get(\"title\"),\n",
    "                \"description\": snippet.get(\"description\"),\n",
    "                \"channel_title\": snippet.get(\"channelTitle\"),\n",
    "                \"published_at\": snippet.get(\"publishedAt\"),\n",
    "                \"view_count\": stats.get(\"viewCount\"),\n",
    "                \"like_count\": stats.get(\"likeCount\"),\n",
    "                \"comment_count\": stats.get(\"commentCount\"),\n",
    "                \"url\": f\"https://www.youtube.com/watch?v={item['id']}\",\n",
    "                \"scraped_at\": scrape_time\n",
    "            })\n",
    "\n",
    "        # In DataFrame umwandeln\n",
    "        df = pd.DataFrame(videos)\n",
    "\n",
    "        # CSV-Pfad vorbereiten\n",
    "        csv_path = Path(\"../app/data/raw/youtube_data.csv\")\n",
    "        csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Falls Datei bereits existiert → zusammenführen\n",
    "        if csv_path.exists():\n",
    "            df_existing = pd.read_csv(csv_path)\n",
    "            df = pd.concat([df_existing, df], ignore_index=True)\n",
    "\n",
    "        # Duplikate entfernen und speichern\n",
    "        df.drop_duplicates(subset=[\"video_id\"], inplace=True)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "\n",
    "        # Vorschau der Daten anzeigen\n",
    "        display(df.head())\n",
    "\n",
    "        print(f\"✅ {len(df)} Videos gespeichert unter: {csv_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Fehler beim YouTube-Scraping: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1069a8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Starte YouTube-Scraping...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>published_at</th>\n",
       "      <th>view_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>url</th>\n",
       "      <th>scraped_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mLpa3zzrVyI</td>\n",
       "      <td>Inter Mailand 4:3 FC Barcelona | Highlights - ...</td>\n",
       "      <td>Viel Spaß mit den Highlights des Spiels Inter ...</td>\n",
       "      <td>Prime Video Sport Deutschland</td>\n",
       "      <td>2025-05-06T21:48:35Z</td>\n",
       "      <td>1375723</td>\n",
       "      <td>25099</td>\n",
       "      <td>2585</td>\n",
       "      <td>https://www.youtube.com/watch?v=mLpa3zzrVyI</td>\n",
       "      <td>2025-05-07 14:12:27.972453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nRw_GB_78xQ</td>\n",
       "      <td>Drei Festnahmen &amp; eine Waffe | Nachtschicht mi...</td>\n",
       "      <td>WERBUNG Mit dem Code HEYAARON kann Finanzguru ...</td>\n",
       "      <td>Hey Aaron!!!</td>\n",
       "      <td>2025-05-06T16:01:28Z</td>\n",
       "      <td>319867</td>\n",
       "      <td>17503</td>\n",
       "      <td>338</td>\n",
       "      <td>https://www.youtube.com/watch?v=nRw_GB_78xQ</td>\n",
       "      <td>2025-05-07 14:12:27.972453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SJP2M5IumRc</td>\n",
       "      <td>Merz scheitert im 1. Wahlgang - und ist jetzt ...</td>\n",
       "      <td>Friedrich Merz ist neuer Kanzler der Bundesrep...</td>\n",
       "      <td>MrWissen2go</td>\n",
       "      <td>2025-05-06T15:21:05Z</td>\n",
       "      <td>430465</td>\n",
       "      <td>20861</td>\n",
       "      <td>3085</td>\n",
       "      <td>https://www.youtube.com/watch?v=SJP2M5IumRc</td>\n",
       "      <td>2025-05-07 14:12:27.972453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m5GRR4D23k0</td>\n",
       "      <td>Was ich euch sagen möchte...</td>\n",
       "      <td>Mein zweiter Kanal: https://www.youtube.com/@A...</td>\n",
       "      <td>SkylineTV</td>\n",
       "      <td>2025-05-06T15:15:05Z</td>\n",
       "      <td>129142</td>\n",
       "      <td>9029</td>\n",
       "      <td>1812</td>\n",
       "      <td>https://www.youtube.com/watch?v=m5GRR4D23k0</td>\n",
       "      <td>2025-05-07 14:12:27.972453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HwMeliz7paA</td>\n",
       "      <td>IN 7 TAGEN 491 KM 🏃🏻‍♂️🤯| I run the full lengt...</td>\n",
       "      <td>Instagram: https://www.instagram.com/ardasaatc...</td>\n",
       "      <td>Arda Saatçi</td>\n",
       "      <td>2025-05-06T16:59:06Z</td>\n",
       "      <td>286018</td>\n",
       "      <td>27371</td>\n",
       "      <td>992</td>\n",
       "      <td>https://www.youtube.com/watch?v=HwMeliz7paA</td>\n",
       "      <td>2025-05-07 14:12:27.972453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                              title  \\\n",
       "0  mLpa3zzrVyI  Inter Mailand 4:3 FC Barcelona | Highlights - ...   \n",
       "1  nRw_GB_78xQ  Drei Festnahmen & eine Waffe | Nachtschicht mi...   \n",
       "2  SJP2M5IumRc  Merz scheitert im 1. Wahlgang - und ist jetzt ...   \n",
       "3  m5GRR4D23k0                       Was ich euch sagen möchte...   \n",
       "4  HwMeliz7paA  IN 7 TAGEN 491 KM 🏃🏻‍♂️🤯| I run the full lengt...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Viel Spaß mit den Highlights des Spiels Inter ...   \n",
       "1  WERBUNG Mit dem Code HEYAARON kann Finanzguru ...   \n",
       "2  Friedrich Merz ist neuer Kanzler der Bundesrep...   \n",
       "3  Mein zweiter Kanal: https://www.youtube.com/@A...   \n",
       "4  Instagram: https://www.instagram.com/ardasaatc...   \n",
       "\n",
       "                   channel_title          published_at view_count like_count  \\\n",
       "0  Prime Video Sport Deutschland  2025-05-06T21:48:35Z    1375723      25099   \n",
       "1                   Hey Aaron!!!  2025-05-06T16:01:28Z     319867      17503   \n",
       "2                    MrWissen2go  2025-05-06T15:21:05Z     430465      20861   \n",
       "3                      SkylineTV  2025-05-06T15:15:05Z     129142       9029   \n",
       "4                    Arda Saatçi  2025-05-06T16:59:06Z     286018      27371   \n",
       "\n",
       "  comment_count                                          url  \\\n",
       "0          2585  https://www.youtube.com/watch?v=mLpa3zzrVyI   \n",
       "1           338  https://www.youtube.com/watch?v=nRw_GB_78xQ   \n",
       "2          3085  https://www.youtube.com/watch?v=SJP2M5IumRc   \n",
       "3          1812  https://www.youtube.com/watch?v=m5GRR4D23k0   \n",
       "4           992  https://www.youtube.com/watch?v=HwMeliz7paA   \n",
       "\n",
       "                  scraped_at  \n",
       "0 2025-05-07 14:12:27.972453  \n",
       "1 2025-05-07 14:12:27.972453  \n",
       "2 2025-05-07 14:12:27.972453  \n",
       "3 2025-05-07 14:12:27.972453  \n",
       "4 2025-05-07 14:12:27.972453  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 50 Videos gespeichert unter: ..\\app\\data\\raw\\youtube_data.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    scrape_youtube_trending()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bafb10",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Fazit\n",
    "\n",
    "Das Scraping für die Plattformen **Reddit**, **TikTok** und **YouTube** wurde erfolgreich implementiert. Für jede dieser Plattformen wurden funktionierende Ansätze entwickelt, die auf öffentlich zugängliche APIs oder geeignete inoffizielle Schnittstellen zurückgreifen. Die lauffähigen und getesteten Skripte befinden sich im Verzeichnis **`src/1_scheduler/jobs`** und können dort für die regelmäßige Datenerhebung genutzt werden.\n",
    "Für **X (ehemals Twitter)** und **Instagram** konnte hingegen kein stabiles und zuverlässiges Scraping-Verfahren umgesetzt werden, da beide Plattformen den Zugriff durch technische und rechtliche Maßnahmen erheblich einschränken.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
