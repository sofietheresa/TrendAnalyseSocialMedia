{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bdbda90",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 16px;\">\n",
    "    <strong>Course:</strong> Machine Learning Operations |\n",
    "    <strong>Lecturer:</strong> Prof. Dr. Klotz |\n",
    "    <strong>Date:</strong> 17.05.2025 |\n",
    "    <strong>Name:</strong> Sofie Pischl\n",
    "</div>\n",
    "\n",
    "# <center>Data Collection </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a95362a",
   "metadata": {},
   "source": [
    "Konzept & Inhalt:\n",
    "\n",
    "Daten von den größten Social media Apps sollen abgegriffen werden. besonderer Fokus auf Texten.\n",
    "\n",
    "1. Setup & Imports\n",
    "2. Reddit: Hot Posts aus Subreddits\n",
    "3. Instagram: Top Posts per Scraping/API (light)\n",
    "4. Twitter: Aktuelle Tweets via snscrape oder Tweepy\n",
    "5. TikTok: Trending Videos\n",
    "6. YouTube: Trending Videos (API/Scraping)\n",
    "7. Fazit & Learnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b780b59",
   "metadata": {},
   "source": [
    "----\n",
    "# 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dceb2f7",
   "metadata": {},
   "source": [
    "Zunächst werden alle benötigten Libraries importiert:\n",
    "- `praw` für den Reddit-Zugriff\n",
    "- `pandas` für Datenverarbeitung\n",
    "- `datetime` für Timestamps\n",
    "- `dotenv` für Umgebungsvariablen\n",
    "- `pathlib` für saubere Pfadangaben\n",
    "- `logging` für Fehlerprotokollierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96b79d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Load from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa66e0a",
   "metadata": {},
   "source": [
    "# 2. Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089b93d3",
   "metadata": {},
   "source": [
    "### Funktionen:\n",
    "- Authentifizierung über OAuth2 via `praw`\n",
    "- Abruf der `hot`-Beiträge aus ausgewählten Subreddits\n",
    "- Speicherung als `.csv` unter `/data/raw/reddit_data.csv`\n",
    "- Fehler-Handling und Logging integriert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d039ab76",
   "metadata": {},
   "source": [
    "**Authentifizierung**\n",
    "\n",
    "Zur Authentifizierung an der Reddit-API wird ein Reddit-Objekt der Bibliothek praw initialisiert. Die benötigten Zugangsdaten – client_id, client_secret und user_agent – werden aus einer .env-Datei geladen, um die Trennung von Code und Konfiguration zu gewährleisten und Sicherheitsrisiken zu minimieren.\n",
    "\n",
    "Diese Parameter dienen der eindeutigen Identifikation der Anwendung gegenüber der API und sind notwendig, um Zugriff auf Reddit-Inhalte zu erhalten. Der user_agent ermöglicht zudem die Rückverfolgbarkeit von API-Anfragen seitens Reddit. Ohne diese Authentifizierung ist ein reguliertes, automatisiertes Scraping nicht zulässig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5789b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=(os.getenv(\"REDDIT_ID\")),\n",
    "    client_secret=(os.getenv(\"REDDIT_SECRET\")),\n",
    "    user_agent=(os.getenv(\"USER_AGENT\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c34f9d7",
   "metadata": {},
   "source": [
    "**Logging-Konfiguration**\n",
    "\n",
    "Bevor das Reddit-Scraping startet, wird ein Logging-System eingerichtet. Dazu wird zunächst ein Pfad zur Log-Datei definiert – in diesem Fall `logs/reddit.log`. Falls das Verzeichnis `logs/` noch nicht existiert, wird es automatisch erstellt. Anschließend wird das Python-Logging so konfiguriert, dass alle Log-Meldungen in diese Datei geschrieben werden.\n",
    "\n",
    "Die Konfiguration legt fest, dass nur Meldungen ab dem Schweregrad `INFO` gespeichert werden. Außerdem wird das Format der Einträge so definiert, dass jeder Log-Eintrag einen Zeitstempel, den Log-Level (wie `INFO` oder `ERROR`) sowie die eigentliche Nachricht enthält. So entsteht eine nachvollziehbare Chronik über den Ablauf und mögliche Fehler des Scripts.\n",
    "\n",
    "Ein typischer Eintrag könnte zum Beispiel so aussehen:\n",
    "\n",
    "```\n",
    "2025-04-19 14:33:07,512 - INFO - Starte Reddit-Scraping...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b20f953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Logging aktiv unter: C:\\Users\\SofiePischl\\Documents\\01_HdM\\10_ML_OPS\\TrendAnalyse Social Media\\logs\\reddit.log\n"
     ]
    }
   ],
   "source": [
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "# Logging einrichten\n",
    "log_path = Path(\"../logs/reddit.log\")\n",
    "log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    filename=log_path,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "print(f\" Logging aktiv unter: {log_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6781e111",
   "metadata": {},
   "source": [
    "**Reddit-Datensammlung mittels Python und PRAW**\n",
    "\n",
    "Die Funktion `scrape_reddit()` dient der systematischen Erhebung textbasierter Inhalte aus der Social-Media-Plattform **Reddit**. Ziel ist es, strukturierte Daten zur Analyse von Trendthemen zu generieren. Zur Umsetzung wird die Bibliothek `praw` (Python Reddit API Wrapper) verwendet, die eine komfortable Schnittstelle zur Reddit-API bereitstellt.\n",
    "\n",
    "Nach der Initialisierung der Protokollierung via `logging` erfolgt die Authentifizierung an der Reddit-API. Hierfür wird ein `Reddit`-Objekt instanziiert, wobei sensible Zugangsdaten wie `client_id`, `client_secret` und `user_agent` aus einer `.env`-Datei geladen werden. Dieses Vorgehen ermöglicht eine sichere Trennung von Konfiguration und Codebasis und schützt vor dem unbeabsichtigten Leaken von API-Schlüsseln.\n",
    "\n",
    "Im Anschluss wird eine Liste von Subreddits definiert, die sowohl populäre als auch als „trending“ markierte Communities umfasst. Aus diesen Subreddits werden jeweils bis zu 100 Beiträge aus dem Hot-Feed abgerufen. Dieses Verfahren stellt sicher, dass aktuelle, stark diskutierte Inhalte gesammelt werden, die ein hohes Relevanzpotenzial für Trendanalysen aufweisen.\n",
    "\n",
    "Die Datenextraktion erfolgt über eine doppelte Schleife: Für jedes Subreddit werden Hot-Beiträge iteriert, wobei ausschließlich „self-posts“ berücksichtigt werden. Diese beinhalten keine externen Links und ermöglichen dadurch eine fokussierte Analyse des vom Nutzer selbst verfassten Textinhalts. Pro Beitrag werden zentrale Metriken wie Titel, Text, Anzahl der Kommentare, Upvotes, Erstellungszeitpunkt sowie die URL gespeichert. Zur besseren zeitlichen Einordnung wird außerdem ein einheitlicher Zeitstempel für alle Einträge vergeben.\n",
    "\n",
    "Die gesammelten Beiträge werden in einem `pandas.DataFrame` strukturiert und anschließend unter `data/raw/reddit_data.csv` abgespeichert. Dabei wird sichergestellt, dass benötigte Verzeichnisse automatisch erstellt werden. Falls bereits Daten vorhanden sind, werden die neuen Einträge angehängt und anschließend Duplikate basierend auf Titel, Textinhalt und Subreddit entfernt. Die finale Version wird ohne Index in die CSV-Datei geschrieben.\n",
    "\n",
    "Abschließend wird die Anzahl der gespeicherten Beiträge im Logfile vermerkt. Etwaige Fehler werden während der Ausführung abgefangen und entsprechend protokolliert. Die Funktion kann sowohl als Modul importiert als auch direkt per Skriptausführung genutzt werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f757739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Gespeichert unter: C:\\Users\\SofiePischl\\Documents\\01_HdM\\10_ML_OPS\\TrendAnalyse Social Media\\app\\data\\raw\\reddit_data.csv\n"
     ]
    }
   ],
   "source": [
    "def scrape_reddit():\n",
    "    try:\n",
    "        logging.info(\"Starte Reddit-Scraping...\")\n",
    "\n",
    "        subreddits = [\"all\", \"popular\", \"trendingreddits\", \"trendingsubreddits\"]\n",
    "        post_limit = 100\n",
    "        all_posts = []\n",
    "        scrape_time = datetime.now()\n",
    "\n",
    "        for sub in subreddits:\n",
    "            subreddit = reddit.subreddit(sub)\n",
    "            for post in subreddit.hot(limit=post_limit):\n",
    "                if post.is_self:\n",
    "                    all_posts.append({\n",
    "                        \"subreddit\": sub,\n",
    "                        \"title\": post.title,\n",
    "                        \"text\": post.selftext,\n",
    "                        \"score\": post.score,\n",
    "                        \"comments\": post.num_comments,\n",
    "                        \"created\": datetime.fromtimestamp(post.created),\n",
    "                        \"url\": post.url,\n",
    "                        \"scraped_at\": scrape_time\n",
    "                    })\n",
    "\n",
    "        df = pd.DataFrame(all_posts)\n",
    "        csv_path = Path(\"../app/data/raw/reddit_data.csv\")\n",
    "        csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if csv_path.exists():\n",
    "            df_existing = pd.read_csv(csv_path)\n",
    "            df = pd.concat([df_existing, df], ignore_index=True)\n",
    "\n",
    "        df.drop_duplicates(subset=[\"title\", \"text\", \"subreddit\"], inplace=True)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\" Gespeichert unter: {csv_path.resolve()}\")\n",
    "\n",
    "        logging.info(f\"{len(df)} Einträge gespeichert unter {csv_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fehler beim Reddit-Scraping: {e}\")\n",
    "\n",
    "# Ausführung bei direktem Aufruf\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_reddit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba082732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>comments</th>\n",
       "      <th>created</th>\n",
       "      <th>url</th>\n",
       "      <th>scraped_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>popular</td>\n",
       "      <td>Is anyone else getting irritated with the new ...</td>\n",
       "      <td>I get it, it’s something I can put in my prefe...</td>\n",
       "      <td>2474</td>\n",
       "      <td>643</td>\n",
       "      <td>2025-04-19 01:53:11</td>\n",
       "      <td>https://www.reddit.com/r/ChatGPT/comments/1k2j...</td>\n",
       "      <td>2025-04-19 14:20:21.514955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>popular</td>\n",
       "      <td>What is the first thing you’d buy if you get f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5116</td>\n",
       "      <td>7975</td>\n",
       "      <td>2025-04-18 21:51:33</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/1k...</td>\n",
       "      <td>2025-04-19 14:23:21.324902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>popular</td>\n",
       "      <td>Under current law, the Social Security payroll...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9256</td>\n",
       "      <td>1062</td>\n",
       "      <td>2025-04-18 18:47:13</td>\n",
       "      <td>https://www.reddit.com/r/SocialSecurity/commen...</td>\n",
       "      <td>2025-04-19 14:23:21.324902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>trendingreddits</td>\n",
       "      <td>We are open again!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2024-09-27 20:14:11</td>\n",
       "      <td>https://www.reddit.com/r/TrendingReddits/comme...</td>\n",
       "      <td>2025-04-19 14:23:21.324902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>trendingreddits</td>\n",
       "      <td>Hi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-04-02 21:13:09</td>\n",
       "      <td>https://www.reddit.com/r/TrendingReddits/comme...</td>\n",
       "      <td>2025-04-19 14:23:21.324902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           subreddit                                              title  \\\n",
       "162          popular  Is anyone else getting irritated with the new ...   \n",
       "163          popular  What is the first thing you’d buy if you get f...   \n",
       "164          popular  Under current law, the Social Security payroll...   \n",
       "165  trendingreddits                                 We are open again!   \n",
       "166  trendingreddits                                                 Hi   \n",
       "\n",
       "                                                  text  score  comments  \\\n",
       "162  I get it, it’s something I can put in my prefe...   2474       643   \n",
       "163                                                NaN   5116      7975   \n",
       "164                                                NaN   9256      1062   \n",
       "165                                                NaN      4         2   \n",
       "166                                                NaN      1         4   \n",
       "\n",
       "                 created                                                url  \\\n",
       "162  2025-04-19 01:53:11  https://www.reddit.com/r/ChatGPT/comments/1k2j...   \n",
       "163  2025-04-18 21:51:33  https://www.reddit.com/r/AskReddit/comments/1k...   \n",
       "164  2025-04-18 18:47:13  https://www.reddit.com/r/SocialSecurity/commen...   \n",
       "165  2024-09-27 20:14:11  https://www.reddit.com/r/TrendingReddits/comme...   \n",
       "166  2025-04-02 21:13:09  https://www.reddit.com/r/TrendingReddits/comme...   \n",
       "\n",
       "                     scraped_at  \n",
       "162  2025-04-19 14:20:21.514955  \n",
       "163  2025-04-19 14:23:21.324902  \n",
       "164  2025-04-19 14:23:21.324902  \n",
       "165  2025-04-19 14:23:21.324902  \n",
       "166  2025-04-19 14:23:21.324902  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zeuge neuste Einträge\n",
    "df = pd.read_csv(\"../app/data/raw/reddit_data.csv\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c86300dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8152499",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Instagram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125499c",
   "metadata": {},
   "source": [
    "was nicht funktioniert hat: instaloader, playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00ba8d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a93c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e585a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTA_USERNAME= os.getenv(\"INSTA_USERNAME\")\n",
    "INSTA_PASSWORD= os.getenv(\"INSTA_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9727179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging aktiv unter: C:\\Users\\SofiePischl\\Documents\\01_HdM\\10_ML_OPS\\TrendAnalyse Social Media\\logs\\instagram.log\n"
     ]
    }
   ],
   "source": [
    "# Logging initialisieren\n",
    "log_path = Path(\"../logs/instagram.log\")\n",
    "log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    filename=log_path,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "print(f\"Logging aktiv unter: {log_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c344738",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_instagram_explore():\n",
    "    try:\n",
    "        logging.info(\"Starte Instagram-Explore-Scraping...\")\n",
    "\n",
    "        # Headless-Browser konfigurieren\n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless=new\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--window-size=1920x1080\")\n",
    "\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        driver.get(\"https://www.instagram.com/accounts/login/\")\n",
    "\n",
    "        # Login\n",
    "        username = os.getenv(\"INSTA_USERNAME\")\n",
    "        password = os.getenv(\"INSTA_PASSWORD\")\n",
    "\n",
    "        time.sleep(3)\n",
    "        driver.find_element(By.NAME, \"username\").send_keys(username)\n",
    "        driver.find_element(By.NAME, \"password\").send_keys(password)\n",
    "        driver.find_element(By.XPATH, \"//button[@type='submit']\").click()\n",
    "\n",
    "        time.sleep(7)  # Warte auf Login\n",
    "\n",
    "        # Gehe zur Explore-Seite\n",
    "        driver.get(\"https://www.instagram.com/explore/\")\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Seite scrollen, um mehr Posts zu laden\n",
    "        for _ in range(3):  # 3x scrollen → kannst du erhöhen\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(3)\n",
    "\n",
    "        # Beiträge extrahieren\n",
    "        posts = driver.find_elements(By.XPATH, '//a[contains(@href, \"/p/\")]')\n",
    "        post_data = []\n",
    "\n",
    "        for post in posts:\n",
    "            try:\n",
    "                url = post.get_attribute(\"href\")\n",
    "                img = post.find_element(By.TAG_NAME, \"img\")\n",
    "                img_url = img.get_attribute(\"src\")\n",
    "                description = img.get_attribute(\"alt\")\n",
    "\n",
    "                post_data.append({\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"url\": url,\n",
    "                    \"image\": img_url,\n",
    "                    \"description\": description\n",
    "                })\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Fehler beim Extrahieren eines Posts: {e}\")\n",
    "                continue\n",
    "\n",
    "        # In CSV speichern\n",
    "        csv_path = Path(\"../app/data/raw/instagram_data.csv\")\n",
    "        csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        df = pd.DataFrame(post_data)\n",
    "\n",
    "        if csv_path.exists():\n",
    "            df_existing = pd.read_csv(csv_path)\n",
    "            df = pd.concat([df_existing, df], ignore_index=True)\n",
    "\n",
    "        df.drop_duplicates(subset=[\"url\", \"image\", \"description\"], inplace=True)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "\n",
    "        logging.info(f\"{len(df)} Einträge gespeichert unter {csv_path}\")\n",
    "        driver.quit()\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fehler beim Instagram-Explore-Scraping: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e397062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jetzt ausführen\n",
    "scrape_instagram_explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c60440bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_instagram_post(url):\n",
    "    try:\n",
    "        logging.info(f\"Starte Instagram-Scraping für URL: {url}\")\n",
    "        \n",
    "        # Zielpfad vorbereiten\n",
    "        csv_path = Path(\"../app/data/raw/instagram_data.csv\")\n",
    "        csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Browser initialisieren\n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless=new\")  # im Hintergrund ausführen\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--window-size=1920x1080\")\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        wait = WebDriverWait(driver, 15)\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        # Beitrag laden\n",
    "        wait.until(EC.presence_of_element_located((By.TAG_NAME, \"article\")))\n",
    "\n",
    "        # Login-Popup schließen (falls vorhanden)\n",
    "        try:\n",
    "            close_btn = wait.until(EC.element_to_be_clickable((By.XPATH, \"//div[@role='dialog']//button\")))\n",
    "            close_btn.click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Username extrahieren\n",
    "        try:\n",
    "            username_elem = wait.until(EC.visibility_of_element_located(\n",
    "                (By.XPATH, \"//a[contains(@href, '/') and contains(@class, 'notranslate')]//span[1]\")\n",
    "            ))\n",
    "            username = username_elem.text.strip()\n",
    "        except:\n",
    "            username = \"\"\n",
    "\n",
    "        # Caption extrahieren\n",
    "        try:\n",
    "            caption_elem = wait.until(EC.visibility_of_element_located(\n",
    "                (By.XPATH, \"//div[@data-testid='post-comment-root']//span\")\n",
    "            ))\n",
    "            caption = caption_elem.text.strip()\n",
    "        except:\n",
    "            caption = \"\"\n",
    "\n",
    "        # Likes (optional)\n",
    "        try:\n",
    "            likes_elem = wait.until(EC.visibility_of_element_located(\n",
    "                (By.XPATH, \"//section//span[contains(text(), 'Gefällt')]\")\n",
    "            ))\n",
    "            likes = likes_elem.text.strip()\n",
    "        except:\n",
    "            likes = \"\"\n",
    "\n",
    "        # Timestamp + Struktur\n",
    "        data = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"url\": url,\n",
    "            \"username\": username,\n",
    "            \"caption\": caption,\n",
    "            \"likes\": likes\n",
    "        }\n",
    "\n",
    "        df_new = pd.DataFrame([data])\n",
    "\n",
    "        if csv_path.exists():\n",
    "            df_existing = pd.read_csv(csv_path)\n",
    "            df_new = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "\n",
    "        df_new.drop_duplicates(subset=[\"url\", \"caption\", \"username\"], inplace=True)\n",
    "        df_new.to_csv(csv_path, index=False)\n",
    "\n",
    "        logging.info(f\"Erfolgreich gespeichert unter {csv_path}\")\n",
    "        driver.quit()\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fehler beim Scrapen von {url}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9f1d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispielaufruf\n",
    "scrape_instagram_post(\"https://www.instagram.com/p/DICWDtYNq7E/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafbbd32",
   "metadata": {},
   "source": [
    "# 1. TWITTER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25574e53",
   "metadata": {},
   "source": [
    "\n",
    "##### 1. Versuche mit der offiziellen Twitter API (v2)\n",
    "\n",
    "Zunächst wurde ein Zugriff über die offizielle Twitter Developer API (v2) realisiert, wobei ein Bearer Token über ein entsprechendes Developer-Konto eingebunden wurde. Die Verbindung wurde in der Regel erfolgreich hergestellt, jedoch resultierten sämtliche Anfragen bereits nach wenigen Requests in einem **HTTP 429 – Too Many Requests** Fehler. Selbst bei einer Limitierung auf nur drei Tweets wurde die Anfrage durch die API geblockt. Dies deutet auf eine äußerst restriktive Ratenbegrenzung hin, selbst im Rahmen von „Essential Access“-Leveln.\n",
    "\n",
    "Außerdem sind in der kostenlos erstellbaren Developer App nur 100 Tweets abgreifbar, sodass diese Lösung, selbst wenn sie funktionieren würde, nicht sillführend währe.\n",
    "\n",
    "### 2. Nutzung alternativer Scraping-Bibliotheken (z. B. `snscrape`)\n",
    "\n",
    "In einem zweiten Schritt wurde die populäre Bibliothek [`snscrape`](https://github.com/JustAnotherArchivist/snscrape) eingesetzt, welche ohne API-Zugang direkt über die öffentliche Webschnittstelle von Twitter (u. a. GraphQL-Endpunkte) operiert. Dieses Vorgehen war früher eine gängige Methode zur Umgehung von Authentifizierungsbeschränkungen.\n",
    "\n",
    "Aktuell führt jedoch auch dieser Ansatz regelmäßig zu Fehlern. Konkret wurde mehrfach der folgende Exception ausgelöst:\n",
    "\n",
    "```\n",
    "ScraperException: 4 requests to .../SearchTimeline failed, giving up.\n",
    "```\n",
    "\n",
    "Diese Meldung ist ein Hinweis darauf, dass Twitter die internen API-Endpunkte (GraphQL) entweder modifiziert oder stark abgesichert hat. `snscrape` kann infolgedessen die erwarteten Objekte nicht mehr extrahieren. Entsprechende Issues sind auch in der GitHub-Community der Bibliothek dokumentiert, eine stabile Lösung existiert derzeit nicht.\n",
    "\n",
    "\n",
    "Klar! Hier ist dein Text ergänzt um eine saubere und wissenschaftlich formulierte Beschreibung des Problems mit **Playwright**, passend zu deiner bisherigen Struktur:\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Headless-Scraping mit Playwright\n",
    "\n",
    "Ein weiterer Ansatz zur Umgehung offizieller API-Beschränkungen bestand im Einsatz der Automatisierungsbibliothek [`Playwright`](https://playwright.dev/python/), die über einen echten Chromium-Browser Webinhalte visuell bzw. DOM-basiert extrahieren kann. Diese Methode ist grundsätzlich vielversprechend, da sie wie ein menschlicher Nutzer agiert und damit keine API-Limits verletzt.\n",
    "\n",
    "Im konkreten Fall wurde Playwright zunächst als **synchrones Skript** innerhalb eines Jupyter Notebooks getestet. Dabei trat jedoch ein technisches Problem auf: Da Jupyter selbst auf einem laufenden `asyncio`-Event-Loop basiert, ist die Verwendung von `sync_playwright()` nicht erlaubt. Dies führte unmittelbar zu folgender Fehlermeldung:\n",
    "\n",
    "```\n",
    "Error: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
    "```\n",
    "\n",
    "Als Reaktion darauf wurde auf die empfohlene **asynchrone API-Variante** (`async_playwright()`) umgestellt. Auch dieser Ansatz scheiterte jedoch – insbesondere unter Windows – mit folgendem Fehler:\n",
    "\n",
    "```\n",
    "NotImplementedError: asyncio subprocess transport is not implemented\n",
    "```\n",
    "\n",
    "Diese Meldung verweist auf eine bekannte Einschränkung der Playwright-Implementierung im Zusammenspiel mit dem Event-Loop von Windows und Jupyter. Der Start des internen Chromium-Drivers schlägt aufgrund fehlender Unterstützung für `asyncio.create_subprocess_exec()` fehl. Damit ist Playwright im Rahmen einer Notebook-Umgebung faktisch **nicht lauffähig**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2717e9e3",
   "metadata": {},
   "source": [
    "# 4. TikTok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1415a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TikTokApi import TikTokApi\n",
    "import asyncio\n",
    "import os\n",
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env values\n",
    "load_dotenv()\n",
    "\n",
    "ms_token = os.getenv(\"MS_TOKEN\")\n",
    "csv_path = os.getenv(\"OUTPUT_PATH\", \"trending_videos.csv\")\n",
    "\n",
    "\n",
    "async def trending_videos():\n",
    "    async with TikTokApi() as api:\n",
    "        await api.create_sessions(\n",
    "            ms_tokens=[ms_token],\n",
    "            num_sessions=1,\n",
    "            sleep_after=3,\n",
    "            browser=os.getenv(\"TIKTOK_BROWSER\", \"chromium\")\n",
    "        )\n",
    "\n",
    "        data = []\n",
    "\n",
    "        async for video in api.trending.videos(count=30):\n",
    "            info = video.as_dict\n",
    "            data.append({\n",
    "                \"id\": info.get(\"id\"),\n",
    "                \"description\": info.get(\"desc\"),\n",
    "                \"author_username\": info.get(\"author\", {}).get(\"uniqueId\"),\n",
    "                \"author_id\": info.get(\"author\", {}).get(\"id\"),\n",
    "                \"likes\": info.get(\"stats\", {}).get(\"diggCount\"),\n",
    "                \"shares\": info.get(\"stats\", {}).get(\"shareCount\"),\n",
    "                \"comments\": info.get(\"stats\", {}).get(\"commentCount\"),\n",
    "                \"plays\": info.get(\"stats\", {}).get(\"playCount\"),\n",
    "                \"video_url\": info.get(\"video\", {}).get(\"downloadAddr\"),\n",
    "                \"created_time\": info.get(\"createTime\"),\n",
    "            })\n",
    "\n",
    "        os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "\n",
    "        file_exists = os.path.isfile(csv_path)\n",
    "        with open(csv_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=data[0].keys())\n",
    "            if not file_exists:\n",
    "                writer.writeheader()\n",
    "            writer.writerows(data)\n",
    "\n",
    "        print(f\"\\n✅ Erfolgreich {len(data)} Videos in '{csv_path}' gespeichert.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(trending_videos())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc307378",
   "metadata": {},
   "source": [
    "# 5. YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc6c0041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>published_at</th>\n",
       "      <th>view_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>url</th>\n",
       "      <th>scraped_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4sHs9ujo1eg</td>\n",
       "      <td>Die Heuchelei der Stars &amp; Influencer auf dem C...</td>\n",
       "      <td>Das Coachella Festival ist grade bei vielen In...</td>\n",
       "      <td>Alicia Joe</td>\n",
       "      <td>2025-04-21T16:02:49Z</td>\n",
       "      <td>322656</td>\n",
       "      <td>19853</td>\n",
       "      <td>1352</td>\n",
       "      <td>https://www.youtube.com/watch?v=4sHs9ujo1eg</td>\n",
       "      <td>2025-04-22 22:06:12.302112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gTA6gQIC39A</td>\n",
       "      <td>Warum ausgerechnet Katy Perry jetzt ins All fl...</td>\n",
       "      <td>Anzeige | Ladet euch für eure nächste Reise Ai...</td>\n",
       "      <td>Desy</td>\n",
       "      <td>2025-04-21T17:30:01Z</td>\n",
       "      <td>172254</td>\n",
       "      <td>10729</td>\n",
       "      <td>1183</td>\n",
       "      <td>https://www.youtube.com/watch?v=gTA6gQIC39A</td>\n",
       "      <td>2025-04-22 22:06:12.302112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ncXAUBGV8JI</td>\n",
       "      <td>Problemmotoren von VW, Ford, Stellantis und Co...</td>\n",
       "      <td>Ein nasser, in Öl geführter Zahnriemen schien ...</td>\n",
       "      <td>auto motor und sport</td>\n",
       "      <td>2025-04-21T15:01:16Z</td>\n",
       "      <td>159413</td>\n",
       "      <td>2657</td>\n",
       "      <td>768</td>\n",
       "      <td>https://www.youtube.com/watch?v=ncXAUBGV8JI</td>\n",
       "      <td>2025-04-22 22:06:12.302112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dFKCoqPjJ28</td>\n",
       "      <td>Fynn Kliemanns Comeback ist ein Fiebertraum</td>\n",
       "      <td>Fynn Kliemann ist zurück. Der DIY-Künstler und...</td>\n",
       "      <td>Der Dunkle Parabelritter</td>\n",
       "      <td>2025-04-21T17:30:02Z</td>\n",
       "      <td>169827</td>\n",
       "      <td>10718</td>\n",
       "      <td>1113</td>\n",
       "      <td>https://www.youtube.com/watch?v=dFKCoqPjJ28</td>\n",
       "      <td>2025-04-22 22:06:12.302112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gzCXZID3AlY</td>\n",
       "      <td>Streit mit PAULA ? Kontakt mit leiblichen ELTE...</td>\n",
       "      <td>Anzeige | \\nHey, wie versprochen kommen hier a...</td>\n",
       "      <td>Beauty Benzz</td>\n",
       "      <td>2025-04-21T14:00:58Z</td>\n",
       "      <td>70563</td>\n",
       "      <td>2880</td>\n",
       "      <td>293</td>\n",
       "      <td>https://www.youtube.com/watch?v=gzCXZID3AlY</td>\n",
       "      <td>2025-04-22 22:06:12.302112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                              title  \\\n",
       "0  4sHs9ujo1eg  Die Heuchelei der Stars & Influencer auf dem C...   \n",
       "1  gTA6gQIC39A  Warum ausgerechnet Katy Perry jetzt ins All fl...   \n",
       "2  ncXAUBGV8JI  Problemmotoren von VW, Ford, Stellantis und Co...   \n",
       "3  dFKCoqPjJ28        Fynn Kliemanns Comeback ist ein Fiebertraum   \n",
       "4  gzCXZID3AlY  Streit mit PAULA ? Kontakt mit leiblichen ELTE...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Das Coachella Festival ist grade bei vielen In...   \n",
       "1  Anzeige | Ladet euch für eure nächste Reise Ai...   \n",
       "2  Ein nasser, in Öl geführter Zahnriemen schien ...   \n",
       "3  Fynn Kliemann ist zurück. Der DIY-Künstler und...   \n",
       "4  Anzeige | \\nHey, wie versprochen kommen hier a...   \n",
       "\n",
       "              channel_title          published_at view_count like_count  \\\n",
       "0                Alicia Joe  2025-04-21T16:02:49Z     322656      19853   \n",
       "1                      Desy  2025-04-21T17:30:01Z     172254      10729   \n",
       "2      auto motor und sport  2025-04-21T15:01:16Z     159413       2657   \n",
       "3  Der Dunkle Parabelritter  2025-04-21T17:30:02Z     169827      10718   \n",
       "4              Beauty Benzz  2025-04-21T14:00:58Z      70563       2880   \n",
       "\n",
       "  comment_count                                          url  \\\n",
       "0          1352  https://www.youtube.com/watch?v=4sHs9ujo1eg   \n",
       "1          1183  https://www.youtube.com/watch?v=gTA6gQIC39A   \n",
       "2           768  https://www.youtube.com/watch?v=ncXAUBGV8JI   \n",
       "3          1113  https://www.youtube.com/watch?v=dFKCoqPjJ28   \n",
       "4           293  https://www.youtube.com/watch?v=gzCXZID3AlY   \n",
       "\n",
       "                   scraped_at  \n",
       "0  2025-04-22 22:06:12.302112  \n",
       "1  2025-04-22 22:06:12.302112  \n",
       "2  2025-04-22 22:06:12.302112  \n",
       "3  2025-04-22 22:06:12.302112  \n",
       "4  2025-04-22 22:06:12.302112  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# === ENV & Logging ===\n",
    "load_dotenv()\n",
    "\n",
    "log_path = Path(\"../logs/youtube.log\")\n",
    "log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    filename=log_path,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# === YouTube API Setup ===\n",
    "API_KEY = os.getenv(\"YT_KEY\")\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "def scrape_youtube_trending(region=\"DE\", max_results=50):\n",
    "    try:\n",
    "        logging.info(\"Starte YouTube-Scraping...\")\n",
    "\n",
    "        request = youtube.videos().list(\n",
    "            part=\"snippet,statistics\",\n",
    "            chart=\"mostPopular\",\n",
    "            regionCode=region,\n",
    "            maxResults=max_results\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        videos = []\n",
    "        scrape_time = datetime.now()\n",
    "\n",
    "        for item in response.get(\"items\", []):\n",
    "            snippet = item[\"snippet\"]\n",
    "            stats = item.get(\"statistics\", {})\n",
    "\n",
    "            videos.append({\n",
    "                \"video_id\": item[\"id\"],\n",
    "                \"title\": snippet.get(\"title\"),\n",
    "                \"description\": snippet.get(\"description\"),\n",
    "                \"channel_title\": snippet.get(\"channelTitle\"),\n",
    "                \"published_at\": snippet.get(\"publishedAt\"),\n",
    "                \"view_count\": stats.get(\"viewCount\"),\n",
    "                \"like_count\": stats.get(\"likeCount\"),\n",
    "                \"comment_count\": stats.get(\"commentCount\"),\n",
    "                \"url\": f\"https://www.youtube.com/watch?v={item['id']}\",\n",
    "                \"scraped_at\": scrape_time\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(videos)\n",
    "        csv_path = Path(\"../app/data/raw/youtube_data.csv\")\n",
    "        csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if csv_path.exists():\n",
    "            df_existing = pd.read_csv(csv_path)\n",
    "            df = pd.concat([df_existing, df], ignore_index=True)\n",
    "\n",
    "        df.drop_duplicates(subset=[\"video_id\"], inplace=True)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "\n",
    "        display(df.head())\n",
    "\n",
    "        logging.info(f\"{len(df)} Videos gespeichert unter {csv_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fehler beim YouTube-Scraping: {e}\")\n",
    "        print(f\" Fehler: {e}\")\n",
    "\n",
    "# Direkter Aufruf\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_youtube_trending()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
